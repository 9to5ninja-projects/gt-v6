# Core dependencies
numpy==2.3.5
torch==2.10.0
transformers==5.0.0
datasets  # version not found in freeze, add if needed
tqdm==4.67.1
einops==0.8.2
filelock==3.20.3
typing-extensions==4.15.0
sympy==1.14.0
networkx==3.6.1
jinja2==3.1.6
fsspec==2026.1.0
cuda-bindings==12.9.4
nvidia-cuda-nvrtc-cu12==12.8.93
nvidia-cuda-runtime-cu12==12.8.90
nvidia-cuda-cupti-cu12==12.8.90
nvidia-cudnn-cu12==9.10.2.21
nvidia-cublas-cu12==12.8.4.1
nvidia-cufft-cu12==11.3.3.83
nvidia-curand-cu12==10.3.9.90
nvidia-cusolver-cu12==11.7.3.90
nvidia-cusparse-cu12==12.5.8.93
nvidia-cusparselt-cu12==0.7.1
nvidia-nccl-cu12==2.27.5
nvidia-nvshmem-cu12==3.4.5
nvidia-nvtx-cu12==12.8.90
nvidia-nvjitlink-cu12==12.8.93
nvidia-cufile-cu12==1.13.1.3
triton==3.6.0
# Project-specific
flash-attn @ file:///home/m_tes/groundthink/gt-v6/flash-attn-unpacked/kaggle/working/flash-attention/dist/flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl#sha256=68ad03527ff1073019a891c196262b36cdc1939e5a924ca422c579b08838c9c9
flash-linear-attention==0.4.1
fla-core==0.4.1
# Utilities
matplotlib-inline==0.2.1
# Optional: for Jupyter/IPython
ipython==9.9.0
jupyter_client==8.8.0
jupyter_core==5.9.1
