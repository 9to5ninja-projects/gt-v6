{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroundThink V6 - Hybrid GatedDeltaNet + SWA\n",
    "\n",
    "- GatedDeltaNet from `fla.layers`\n",
    "- SWA via `flash_attn` with `window_size` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 0: INSTALL\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install -q triton\n",
    "!pip install -q flash-linear-attention\n",
    "!pip install -q flash-attn --no-build-isolation\n",
    "!pip install -q datasets transformers\n",
    "\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "\n",
    "# Verify imports\n",
    "from fla.layers import GatedDeltaNet\n",
    "print(\"GatedDeltaNet: OK\")\n",
    "\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    print(\"flash_attn: OK\")\n",
    "    FLASH_ATTN_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"flash_attn: FAILED ({e}) - will use custom SWA\")\n",
    "    FLASH_ATTN_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: CONFIG\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = 50257\n",
    "    d_model: int = 512\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 8\n",
    "    head_dim: int = 64\n",
    "    attn_interval: int = 4  # SWA every N layers\n",
    "    window_size: int = 512  # Sliding window size\n",
    "    expand_k: float = 1.0\n",
    "    expand_v: float = 2.0\n",
    "    use_gradient_checkpointing: bool = True\n",
    "    tie_weights: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.head_dim = self.d_model // self.n_heads\n",
    "        \n",
    "    def get_swa_layer_indices(self):\n",
    "        return [i for i in range(self.n_layers) if i % self.attn_interval == (self.attn_interval - 1)]\n",
    "\n",
    "@dataclass \n",
    "class TrainConfig:\n",
    "    dataset_name: str = \"HuggingFaceFW/fineweb-edu\"\n",
    "    dataset_subset: str = \"sample-10BT\"\n",
    "    target_tokens: int = 20_000_000\n",
    "    batch_size: int = 2\n",
    "    seq_len: int = 512\n",
    "    accum_steps: int = 2\n",
    "    steps: int = 10000\n",
    "    warmup_ratio: float = 0.1\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.1\n",
    "    grad_clip: float = 1.0\n",
    "    betas: tuple = (0.9, 0.95)\n",
    "    log_interval: int = 50\n",
    "    grad_log_interval: int = 500\n",
    "    niah_checkpoints: List[int] = field(default_factory=lambda: [500, 1000, 2000, 3000, 5000, 7500, 10000])\n",
    "    \n",
    "    @property\n",
    "    def warmup_steps(self): return int(self.steps * self.warmup_ratio)\n",
    "    @property\n",
    "    def effective_batch_size(self): return self.batch_size * self.accum_steps\n",
    "\n",
    "MODEL_CFG = ModelConfig()\n",
    "TRAIN_CFG = TrainConfig()\n",
    "print(f\"Config: d={MODEL_CFG.d_model}, layers={MODEL_CFG.n_layers}\")\n",
    "print(f\"SWA at layers: {MODEL_CFG.get_swa_layer_indices()}\")\n",
    "print(f\"Window size: {MODEL_CFG.window_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: IMPORTS\n",
    "import math, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {props.name} ({props.total_memory/1e9:.1f}GB)\")\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# FLA GatedDeltaNet\n",
    "from fla.layers import GatedDeltaNet\n",
    "print(\"GatedDeltaNet loaded\")\n",
    "\n",
    "# Flash attention for SWA\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    FLASH_ATTN = True\n",
    "    print(\"flash_attn loaded\")\n",
    "except ImportError:\n",
    "    FLASH_ATTN = False\n",
    "    print(\"flash_attn not available - using custom SWA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: MODEL COMPONENTS\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        return (x.float() * x.float().pow(2).mean(-1, keepdim=True).add(self.eps).rsqrt()).type_as(x) * self.weight\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, d_model, expansion=8/3):\n",
    "        super().__init__()\n",
    "        hidden = ((int(d_model * expansion) + 63) // 64) * 64\n",
    "        self.w1 = nn.Linear(d_model, hidden, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, hidden, bias=False)\n",
    "        self.w2 = nn.Linear(hidden, d_model, bias=False)\n",
    "        self.norm = RMSNorm(d_model)\n",
    "    def forward(self, x):\n",
    "        h = self.norm(x)\n",
    "        return x + self.w2(F.silu(self.w1(h)) * self.w3(h))\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    \"\"\"Sliding Window Attention using flash_attn or custom fallback.\"\"\"\n",
    "    def __init__(self, d_model, n_heads, window_size, layer_idx=0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.window_size = window_size\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        \n",
    "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        \n",
    "        if FLASH_ATTN:\n",
    "            # flash_attn expects (B, T, H, D) and handles sliding window\n",
    "            out = flash_attn_func(\n",
    "                q, k, v,\n",
    "                causal=True,\n",
    "                window_size=(self.window_size, 0)  # (left, right) - causal so right=0\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: manual sliding window attention\n",
    "            q = q.transpose(1, 2)  # (B, H, T, D)\n",
    "            k = k.transpose(1, 2)\n",
    "            v = v.transpose(1, 2)\n",
    "            \n",
    "            # Create sliding window + causal mask\n",
    "            mask = torch.ones(T, T, device=x.device, dtype=torch.bool).triu(1)  # causal\n",
    "            mask |= torch.ones(T, T, device=x.device, dtype=torch.bool).tril(-self.window_size)  # window\n",
    "            \n",
    "            attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "            attn = attn.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "            out = (attn @ v).transpose(1, 2)  # (B, T, H, D)\n",
    "        \n",
    "        out = out.reshape(B, T, D)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class HybridBlock(nn.Module):\n",
    "    \"\"\"Either GatedDeltaNet or SlidingWindowAttention.\"\"\"\n",
    "    def __init__(self, d_model, is_attention, n_heads=8, window_size=512,\n",
    "                 expand_k=1.0, expand_v=2.0, layer_idx=0):\n",
    "        super().__init__()\n",
    "        self.is_attention = is_attention\n",
    "        self.layer_idx = layer_idx\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        \n",
    "        if is_attention:\n",
    "            self.layer = SlidingWindowAttention(d_model, n_heads, window_size, layer_idx)\n",
    "        else:\n",
    "            self.layer = GatedDeltaNet(\n",
    "                hidden_size=d_model, \n",
    "                expand_k=expand_k,\n",
    "                expand_v=expand_v,\n",
    "                layer_idx=layer_idx\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, past_state=None, use_cache=False):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        new_state = None\n",
    "        \n",
    "        if self.is_attention:\n",
    "            x = self.layer(x)\n",
    "        else:\n",
    "            if use_cache:\n",
    "                x, new_state = self.layer(x, past_state=past_state, use_cache=True)\n",
    "            else:\n",
    "                x = self.layer(x)\n",
    "        \n",
    "        return residual + x, new_state\n",
    "\n",
    "class GroundThinkLM(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        \n",
    "        swa_indices = set(cfg.get_swa_layer_indices())\n",
    "        self._swa_indices = swa_indices\n",
    "        self._gdn_indices = set(range(cfg.n_layers)) - swa_indices\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        for i in range(cfg.n_layers):\n",
    "            self.blocks.append(HybridBlock(\n",
    "                cfg.d_model, is_attention=(i in swa_indices),\n",
    "                n_heads=cfg.n_heads, window_size=cfg.window_size,\n",
    "                expand_k=cfg.expand_k, expand_v=cfg.expand_v, layer_idx=i))\n",
    "            self.ffns.append(SwiGLUFFN(cfg.d_model))\n",
    "        \n",
    "        self.norm_f = RMSNorm(cfg.d_model)\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "        if cfg.tie_weights:\n",
    "            self.lm_head.weight = self.embed.weight\n",
    "        \n",
    "    def forward(self, input_ids, targets=None, past_states=None, use_cache=False):\n",
    "        x = self.embed(input_ids)\n",
    "        new_states = [] if use_cache else None\n",
    "        \n",
    "        for i, (block, ffn) in enumerate(zip(self.blocks, self.ffns)):\n",
    "            past_state = past_states[i] if past_states else None\n",
    "            if self.cfg.use_gradient_checkpointing and self.training and i in self._swa_indices:\n",
    "                x = checkpoint(self._fwd_block, block, ffn, x, use_reentrant=False)\n",
    "            else:\n",
    "                x, state = block(x, past_state, use_cache)\n",
    "                x = ffn(x)\n",
    "                if use_cache: new_states.append(state)\n",
    "        \n",
    "        logits = self.lm_head(self.norm_f(x))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None\n",
    "        return logits, loss, new_states\n",
    "    \n",
    "    @staticmethod\n",
    "    def _fwd_block(block, ffn, x):\n",
    "        x, _ = block(x, None, False)\n",
    "        return ffn(x)\n",
    "    \n",
    "    def get_layer_types(self):\n",
    "        return ['SWA' if i in self._swa_indices else 'GDN' for i in range(self.cfg.n_layers)]\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        c = {'embed': sum(p.numel() for p in self.embed.parameters()), 'gdn': 0, 'swa': 0, 'ffn': 0}\n",
    "        for i, (b, f) in enumerate(zip(self.blocks, self.ffns)):\n",
    "            bp, fp = sum(p.numel() for p in b.parameters()), sum(p.numel() for p in f.parameters())\n",
    "            c['swa' if i in self._swa_indices else 'gdn'] += bp\n",
    "            c['ffn'] += fp\n",
    "        c['total'] = sum(c.values())\n",
    "        return c\n",
    "\n",
    "print(\"Model components defined\")\n",
    "print(f\"Using flash_attn: {FLASH_ATTN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: MONITORING\n",
    "\n",
    "def print_gradient_summary(model):\n",
    "    agg = {'embed': [], 'gdn': [], 'swa': [], 'ffn': []}\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.grad is None: continue\n",
    "        n = p.grad.norm().item()\n",
    "        if 'embed' in name: agg['embed'].append(n)\n",
    "        elif 'ffn' in name: agg['ffn'].append(n)\n",
    "        elif 'blocks' in name:\n",
    "            idx = int(name.split('.')[1])\n",
    "            agg['swa' if idx in model._swa_indices else 'gdn'].append(n)\n",
    "    print(\"Gradient Norms:\")\n",
    "    for k, v in agg.items():\n",
    "        if v: print(f\"  {k}: mean={np.mean(v):.3f} max={np.max(v):.2f}\")\n",
    "\n",
    "def needle_test(model, tokenizer, seq_len=512, n_trials=50, needle_token=50250, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_trials):\n",
    "            tokens = torch.randint(1000, 10000, (1, seq_len), device=device)\n",
    "            pos = torch.randint(64, seq_len - 64, (1,)).item()\n",
    "            tokens[0, pos] = needle_token\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits, _, _ = model(tokens)\n",
    "            probs.append(F.softmax(logits[0, -1].float(), dim=-1)[needle_token].item())\n",
    "    rc = 1.0 / tokenizer.vocab_size\n",
    "    return {'mean': np.mean(probs), 'std': np.std(probs), 'ratio': np.mean(probs) / rc}\n",
    "\n",
    "def probe_layers(model, tokenizer, needle_id=50250, seq_len=512, pos=256, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    tokens = torch.randint(1000, 10000, (1, seq_len), device=device)\n",
    "    tokens[0, pos] = needle_id\n",
    "    with torch.no_grad():\n",
    "        x = model.embed(tokens)\n",
    "        emb = model.embed.weight[needle_id].float()\n",
    "        print(f\"Needle representation:\")\n",
    "        for i, (b, f) in enumerate(zip(model.blocks, model.ffns)):\n",
    "            x, _ = b(x, None, False)\n",
    "            x = f(x)\n",
    "            sim = F.cosine_similarity(x[0, pos].float(), emb, dim=0).item()\n",
    "            t = \"SWA\" if i in model._swa_indices else \"GDN\"\n",
    "            print(f\"  L{i:2d}[{t}]: {sim:+.3f}\")\n",
    "\n",
    "print(\"Monitoring ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: DATA\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "MODEL_CFG.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(f\"Streaming {TRAIN_CFG.dataset_name}...\")\n",
    "ds = load_dataset(TRAIN_CFG.dataset_name, name=TRAIN_CFG.dataset_subset, split=\"train\", streaming=True)\n",
    "buf = []\n",
    "pbar = tqdm(total=TRAIN_CFG.target_tokens, unit=\"tok\")\n",
    "for ex in ds:\n",
    "    toks = tokenizer.encode(ex['text']) + [tokenizer.eos_token_id]\n",
    "    buf.extend(toks)\n",
    "    pbar.update(len(toks))\n",
    "    if len(buf) >= TRAIN_CFG.target_tokens: break\n",
    "pbar.close()\n",
    "\n",
    "all_tokens = torch.tensor(buf[:TRAIN_CFG.target_tokens], dtype=torch.long)\n",
    "del buf, ds\n",
    "print(f\"Loaded {len(all_tokens):,} tokens\")\n",
    "\n",
    "def get_batch():\n",
    "    ix = torch.randint(len(all_tokens) - TRAIN_CFG.seq_len - 1, (TRAIN_CFG.batch_size,))\n",
    "    x = torch.stack([all_tokens[i:i+TRAIN_CFG.seq_len] for i in ix])\n",
    "    y = torch.stack([all_tokens[i+1:i+TRAIN_CFG.seq_len+1] for i in ix])\n",
    "    return x.to(DEVICE), y.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: BUILD MODEL\n",
    "print(\"Building...\")\n",
    "model = GroundThinkLM(MODEL_CFG).to(DEVICE).to(torch.bfloat16)\n",
    "p = model.count_parameters()\n",
    "print(f\"Params: {p['total']/1e6:.2f}M (GDN:{p['gdn']/1e6:.1f}M, SWA:{p['swa']/1e6:.1f}M, FFN:{p['ffn']/1e6:.1f}M)\")\n",
    "print(f\"Layers: {model.get_layer_types()}\")\n",
    "\n",
    "x, y = get_batch()\n",
    "with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "    _, loss, _ = model(x, y)\n",
    "loss.backward()\n",
    "print(f\"Test: loss={loss.item():.4f}, mem={torch.cuda.max_memory_allocated()/1e9:.2f}GB\")\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: TRAIN\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=TRAIN_CFG.lr, betas=TRAIN_CFG.betas, weight_decay=TRAIN_CFG.weight_decay)\n",
    "losses, niah_traj = [], []\n",
    "start = time.time()\n",
    "\n",
    "print(f\"\\nTRAINING {TRAIN_CFG.steps} steps, batch={TRAIN_CFG.effective_batch_size}\\n\")\n",
    "model.train()\n",
    "\n",
    "for step in range(TRAIN_CFG.steps):\n",
    "    lr = TRAIN_CFG.lr * (step+1)/TRAIN_CFG.warmup_steps if step < TRAIN_CFG.warmup_steps else \\\n",
    "         TRAIN_CFG.lr * 0.5 * (1 + math.cos(math.pi * (step-TRAIN_CFG.warmup_steps)/(TRAIN_CFG.steps-TRAIN_CFG.warmup_steps)))\n",
    "    for pg in opt.param_groups: pg['lr'] = lr\n",
    "    \n",
    "    acc_loss = 0\n",
    "    for _ in range(TRAIN_CFG.accum_steps):\n",
    "        x, y = get_batch()\n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            _, loss, _ = model(x, y)\n",
    "        (loss / TRAIN_CFG.accum_steps).backward()\n",
    "        acc_loss += loss.item()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), TRAIN_CFG.grad_clip)\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    losses.append(acc_loss / TRAIN_CFG.accum_steps)\n",
    "    \n",
    "    if step % TRAIN_CFG.log_interval == 0:\n",
    "        avg = np.mean(losses[-50:]) if len(losses) >= 50 else np.mean(losses)\n",
    "        tps = (step+1) * TRAIN_CFG.effective_batch_size * TRAIN_CFG.seq_len / (time.time()-start)\n",
    "        print(f\"[{step:5d}] loss={avg:.4f} lr={lr:.2e} {tps:,.0f}tok/s\")\n",
    "    \n",
    "    if (step+1) % TRAIN_CFG.grad_log_interval == 0:\n",
    "        print_gradient_summary(model)\n",
    "    \n",
    "    if (step+1) in TRAIN_CFG.niah_checkpoints:\n",
    "        n = needle_test(model, tokenizer, TRAIN_CFG.seq_len, 30, device=DEVICE)\n",
    "        niah_traj.append((step+1, n['ratio']))\n",
    "        print(f\"  NIAH@{step+1}: {n['ratio']:.2f}x\")\n",
    "        model.train()\n",
    "\n",
    "print(f\"\\nDone in {(time.time()-start)/60:.1f}min\")\n",
    "print(f\"Loss: {np.mean(losses[:50]):.4f} -> {np.mean(losses[-50:]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: EVAL\n",
    "print(\"\\nFINAL EVAL\")\n",
    "for L in [128, 256, 512, 1024]:\n",
    "    n = needle_test(model, tokenizer, L, 50, device=DEVICE)\n",
    "    print(f\"  NIAH@{L}: {n['ratio']:.2f}x\")\n",
    "probe_layers(model, tokenizer, device=DEVICE)\n",
    "print(f\"\\nLM: {'PASS' if np.mean(losses[:50])-np.mean(losses[-50:])>2 else 'MARGINAL'}\")\n",
    "print(f\"NIAH: {'PASS' if any(r>1 for _,r in niah_traj) else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: SAVE\n",
    "from datetime import datetime\n",
    "rid = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "path = f\"/content/drive/MyDrive/groundthink/colab-exports/v6_{rid}.pt\"\n",
    "torch.save({'state': model.state_dict(), 'cfg': MODEL_CFG, 'losses': losses, 'niah': niah_traj}, path)\n",
    "print(f\"Saved: {path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {"gpuType": "A100"},
  "kernelspec": {"display_name": "Python 3", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
