{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroundThink V6 - Hybrid GatedDeltaNet + SWAttention\n",
    "\n",
    "**Architecture:** Uses FLA library's `GatedDeltaNet` and `SlidingWindowAttention` directly.\n",
    "\n",
    "**Key features:**\n",
    "- FLA kernels (NVIDIA recommended)\n",
    "- vLLM IsHybrid protocol ready (state management, introspection methods)\n",
    "- Configurable layer pattern via `attn_interval`\n",
    "- Gradient checkpointing on SWA layers\n",
    "- NIAH testing + gradient monitoring from V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3057673335.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# CELL 0: ENVIRONMENT SETUP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q flash-linear-attention'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "# CELL 0: ENVIRONMENT SETUP\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install -q flash-linear-attention\n",
    "!pip install -q datasets transformers\n",
    "\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GROUNDTHINK V6 CONFIG\n",
      "============================================================\n",
      "d_model:       256\n",
      "n_layers:      12\n",
      "attn_interval: 4 -> SWA at [3, 7, 11]\n",
      "window_size:   2048\n",
      "batch:         4\n",
      "steps:         10000\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: CONFIGURATION\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = 50257\n",
    "    d_model: int = 256\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 8\n",
    "    head_dim: int = 64\n",
    "    attn_interval: int = 4  # SWA every N layers (4 = 3:1 ratio)\n",
    "    window_size: int = 2048\n",
    "    expand_k: float = 1.0\n",
    "    expand_v: float = 2.0\n",
    "    max_seq_len: int = 2048\n",
    "    use_gradient_checkpointing: bool = True\n",
    "    tie_weights: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        self.head_dim = self.d_model // self.n_heads\n",
    "        \n",
    "    def get_swa_layer_indices(self) -> List[int]:\n",
    "        return [i for i in range(self.n_layers) \n",
    "                if i % self.attn_interval == (self.attn_interval - 1)]\n",
    "\n",
    "@dataclass \n",
    "class TrainConfig:\n",
    "    dataset_name: str = \"HuggingFaceFW/fineweb-edu\"\n",
    "    dataset_subset: str = \"sample-10BT\"\n",
    "    target_tokens: int = 20_000_000\n",
    "    batch_size: int = 2\n",
    "    seq_len: int = 512\n",
    "    accum_steps: int = 2\n",
    "    steps: int = 10000\n",
    "    warmup_ratio: float = 0.1\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.1\n",
    "    grad_clip: float = 1.0\n",
    "    betas: tuple = (0.9, 0.95)\n",
    "    log_interval: int = 50\n",
    "    grad_log_interval: int = 500\n",
    "    niah_checkpoints: List[int] = field(default_factory=lambda: [\n",
    "        500, 1000, 2000, 3000, 5000, 7500, 10000\n",
    "    ])\n",
    "    \n",
    "    @property\n",
    "    def warmup_steps(self) -> int:\n",
    "        return int(self.steps * self.warmup_ratio)\n",
    "    \n",
    "    @property\n",
    "    def effective_batch_size(self) -> int:\n",
    "        return self.batch_size * self.accum_steps\n",
    "\n",
    "MODEL_CFG = ModelConfig()\n",
    "TRAIN_CFG = TrainConfig()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GROUNDTHINK V6 CONFIG\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"d_model:       {MODEL_CFG.d_model}\")\n",
    "print(f\"n_layers:      {MODEL_CFG.n_layers}\")\n",
    "print(f\"attn_interval: {MODEL_CFG.attn_interval} -> SWA at {MODEL_CFG.get_swa_layer_indices()}\")\n",
    "print(f\"window_size:   {MODEL_CFG.window_size}\")\n",
    "print(f\"batch:         {TRAIN_CFG.effective_batch_size}\")\n",
    "print(f\"steps:         {TRAIN_CFG.steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fla'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1718625335.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_tf32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGatedDeltaNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSlidingWindowAttention\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mFLA_SWA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FLA loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fla'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: IMPORTS & DEVICE\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {props.name} ({props.total_memory/1e9:.1f}GB)\")\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "from fla.layers import GatedDeltaNet\n",
    "from fla.layers import SlidingWindowAttention as FLA_SWA\n",
    "print(\"FLA loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: MODEL COMPONENTS\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = x.float().pow(2).mean(-1, keepdim=True).add(self.eps).rsqrt()\n",
    "        return (x.float() * norm).type_as(x) * self.weight\n",
    "\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, d_model: int, expansion: float = 8/3):\n",
    "        super().__init__()\n",
    "        hidden = int(d_model * expansion)\n",
    "        hidden = ((hidden + 63) // 64) * 64\n",
    "        self.w1 = nn.Linear(d_model, hidden, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, hidden, bias=False)\n",
    "        self.w2 = nn.Linear(hidden, d_model, bias=False)\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        nn.init.normal_(self.w1.weight, std=0.02)\n",
    "        nn.init.normal_(self.w3.weight, std=0.02)\n",
    "        nn.init.normal_(self.w2.weight, std=0.02 / math.sqrt(2))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        return residual + self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class HybridBlock(nn.Module):\n",
    "    \"\"\"GatedDeltaNet or SWA with vLLM IsHybrid state support.\"\"\"\n",
    "    def __init__(self, d_model: int, is_attention: bool, n_heads: int = 8,\n",
    "                 window_size: int = 2048, expand_k: float = 1.0, \n",
    "                 expand_v: float = 2.0, layer_idx: int = 0):\n",
    "        super().__init__()\n",
    "        self.is_attention = is_attention\n",
    "        self.layer_idx = layer_idx\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        \n",
    "        if is_attention:\n",
    "            self.layer = FLA_SWA(\n",
    "                hidden_size=d_model, num_heads=n_heads,\n",
    "                window_size=window_size, layer_idx=layer_idx)\n",
    "        else:\n",
    "            self.layer = GatedDeltaNet(\n",
    "                hidden_size=d_model, expand_k=expand_k,\n",
    "                expand_v=expand_v, layer_idx=layer_idx)\n",
    "    \n",
    "    def forward(self, x, attention_mask=None, past_state=None, use_cache=False):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        new_state = None\n",
    "        \n",
    "        if use_cache:\n",
    "            if self.is_attention:\n",
    "                x, new_state = self.layer(x, attention_mask=attention_mask,\n",
    "                                          past_key_values=past_state, use_cache=True)\n",
    "            else:\n",
    "                x, new_state = self.layer(x, past_state=past_state, use_cache=True)\n",
    "        else:\n",
    "            if self.is_attention and attention_mask is not None:\n",
    "                x = self.layer(x, attention_mask=attention_mask)\n",
    "            else:\n",
    "                x = self.layer(x)\n",
    "        \n",
    "        return residual + x, new_state\n",
    "\n",
    "\n",
    "class GroundThinkLM(nn.Module):\n",
    "    \"\"\"Hybrid LM with vLLM IsHybrid protocol support.\"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        nn.init.normal_(self.embed.weight, std=0.02)\n",
    "        \n",
    "        swa_indices = set(cfg.get_swa_layer_indices())\n",
    "        self._swa_indices = swa_indices\n",
    "        self._gdn_indices = set(range(cfg.n_layers)) - swa_indices\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        \n",
    "        for i in range(cfg.n_layers):\n",
    "            is_attn = i in swa_indices\n",
    "            self.blocks.append(HybridBlock(\n",
    "                d_model=cfg.d_model, is_attention=is_attn,\n",
    "                n_heads=cfg.n_heads, window_size=cfg.window_size,\n",
    "                expand_k=cfg.expand_k, expand_v=cfg.expand_v, layer_idx=i))\n",
    "            self.ffns.append(SwiGLUFFN(cfg.d_model))\n",
    "        \n",
    "        self.norm_f = RMSNorm(cfg.d_model)\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "        if cfg.tie_weights:\n",
    "            self.lm_head.weight = self.embed.weight\n",
    "        \n",
    "    def forward(self, input_ids, targets=None, attention_mask=None,\n",
    "                past_states=None, use_cache=False):\n",
    "        x = self.embed(input_ids)\n",
    "        new_states = [] if use_cache else None\n",
    "        \n",
    "        for i, (block, ffn) in enumerate(zip(self.blocks, self.ffns)):\n",
    "            past_state = past_states[i] if past_states else None\n",
    "            \n",
    "            if self.cfg.use_gradient_checkpointing and self.training and i in self._swa_indices:\n",
    "                x = checkpoint(self._forward_block_train, block, ffn, x, attention_mask,\n",
    "                               use_reentrant=False)\n",
    "            else:\n",
    "                x, state = block(x, attention_mask, past_state, use_cache)\n",
    "                x = ffn(x)\n",
    "                if use_cache:\n",
    "                    new_states.append(state)\n",
    "        \n",
    "        x = self.norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss, new_states\n",
    "    \n",
    "    @staticmethod\n",
    "    def _forward_block_train(block, ffn, x, attention_mask):\n",
    "        x, _ = block(x, attention_mask, None, False)\n",
    "        x = ffn(x)\n",
    "        return x\n",
    "    \n",
    "    # vLLM IsHybrid protocol\n",
    "    @classmethod\n",
    "    def get_state_dtype_from_config(cls, config):\n",
    "        return {'gdn': torch.bfloat16, 'swa': torch.bfloat16}\n",
    "    \n",
    "    @classmethod\n",
    "    def get_state_shape_from_config(cls, config, batch_size=1):\n",
    "        head_dim = config.d_model // config.n_heads\n",
    "        gdn_state_dim = int(config.d_model * config.expand_v)\n",
    "        return {\n",
    "            'gdn': (batch_size, gdn_state_dim),\n",
    "            'swa_k': (batch_size, config.n_heads, config.window_size, head_dim),\n",
    "            'swa_v': (batch_size, config.n_heads, config.window_size, head_dim),\n",
    "        }\n",
    "    \n",
    "    def get_layer_types(self):\n",
    "        return ['swa' if i in self._swa_indices else 'gdn' for i in range(self.cfg.n_layers)]\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        counts = {'embed': sum(p.numel() for p in self.embed.parameters()),\n",
    "                  'gdn': 0, 'swa': 0, 'ffn': 0}\n",
    "        for i, (block, ffn) in enumerate(zip(self.blocks, self.ffns)):\n",
    "            bp = sum(p.numel() for p in block.parameters())\n",
    "            fp = sum(p.numel() for p in ffn.parameters())\n",
    "            if i in self._swa_indices:\n",
    "                counts['swa'] += bp\n",
    "            else:\n",
    "                counts['gdn'] += bp\n",
    "            counts['ffn'] += fp\n",
    "        counts['total'] = sum(counts.values())\n",
    "        return counts\n",
    "\n",
    "print(\"Model components defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: MONITORING\n",
    "\n",
    "def print_gradient_summary(model):\n",
    "    agg = {'embed': [], 'gdn': [], 'swa': [], 'ffn': [], 'norm': []}\n",
    "    swa_idx = model._swa_indices\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is None:\n",
    "            continue\n",
    "        norm = param.grad.norm().item()\n",
    "        if 'embed' in name:\n",
    "            agg['embed'].append(norm)\n",
    "        elif 'norm' in name:\n",
    "            agg['norm'].append(norm)\n",
    "        elif 'ffn' in name:\n",
    "            agg['ffn'].append(norm)\n",
    "        elif 'blocks' in name:\n",
    "            try:\n",
    "                idx = int(name.split('.')[1])\n",
    "                if idx in swa_idx:\n",
    "                    agg['swa'].append(norm)\n",
    "                else:\n",
    "                    agg['gdn'].append(norm)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(\"\\nGradient Norms:\")\n",
    "    for comp, vals in agg.items():\n",
    "        if vals:\n",
    "            m, mx = np.mean(vals), np.max(vals)\n",
    "            flag = \" WARNING\" if mx > 5.0 else \"\"\n",
    "            print(f\"  {comp:<6} mean={m:6.3f} max={mx:6.2f}{flag}\")\n",
    "\n",
    "\n",
    "def needle_test(model, tokenizer, seq_len=512, n_trials=50, needle_token=50250, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    random_chance = 1.0 / tokenizer.vocab_size\n",
    "    probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_trials):\n",
    "            tokens = torch.randint(1000, 10000, (1, seq_len), device=device)\n",
    "            pos = torch.randint(64, seq_len - 64, (1,)).item()\n",
    "            tokens[0, pos] = needle_token\n",
    "            \n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits, _, _ = model(tokens)\n",
    "            \n",
    "            p = F.softmax(logits[0, -1].float(), dim=-1)[needle_token].item()\n",
    "            probs.append(p)\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(probs), 'std': np.std(probs),\n",
    "        'random_chance': random_chance, 'ratio': np.mean(probs) / random_chance\n",
    "    }\n",
    "\n",
    "\n",
    "def probe_layer_representations(model, tokenizer, needle_id=50250, seq_len=512, \n",
    "                                 needle_pos=256, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    tokens = torch.randint(1000, 10000, (1, seq_len), device=device)\n",
    "    tokens[0, needle_pos] = needle_id\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x = model.embed(tokens)\n",
    "        needle_embed = model.embed.weight[needle_id].float()\n",
    "        \n",
    "        print(f\"\\nNeedle ({needle_id}) representation through layers:\")\n",
    "        swa_idx = model._swa_indices\n",
    "        \n",
    "        for i, (block, ffn) in enumerate(zip(model.blocks, model.ffns)):\n",
    "            x, _ = block(x, None, None, False)\n",
    "            x = ffn(x)\n",
    "            \n",
    "            sim = F.cosine_similarity(x[0, needle_pos].float(), needle_embed, dim=0).item()\n",
    "            layer_type = \"SWA\" if i in swa_idx else \"GDN\"\n",
    "            bar = \"#\" * int(max(0, (sim + 1) * 10))\n",
    "            print(f\"  L{i:2d} [{layer_type}]: {sim:+.3f} {bar}\")\n",
    "\n",
    "print(\"Monitoring functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: DATA LOADING\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "MODEL_CFG.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(f\"Streaming: {TRAIN_CFG.dataset_name}/{TRAIN_CFG.dataset_subset}\")\n",
    "dataset = load_dataset(TRAIN_CFG.dataset_name, name=TRAIN_CFG.dataset_subset, \n",
    "                       split=\"train\", streaming=True)\n",
    "\n",
    "token_buffer = []\n",
    "pbar = tqdm(total=TRAIN_CFG.target_tokens, desc=\"Tokenizing\", unit=\"tok\")\n",
    "\n",
    "for example in dataset:\n",
    "    tokens = tokenizer.encode(example['text']) + [tokenizer.eos_token_id]\n",
    "    token_buffer.extend(tokens)\n",
    "    pbar.update(len(tokens))\n",
    "    if len(token_buffer) >= TRAIN_CFG.target_tokens:\n",
    "        break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "all_tokens = torch.tensor(token_buffer[:TRAIN_CFG.target_tokens], dtype=torch.long)\n",
    "del token_buffer, dataset\n",
    "import gc; gc.collect()\n",
    "\n",
    "print(f\"Loaded {len(all_tokens):,} tokens\")\n",
    "\n",
    "def get_batch(batch_size=TRAIN_CFG.batch_size, seq_len=TRAIN_CFG.seq_len):\n",
    "    ix = torch.randint(len(all_tokens) - seq_len - 1, (batch_size,))\n",
    "    x = torch.stack([all_tokens[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([all_tokens[i+1:i+seq_len+1] for i in ix])\n",
    "    return x.to(DEVICE), y.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: BUILD MODEL\n",
    "print(\"Building model...\")\n",
    "model = GroundThinkLM(MODEL_CFG).to(DEVICE).to(torch.bfloat16)\n",
    "\n",
    "params = model.count_parameters()\n",
    "print(f\"Parameters: {params['total']/1e6:.2f}M\")\n",
    "print(f\"  Embed: {params['embed']/1e6:.2f}M\")\n",
    "print(f\"  GDN:   {params['gdn']/1e6:.2f}M\")\n",
    "print(f\"  SWA:   {params['swa']/1e6:.2f}M\")\n",
    "print(f\"  FFN:   {params['ffn']/1e6:.2f}M\")\n",
    "print(f\"SWA at layers: {sorted(model._swa_indices)}\")\n",
    "print(f\"Layer types: {model.get_layer_types()}\")\n",
    "\n",
    "# Test\n",
    "print(\"\\nTesting forward/backward...\")\n",
    "x, y = get_batch()\n",
    "with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "    logits, loss, _ = model(x, y)\n",
    "loss.backward()\n",
    "print(f\"Forward: loss={loss.item():.4f}\")\n",
    "print(f\"Backward: OK\")\n",
    "print(f\"Peak memory: {torch.cuda.max_memory_allocated()/1e9:.2f}GB\")\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: TRAINING\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=TRAIN_CFG.lr,\n",
    "                               betas=TRAIN_CFG.betas, weight_decay=TRAIN_CFG.weight_decay)\n",
    "\n",
    "losses, grad_norms, niah_trajectory = [], [], []\n",
    "random_chance = 1.0 / tokenizer.vocab_size\n",
    "start = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"TRAINING ({TRAIN_CFG.steps} steps)\")\n",
    "print(f\"  Effective batch: {TRAIN_CFG.effective_batch_size}\")\n",
    "print(f\"  Tokens/step: {TRAIN_CFG.effective_batch_size * TRAIN_CFG.seq_len:,}\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for step in range(TRAIN_CFG.steps):\n",
    "    # LR schedule\n",
    "    if step < TRAIN_CFG.warmup_steps:\n",
    "        lr = TRAIN_CFG.lr * (step + 1) / TRAIN_CFG.warmup_steps\n",
    "    else:\n",
    "        progress = (step - TRAIN_CFG.warmup_steps) / (TRAIN_CFG.steps - TRAIN_CFG.warmup_steps)\n",
    "        lr = TRAIN_CFG.lr * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    \n",
    "    for pg in optimizer.param_groups:\n",
    "        pg['lr'] = lr\n",
    "    \n",
    "    # Accumulation\n",
    "    accum_loss = 0.0\n",
    "    for _ in range(TRAIN_CFG.accum_steps):\n",
    "        x, y = get_batch()\n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            _, loss, _ = model(x, y)\n",
    "            loss_scaled = loss / TRAIN_CFG.accum_steps\n",
    "        loss_scaled.backward()\n",
    "        accum_loss += loss.item()\n",
    "    \n",
    "    total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), TRAIN_CFG.grad_clip)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    avg_loss = accum_loss / TRAIN_CFG.accum_steps\n",
    "    losses.append(avg_loss)\n",
    "    grad_norms.append(total_norm.item())\n",
    "    \n",
    "    if step % TRAIN_CFG.log_interval == 0:\n",
    "        avg = np.mean(losses[-50:]) if len(losses) >= 50 else np.mean(losses)\n",
    "        elapsed = time.time() - start\n",
    "        tps = (step + 1) * TRAIN_CFG.effective_batch_size * TRAIN_CFG.seq_len / elapsed\n",
    "        print(f\"[{step:5d}/{TRAIN_CFG.steps}] loss={avg:.4f} | grad={total_norm.item():.3f} | lr={lr:.2e} | {tps:,.0f} tok/s\")\n",
    "    \n",
    "    if (step + 1) % TRAIN_CFG.grad_log_interval == 0:\n",
    "        print_gradient_summary(model)\n",
    "    \n",
    "    if (step + 1) in TRAIN_CFG.niah_checkpoints:\n",
    "        niah = needle_test(model, tokenizer, TRAIN_CFG.seq_len, n_trials=30, device=DEVICE)\n",
    "        ratio = niah['ratio']\n",
    "        niah_trajectory.append((step + 1, ratio))\n",
    "        status = \"PASS\" if ratio > 1.0 else \"MARGINAL\" if ratio > 0.5 else \"FAIL\"\n",
    "        print(f\"  >>> NIAH@{step+1}: {ratio:.2f}x random [{status}]\")\n",
    "        model.train()\n",
    "\n",
    "# Summary\n",
    "elapsed = time.time() - start\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Time: {elapsed/60:.1f} min\")\n",
    "print(f\"Speed: {TRAIN_CFG.steps * TRAIN_CFG.effective_batch_size * TRAIN_CFG.seq_len / elapsed:,.0f} tok/s\")\n",
    "print(f\"Initial loss: {np.mean(losses[:50]):.4f}\")\n",
    "print(f\"Final loss: {np.mean(losses[-50:]):.4f}\")\n",
    "print(f\"\\nNIAH Trajectory:\")\n",
    "for step, ratio in niah_trajectory:\n",
    "    bar = \"#\" * int(min(ratio * 5, 20))\n",
    "    print(f\"  {step:>6}: {ratio:.2f}x {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: FINAL EVALUATION\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nNeedle-in-a-Haystack:\")\n",
    "for length in [128, 256, 512, 1024]:\n",
    "    niah = needle_test(model, tokenizer, length, n_trials=50, device=DEVICE)\n",
    "    status = \"PASS\" if niah['ratio'] > 1.0 else \"MARGINAL\" if niah['ratio'] > 0.5 else \"FAIL\"\n",
    "    print(f\"  NIAH@{length}: {niah['ratio']:.2f}x random [{status}] (P={niah['mean']:.2e})\")\n",
    "\n",
    "probe_layer_representations(model, tokenizer, device=DEVICE)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "loss_drop = np.mean(losses[:50]) - np.mean(losses[-50:])\n",
    "niah_pass = any(r > 1.0 for _, r in niah_trajectory)\n",
    "\n",
    "print(f\"Language Modeling: {'PASS' if loss_drop > 2.0 else 'MARGINAL'} (drop={loss_drop:.2f})\")\n",
    "print(f\"NIAH Retrieval: {'PASS' if niah_pass else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: EXPORT\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "export_dir = \"/content/drive/MyDrive/groundthink/colab-exports\"\n",
    "\n",
    "# Checkpoint\n",
    "ckpt_path = f\"{export_dir}/v6_{run_id}.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': MODEL_CFG,\n",
    "    'train_config': TRAIN_CFG,\n",
    "    'losses': losses,\n",
    "    'niah_trajectory': niah_trajectory,\n",
    "}, ckpt_path)\n",
    "print(f\"Checkpoint: {ckpt_path}\")\n",
    "\n",
    "# CSV\n",
    "csv_path = f\"{export_dir}/v6_{run_id}.csv\"\n",
    "final_niah = needle_test(model, tokenizer, 512, n_trials=50, device=DEVICE)\n",
    "\n",
    "with open(csv_path, 'w', newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['run_id', 'd_model', 'n_layers', 'attn_interval', 'window_size',\n",
    "                'seq_len', 'steps', 'initial_loss', 'final_loss', 'niah_ratio'])\n",
    "    w.writerow([run_id, MODEL_CFG.d_model, MODEL_CFG.n_layers, MODEL_CFG.attn_interval,\n",
    "                MODEL_CFG.window_size, TRAIN_CFG.seq_len, TRAIN_CFG.steps,\n",
    "                f\"{np.mean(losses[:50]):.4f}\", f\"{np.mean(losses[-50:]):.4f}\",\n",
    "                f\"{final_niah['ratio']:.2f}x\"])\n",
    "print(f\"Results: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
