{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114a5d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Building TransparentHybrid\n",
      "============================================================\n",
      "Architecture: GS\n",
      "  GDN layers: [0]\n",
      "  SWA layers: [1]\n",
      "Parameters: 32.71M (GDN: 1.58M, SWA: 1.07M)\n",
      "\n",
      "Forward pass:\n",
      "  Logits: torch.Size([1, 128, 50257])\n",
      "  Loss: 10.9375 (expected ~10.82)\n",
      "  Final state: torch.Size([1, 8, 64, 128])\n",
      "\n",
      "Per-layer diagnostics:\n",
      "  Layer 0 [GDN]: {'beta_mean': 0.50390625, 'beta_std': 0.134765625, 'g_mean': 0.49609375, 'g_std': 0.1337890625, 'state_norm': 10.199555397033691, 'state_shape': (1, 8, 64, 128), 'layer_type': 'GDN', 'layer_idx': 0}\n",
      "  Layer 1 [SWA]: {'local_attn_entropy': 3.828125, 'gate_mean': 0.5, 'gate_std': 0.0, 'global_attn_entropy': 4.15625, 'layer_type': 'SWA', 'layer_idx': 1}\n"
     ]
    }
   ],
   "source": [
    "# CELL 0: Transparent Hybrid Architecture with Diagnostics\n",
    "# Uses raw fla.ops, exposes all state, configurable layers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fla.ops.gated_delta_rule import chunk_gated_delta_rule\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Literal\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class HybridConfig:\n",
    "    \"\"\"Fully configurable - start small, scale up\"\"\"\n",
    "    d_model: int = 256\n",
    "    n_heads: int = 8\n",
    "    head_dim: int = 32        # d_model // n_heads\n",
    "    expand_v: float = 2.0     # value expansion (GDN default)\n",
    "    vocab_size: int = 50257\n",
    "    \n",
    "    # Layer pattern: 'G' = GDN, 'S' = SWA\n",
    "    # Examples: \"GS\", \"GGS\", \"GGSG\", \"GGGSGGGS\"\n",
    "    layer_pattern: str = \"GS\"\n",
    "    \n",
    "    # SWA config\n",
    "    window_size: int = 1024\n",
    "    \n",
    "    # Init\n",
    "    init_std: float = 0.02\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.head_dim = self.d_model // self.n_heads\n",
    "        self.value_dim = int(self.head_dim * self.expand_v)\n",
    "        \n",
    "    @property\n",
    "    def n_layers(self):\n",
    "        return len(self.layer_pattern)\n",
    "    \n",
    "    def layer_type(self, idx: int) -> str:\n",
    "        return self.layer_pattern[idx]\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = x.float().pow(2).mean(-1, keepdim=True).add(self.eps).rsqrt()\n",
    "        return (x.float() * norm).type_as(x) * self.weight\n",
    "\n",
    "\n",
    "class GatedDeltaNetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transparent GDN using raw op.\n",
    "    \n",
    "    Delta Rule: Sₜ = αₜ * Sₜ₋₁ + βₜ * (vₜ ⊗ kₜ)\n",
    "    - αₜ (gate g): controls forgetting (in log space)\n",
    "    - βₜ (beta): controls write strength\n",
    "    - Sₜ: state matrix [B, H, K, V] - the memory\n",
    "    \n",
    "    Output: Sₜ @ qₜ (query the memory)\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: HybridConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(cfg.d_model, cfg.n_heads * cfg.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(cfg.d_model, cfg.n_heads * cfg.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(cfg.d_model, cfg.n_heads * cfg.value_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(cfg.n_heads * cfg.value_dim, cfg.d_model, bias=False)\n",
    "        \n",
    "        # Gate projections (per-head scalars)\n",
    "        self.beta_proj = nn.Linear(cfg.d_model, cfg.n_heads, bias=False)  # write strength\n",
    "        self.g_proj = nn.Linear(cfg.d_model, cfg.n_heads, bias=False)     # forget gate (log space)\n",
    "        \n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, \n",
    "                initial_state: Optional[torch.Tensor] = None,\n",
    "                output_state: bool = True) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, T, D]\n",
    "            initial_state: [B, H, K, V] or None\n",
    "            output_state: whether to return final state\n",
    "            \n",
    "        Returns:\n",
    "            output: [B, T, D]\n",
    "            state: [B, H, K, V] if output_state else None\n",
    "            diagnostics: dict with internal values for inspection\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        H, K, V = self.cfg.n_heads, self.cfg.head_dim, self.cfg.value_dim\n",
    "        \n",
    "        # Pre-norm\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        # Project to q, k, v\n",
    "        q = self.q_proj(x_norm).view(B, T, H, K)\n",
    "        k = self.k_proj(x_norm).view(B, T, H, K)\n",
    "        v = self.v_proj(x_norm).view(B, T, H, V)\n",
    "        \n",
    "        # Normalize k for stability (as per FLA convention)\n",
    "        k = F.normalize(k.float(), p=2, dim=-1).to(x.dtype)\n",
    "        \n",
    "        # Gates\n",
    "        beta = self.beta_proj(x_norm).sigmoid()  # [B, T, H] write strength ∈ (0,1)\n",
    "        g = F.logsigmoid(self.g_proj(x_norm))    # [B, T, H] forget gate in log space\n",
    "        \n",
    "        # Core delta rule op\n",
    "        output, state = chunk_gated_delta_rule(\n",
    "            q, k, v, g, beta,\n",
    "            initial_state=initial_state,\n",
    "            output_final_state=output_state\n",
    "        )\n",
    "        \n",
    "        # Project back\n",
    "        output = output.reshape(B, T, H * V)\n",
    "        output = self.o_proj(output)\n",
    "        \n",
    "        # Residual\n",
    "        output = x + output\n",
    "        \n",
    "        # Diagnostics for understanding\n",
    "        diagnostics = {\n",
    "            'beta_mean': beta.mean().item(),      # avg write strength\n",
    "            'beta_std': beta.std().item(),\n",
    "            'g_mean': g.exp().mean().item(),      # avg forget rate (converted from log)\n",
    "            'g_std': g.exp().std().item(),\n",
    "            'state_norm': state.norm().item() if state is not None else 0,\n",
    "            'state_shape': tuple(state.shape) if state is not None else None,\n",
    "        }\n",
    "        \n",
    "        return output, state, diagnostics\n",
    "\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    SWA with dual attention:\n",
    "    1. Local attention (within window)\n",
    "    2. Global cross-attention to GDN state (the memory bank)\n",
    "    \n",
    "    The model learns WHEN to use local vs global via a gate.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: HybridConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.layer_idx = layer_idx\n",
    "        H, K, V = cfg.n_heads, cfg.head_dim, cfg.value_dim\n",
    "        \n",
    "        # Standard attention projections (for local)\n",
    "        self.q_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        \n",
    "        # Cross-attention to GDN state\n",
    "        # State is [B, H, K, V] - K \"memory slots\", each with V-dim value\n",
    "        # We need to create keys from state and use state as values\n",
    "        self.state_k_proj = nn.Linear(V, K, bias=False)  # V -> K to create keys from state values\n",
    "        self.state_v_proj = nn.Linear(V, K, bias=False)  # V -> K to match output dim\n",
    "        \n",
    "        # Learnable gate: decides local vs global per position\n",
    "        self.gate_proj = nn.Linear(cfg.d_model, H, bias=True)  # per-head gate\n",
    "        nn.init.zeros_(self.gate_proj.weight)  # start with gate ≈ 0.5\n",
    "        nn.init.zeros_(self.gate_proj.bias)\n",
    "        \n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        self.scale = K ** -0.5\n",
    "        \n",
    "    def forward(self, x: torch.Tensor,\n",
    "                gdn_state: Optional[torch.Tensor] = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, T, D]\n",
    "            gdn_state: [B, H, K, V] - accumulated state from GDN layers\n",
    "            \n",
    "        Returns:\n",
    "            output: [B, T, D]\n",
    "            diagnostics: dict\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        H = self.cfg.n_heads\n",
    "        K = self.cfg.head_dim\n",
    "        V = self.cfg.value_dim\n",
    "        W = self.cfg.window_size\n",
    "        \n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        # Project queries, keys, values for LOCAL attention\n",
    "        q = self.q_proj(x_norm).view(B, T, H, K).transpose(1, 2)  # [B, H, T, K]\n",
    "        k_local = self.k_proj(x_norm).view(B, T, H, K).transpose(1, 2)\n",
    "        v_local = self.v_proj(x_norm).view(B, T, H, K).transpose(1, 2)\n",
    "        \n",
    "        # === LOCAL ATTENTION (sliding window) ===\n",
    "        # Sliding window mask: causal + limited lookback\n",
    "        mask = torch.ones(T, T, device=x.device, dtype=torch.bool)\n",
    "        mask = mask.triu(1) | mask.tril(-W - 1)\n",
    "        \n",
    "        attn_local = (q @ k_local.transpose(-2, -1)) * self.scale\n",
    "        attn_local = attn_local.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        attn_weights_local = F.softmax(attn_local, dim=-1)\n",
    "        local_out = attn_weights_local @ v_local  # [B, H, T, K]\n",
    "        \n",
    "        # === GLOBAL ATTENTION (cross-attention to GDN state) ===\n",
    "        global_out = torch.zeros_like(local_out)\n",
    "        gate_values = torch.zeros(B, H, T, 1, device=x.device)\n",
    "        \n",
    "        if gdn_state is not None:\n",
    "            # gdn_state: [B, H, K_state, V_state] where K_state=K, V_state=V\n",
    "            # Treat K_state dimension as \"memory slots\"\n",
    "            # Each slot has a V-dimensional embedding\n",
    "            \n",
    "            # Transpose state to [B, H, V, K] for easier manipulation\n",
    "            # Then we have V \"memory slots\" each with K-dim key\n",
    "            # Actually, let's think about this more carefully:\n",
    "            # State is [B, H, K, V] - this is like H heads, each with a KxV matrix\n",
    "            # We can treat rows (K dim) as keys and columns (V dim) as values per key\n",
    "            \n",
    "            # Create keys from state: [B, H, K, V] -> use K as num_slots, project V to key_dim\n",
    "            # k_global: [B, H, num_slots, key_dim]\n",
    "            state_for_attn = gdn_state.to(q.dtype)  # [B, H, K, V]\n",
    "            num_slots = state_for_attn.shape[2]  # K dimension = number of memory slots\n",
    "            \n",
    "            # Each \"slot\" is a V-dimensional vector, project to key space\n",
    "            k_global = self.state_k_proj(state_for_attn)  # [B, H, K, K] -> K slots with K-dim keys\n",
    "            v_global = self.state_v_proj(state_for_attn)  # [B, H, K, K] -> K slots with K-dim values\n",
    "            \n",
    "            # Cross-attention: query current positions against memory slots\n",
    "            # q: [B, H, T, K], k_global: [B, H, K, K] -> attn: [B, H, T, K]\n",
    "            attn_global = (q @ k_global.transpose(-2, -1)) * self.scale  # [B, H, T, K]\n",
    "            attn_weights_global = F.softmax(attn_global, dim=-1)  # [B, H, T, K]\n",
    "            global_out = attn_weights_global @ v_global  # [B, H, T, K]\n",
    "            \n",
    "            # Learnable gate: how much to use global vs local\n",
    "            # Gate is per-position, per-head\n",
    "            gate_logits = self.gate_proj(x_norm)  # [B, T, H]\n",
    "            gate = torch.sigmoid(gate_logits).transpose(1, 2).unsqueeze(-1)  # [B, H, T, 1]\n",
    "            gate_values = gate\n",
    "            \n",
    "            # Fused output: gate * global + (1 - gate) * local\n",
    "            out = gate * global_out + (1 - gate) * local_out\n",
    "        else:\n",
    "            out = local_out\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).reshape(B, T, D)\n",
    "        out = self.o_proj(out)\n",
    "        out = x + out\n",
    "        \n",
    "        diagnostics = {\n",
    "            'local_attn_entropy': -(attn_weights_local * attn_weights_local.clamp(min=1e-8).log()).sum(-1).mean().item(),\n",
    "            'gate_mean': gate_values.mean().item(),\n",
    "            'gate_std': gate_values.std().item(),\n",
    "            'global_attn_entropy': -(attn_weights_global * attn_weights_global.clamp(min=1e-8).log()).sum(-1).mean().item() if gdn_state is not None else 0,\n",
    "        }\n",
    "        \n",
    "        return out, diagnostics\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    \"\"\"Simple SwiGLU FFN\"\"\"\n",
    "    def __init__(self, cfg: HybridConfig):\n",
    "        super().__init__()\n",
    "        hidden = int(cfg.d_model * 8/3)\n",
    "        hidden = ((hidden + 63) // 64) * 64  # round to 64\n",
    "        \n",
    "        self.w1 = nn.Linear(cfg.d_model, hidden, bias=False)\n",
    "        self.w3 = nn.Linear(cfg.d_model, hidden, bias=False)\n",
    "        self.w2 = nn.Linear(hidden, cfg.d_model, bias=False)\n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.norm(x)\n",
    "        return x + self.w2(F.silu(self.w1(h)) * self.w3(h))\n",
    "\n",
    "\n",
    "class TransparentHybrid(nn.Module):\n",
    "    \"\"\"\n",
    "    Configurable GDN + SWA hybrid with full visibility.\n",
    "    \n",
    "    Key insight: GDN state Sₜ is a [H, K, V] matrix per batch.\n",
    "    - It accumulates information across the sequence\n",
    "    - SWA can query it to retrieve global context\n",
    "    - This is how a needle at pos 100 reaches output at pos 500\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: HybridConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Embedding\n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        nn.init.normal_(self.embed.weight, std=cfg.init_std)\n",
    "        \n",
    "        # Build layers according to pattern\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        \n",
    "        for i, layer_type in enumerate(cfg.layer_pattern):\n",
    "            if layer_type == 'G':\n",
    "                self.layers.append(GatedDeltaNetLayer(cfg, i))\n",
    "            elif layer_type == 'S':\n",
    "                self.layers.append(SlidingWindowAttention(cfg, i))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
    "            self.ffns.append(FFN(cfg))\n",
    "        \n",
    "        self.norm_f = RMSNorm(cfg.d_model)\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embed.weight  # tie weights\n",
    "        \n",
    "        # Track which layers are GDN for state accumulation\n",
    "        self.gdn_indices = [i for i, t in enumerate(cfg.layer_pattern) if t == 'G']\n",
    "        self.swa_indices = [i for i, t in enumerate(cfg.layer_pattern) if t == 'S']\n",
    "        \n",
    "        print(f\"Architecture: {cfg.layer_pattern}\")\n",
    "        print(f\"  GDN layers: {self.gdn_indices}\")\n",
    "        print(f\"  SWA layers: {self.swa_indices}\")\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, \n",
    "                targets: Optional[torch.Tensor] = None,\n",
    "                return_diagnostics: bool = False) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [B, T]\n",
    "            targets: [B, T] for loss computation\n",
    "            return_diagnostics: whether to return per-layer diagnostics\n",
    "            \n",
    "        Returns:\n",
    "            logits: [B, T, V]\n",
    "            loss: scalar if targets provided\n",
    "            diagnostics: dict if return_diagnostics\n",
    "        \"\"\"\n",
    "        x = self.embed(input_ids)\n",
    "        \n",
    "        # Track GDN state - accumulate across GDN layers\n",
    "        accumulated_state = None\n",
    "        all_diagnostics = []\n",
    "        \n",
    "        for i, (layer, ffn) in enumerate(zip(self.layers, self.ffns)):\n",
    "            layer_type = self.cfg.layer_pattern[i]\n",
    "            \n",
    "            if layer_type == 'G':\n",
    "                x, state, diag = layer(x, initial_state=accumulated_state, output_state=True)\n",
    "                # Accumulate state (could also replace - design choice)\n",
    "                if accumulated_state is None:\n",
    "                    accumulated_state = state\n",
    "                else:\n",
    "                    # Weighted combination - newer state more important\n",
    "                    accumulated_state = 0.5 * accumulated_state + 0.5 * state\n",
    "                diag['layer_type'] = 'GDN'\n",
    "                \n",
    "            elif layer_type == 'S':\n",
    "                x, diag = layer(x, gdn_state=accumulated_state)\n",
    "                diag['layer_type'] = 'SWA'\n",
    "            \n",
    "            x = ffn(x)\n",
    "            diag['layer_idx'] = i\n",
    "            all_diagnostics.append(diag)\n",
    "        \n",
    "        x = self.norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        if return_diagnostics:\n",
    "            return logits, loss, all_diagnostics, accumulated_state\n",
    "        return logits, loss, None, None\n",
    "    \n",
    "    def probe_state(self, input_ids: torch.Tensor, \n",
    "                    needle_pos: int,\n",
    "                    query_pos: int) -> dict:\n",
    "        \"\"\"\n",
    "        Diagnostic: Check if needle information is in the state.\n",
    "        \n",
    "        Returns analysis of whether the needle token at needle_pos\n",
    "        can be retrieved from state at query_pos.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            x = self.embed(input_ids)\n",
    "            accumulated_state = None\n",
    "            \n",
    "            results = {'per_layer': []}\n",
    "            \n",
    "            for i, (layer, ffn) in enumerate(zip(self.layers, self.ffns)):\n",
    "                layer_type = self.cfg.layer_pattern[i]\n",
    "                \n",
    "                if layer_type == 'G':\n",
    "                    x, state, _ = layer(x, initial_state=accumulated_state, output_state=True)\n",
    "                    \n",
    "                    if accumulated_state is None:\n",
    "                        accumulated_state = state\n",
    "                    else:\n",
    "                        accumulated_state = 0.5 * accumulated_state + 0.5 * state\n",
    "                    \n",
    "                    # Check: can we find needle in state?\n",
    "                    # Get needle's key representation\n",
    "                    needle_embed = self.embed.weight[input_ids[0, needle_pos]]\n",
    "                    \n",
    "                    # State is [B, H, K, V] - query it\n",
    "                    # A simple probe: project needle through k_proj, query state\n",
    "                    k_proj = layer.k_proj\n",
    "                    needle_key = k_proj(needle_embed).view(self.cfg.n_heads, self.cfg.head_dim)\n",
    "                    needle_key = F.normalize(needle_key.float(), p=2, dim=-1)\n",
    "                    \n",
    "                    # Query state: needle_key @ state -> retrieval\n",
    "                    # state: [1, H, K, V]\n",
    "                    retrieved = torch.einsum('hk,bhkv->bhv', needle_key, accumulated_state.float())\n",
    "                    \n",
    "                    results['per_layer'].append({\n",
    "                        'layer': i,\n",
    "                        'type': 'GDN',\n",
    "                        'state_norm': state.norm().item(),\n",
    "                        'retrieved_norm': retrieved.norm().item(),\n",
    "                    })\n",
    "                    \n",
    "                elif layer_type == 'S':\n",
    "                    x, _ = layer(x, gdn_state=accumulated_state)\n",
    "                    results['per_layer'].append({\n",
    "                        'layer': i,\n",
    "                        'type': 'SWA',\n",
    "                    })\n",
    "                \n",
    "                x = ffn(x)\n",
    "            \n",
    "            results['final_state_norm'] = accumulated_state.norm().item() if accumulated_state is not None else 0\n",
    "            \n",
    "        return results\n",
    "\n",
    "\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    gdn = sum(p.numel() for i, l in enumerate(model.layers) if model.cfg.layer_pattern[i] == 'G' for p in l.parameters())\n",
    "    swa = sum(p.numel() for i, l in enumerate(model.layers) if model.cfg.layer_pattern[i] == 'S' for p in l.parameters())\n",
    "    ffn = sum(p.numel() for f in model.ffns for p in f.parameters())\n",
    "    return {'total': total, 'gdn': gdn, 'swa': swa, 'ffn': ffn}\n",
    "\n",
    "\n",
    "# ============ TEST IT ============\n",
    "print(\"=\"*60)\n",
    "print(\"Building TransparentHybrid\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Start minimal\n",
    "cfg = HybridConfig(\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    layer_pattern=\"GS\",  # Just 1 GDN + 1 SWA to understand\n",
    "    window_size=128,\n",
    ")\n",
    "\n",
    "model = TransparentHybrid(cfg).cuda().bfloat16()\n",
    "params = count_params(model)\n",
    "print(f\"Parameters: {params['total']/1e6:.2f}M (GDN: {params['gdn']/1e6:.2f}M, SWA: {params['swa']/1e6:.2f}M)\")\n",
    "\n",
    "# Test forward\n",
    "x = torch.randint(0, 1000, (1, 128), device='cuda')\n",
    "y = torch.randint(0, 1000, (1, 128), device='cuda')\n",
    "\n",
    "logits, loss, diagnostics, state = model(x, y, return_diagnostics=True)\n",
    "print(f\"\\nForward pass:\")\n",
    "print(f\"  Logits: {logits.shape}\")\n",
    "print(f\"  Loss: {loss.item():.4f} (expected ~{math.log(cfg.vocab_size):.2f})\")\n",
    "print(f\"  Final state: {state.shape if state is not None else None}\")\n",
    "\n",
    "print(f\"\\nPer-layer diagnostics:\")\n",
    "for d in diagnostics:\n",
    "    print(f\"  Layer {d['layer_idx']} [{d['layer_type']}]: {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877370fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIAH (untrained): 0.9423x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "\n",
      "============================================================\n",
      "Probing state for needle information\n",
      "============================================================\n",
      "Final state norm: 11.2189\n",
      "  {'layer': 0, 'type': 'GDN', 'state_norm': 11.218931198120117, 'retrieved_norm': 2.0429799556732178}\n",
      "  {'layer': 1, 'type': 'SWA'}\n"
     ]
    }
   ],
   "source": [
    "# 1. NIAH test - does the needle actually get stored and retrieved?\n",
    "def simple_niah(model, seq_len=128, needle_pos=32, n_trials=20):\n",
    "    \"\"\"Put a rare token early, see if model predicts it at the end\"\"\"\n",
    "    model.eval()\n",
    "    needle_token = 50000  # rare token\n",
    "    \n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_trials):\n",
    "            # Random tokens with needle inserted\n",
    "            tokens = torch.randint(1000, 10000, (1, seq_len), device='cuda')\n",
    "            tokens[0, needle_pos] = needle_token\n",
    "            \n",
    "            logits, _, diags, state = model(tokens, return_diagnostics=True)\n",
    "            \n",
    "            # Check: does the final position predict the needle?\n",
    "            final_probs = F.softmax(logits[0, -1].float(), dim=-1)\n",
    "            needle_prob = final_probs[needle_token].item()\n",
    "            random_baseline = 1.0 / model.cfg.vocab_size\n",
    "            \n",
    "            results.append({\n",
    "                'needle_prob': needle_prob,\n",
    "                'ratio': needle_prob / random_baseline,\n",
    "                'state_norm': state.norm().item(),\n",
    "            })\n",
    "    \n",
    "    avg_ratio = sum(r['ratio'] for r in results) / len(results)\n",
    "    print(f\"NIAH (untrained): {avg_ratio:.4f}x random\")\n",
    "    print(f\"  (>1.0 means model finds needle, <1.0 means no retrieval)\")\n",
    "    return results\n",
    "\n",
    "# Test before training\n",
    "niah_results = simple_niah(model)\n",
    "\n",
    "# 2. Probe: where is needle info stored?\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Probing state for needle information\")\n",
    "print(\"=\"*60)\n",
    "tokens = torch.randint(1000, 10000, (1, 128), device='cuda')\n",
    "tokens[0, 32] = 50000  # needle at pos 32\n",
    "\n",
    "probe = model.probe_state(tokens, needle_pos=32, query_pos=127)\n",
    "print(f\"Final state norm: {probe['final_state_norm']:.4f}\")\n",
    "for layer in probe['per_layer']:\n",
    "    print(f\"  {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04270fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1055 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2,000,000 tokens\n",
      "\n",
      "Training 20000 steps, batch=4, seq_len=128\n",
      "============================================================\n",
      "[   0] loss=10.938 lr=1.50e-06 | β=0.500 g=0.498 state=22.9 gate=0.50 | 739 tok/s\n",
      "[ 100] loss=9.341 lr=1.51e-04 | β=0.500 g=0.490 state=22.3 gate=0.53 | 14,688 tok/s\n",
      "[ 200] loss=7.491 lr=3.00e-04 | β=0.494 g=0.482 state=21.8 gate=0.62 | 16,599 tok/s\n",
      "[ 300] loss=7.288 lr=3.00e-04 | β=0.498 g=0.486 state=21.0 gate=0.64 | 17,201 tok/s\n",
      "[ 400] loss=7.120 lr=3.00e-04 | β=0.498 g=0.496 state=20.6 gate=0.65 | 17,655 tok/s\n",
      "NIAH (untrained): 0.0171x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@500: 0.02x random [FAIL]\n",
      "[ 500] loss=7.087 lr=3.00e-04 | β=0.479 g=0.512 state=20.9 gate=0.66 | 17,672 tok/s\n",
      "[ 600] loss=6.914 lr=3.00e-04 | β=0.482 g=0.496 state=19.6 gate=0.69 | 17,244 tok/s\n",
      "[ 700] loss=6.937 lr=3.00e-04 | β=0.473 g=0.498 state=19.8 gate=0.68 | 17,404 tok/s\n",
      "[ 800] loss=6.911 lr=2.99e-04 | β=0.469 g=0.492 state=22.0 gate=0.68 | 17,526 tok/s\n",
      "[ 900] loss=6.804 lr=2.99e-04 | β=0.494 g=0.504 state=19.4 gate=0.69 | 17,539 tok/s\n",
      "NIAH (untrained): 0.0883x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@1000: 0.09x random [FAIL]\n",
      "[1000] loss=6.801 lr=2.99e-04 | β=0.459 g=0.516 state=20.5 gate=0.69 | 17,560 tok/s\n",
      "[1100] loss=6.832 lr=2.98e-04 | β=0.455 g=0.504 state=19.6 gate=0.69 | 17,683 tok/s\n",
      "[1200] loss=6.734 lr=2.98e-04 | β=0.439 g=0.500 state=18.1 gate=0.71 | 17,747 tok/s\n",
      "[1300] loss=6.784 lr=2.98e-04 | β=0.424 g=0.494 state=18.4 gate=0.67 | 17,847 tok/s\n",
      "[1400] loss=6.726 lr=2.97e-04 | β=0.418 g=0.500 state=19.8 gate=0.65 | 17,924 tok/s\n",
      "NIAH (untrained): 0.0830x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@1500: 0.08x random [FAIL]\n",
      "[1500] loss=6.724 lr=2.97e-04 | β=0.410 g=0.512 state=20.6 gate=0.66 | 17,928 tok/s\n",
      "[1600] loss=6.668 lr=2.96e-04 | β=0.412 g=0.516 state=19.2 gate=0.67 | 17,988 tok/s\n",
      "[1700] loss=6.646 lr=2.96e-04 | β=0.422 g=0.527 state=18.4 gate=0.68 | 17,862 tok/s\n",
      "[1800] loss=6.654 lr=2.95e-04 | β=0.402 g=0.539 state=23.0 gate=0.68 | 17,920 tok/s\n",
      "[1900] loss=6.688 lr=2.95e-04 | β=0.393 g=0.531 state=13.7 gate=0.66 | 17,959 tok/s\n",
      "NIAH (untrained): 0.0997x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@2000: 0.10x random [FAIL]\n",
      "[2000] loss=6.661 lr=2.94e-04 | β=0.379 g=0.543 state=17.7 gate=0.66 | 17,969 tok/s\n",
      "[2100] loss=6.584 lr=2.93e-04 | β=0.377 g=0.543 state=19.2 gate=0.64 | 18,016 tok/s\n",
      "[2200] loss=6.593 lr=2.93e-04 | β=0.371 g=0.562 state=16.7 gate=0.64 | 18,063 tok/s\n",
      "[2300] loss=6.598 lr=2.92e-04 | β=0.367 g=0.555 state=20.4 gate=0.63 | 18,112 tok/s\n",
      "[2400] loss=6.536 lr=2.91e-04 | β=0.352 g=0.547 state=18.6 gate=0.66 | 18,156 tok/s\n",
      "NIAH (untrained): 0.1363x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@2500: 0.14x random [FAIL]\n",
      "[2500] loss=6.566 lr=2.90e-04 | β=0.344 g=0.527 state=15.2 gate=0.64 | 18,142 tok/s\n",
      "[2600] loss=6.590 lr=2.89e-04 | β=0.328 g=0.535 state=19.8 gate=0.63 | 18,171 tok/s\n",
      "[2700] loss=6.575 lr=2.88e-04 | β=0.334 g=0.551 state=16.6 gate=0.64 | 18,191 tok/s\n",
      "[2800] loss=6.514 lr=2.87e-04 | β=0.334 g=0.535 state=16.2 gate=0.63 | 18,226 tok/s\n",
      "[2900] loss=6.478 lr=2.86e-04 | β=0.299 g=0.539 state=18.5 gate=0.66 | 18,148 tok/s\n",
      "NIAH (untrained): 0.1503x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@3000: 0.15x random [FAIL]\n",
      "[3000] loss=6.452 lr=2.85e-04 | β=0.320 g=0.582 state=14.8 gate=0.61 | 18,142 tok/s\n",
      "[3100] loss=6.453 lr=2.84e-04 | β=0.318 g=0.574 state=17.1 gate=0.65 | 18,175 tok/s\n",
      "[3200] loss=6.488 lr=2.83e-04 | β=0.301 g=0.578 state=17.0 gate=0.65 | 18,192 tok/s\n",
      "[3300] loss=6.475 lr=2.82e-04 | β=0.299 g=0.570 state=18.7 gate=0.64 | 18,224 tok/s\n",
      "[3400] loss=6.461 lr=2.81e-04 | β=0.295 g=0.562 state=16.2 gate=0.62 | 18,243 tok/s\n",
      "NIAH (untrained): 0.1767x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@3500: 0.18x random [FAIL]\n",
      "[3500] loss=6.468 lr=2.80e-04 | β=0.297 g=0.555 state=14.3 gate=0.62 | 18,238 tok/s\n",
      "[3600] loss=6.474 lr=2.79e-04 | β=0.277 g=0.562 state=15.2 gate=0.61 | 18,270 tok/s\n",
      "[3700] loss=6.400 lr=2.77e-04 | β=0.271 g=0.562 state=11.8 gate=0.62 | 18,299 tok/s\n",
      "[3800] loss=6.346 lr=2.76e-04 | β=0.268 g=0.574 state=17.0 gate=0.63 | 18,318 tok/s\n",
      "[3900] loss=6.395 lr=2.75e-04 | β=0.262 g=0.578 state=13.2 gate=0.61 | 18,332 tok/s\n",
      "NIAH (untrained): 0.1160x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@4000: 0.12x random [FAIL]\n",
      "[4000] loss=6.421 lr=2.74e-04 | β=0.254 g=0.562 state=12.8 gate=0.59 | 18,269 tok/s\n",
      "[4100] loss=6.389 lr=2.72e-04 | β=0.252 g=0.570 state=17.3 gate=0.62 | 18,172 tok/s\n",
      "[4200] loss=6.436 lr=2.71e-04 | β=0.258 g=0.566 state=15.6 gate=0.61 | 18,189 tok/s\n",
      "[4300] loss=6.429 lr=2.69e-04 | β=0.246 g=0.562 state=14.8 gate=0.62 | 18,183 tok/s\n",
      "[4400] loss=6.251 lr=2.68e-04 | β=0.241 g=0.586 state=12.3 gate=0.62 | 18,188 tok/s\n",
      "NIAH (untrained): 0.1124x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@4500: 0.11x random [FAIL]\n",
      "[4500] loss=6.376 lr=2.66e-04 | β=0.245 g=0.574 state=12.7 gate=0.62 | 18,178 tok/s\n",
      "[4600] loss=6.365 lr=2.65e-04 | β=0.236 g=0.582 state=16.7 gate=0.62 | 18,195 tok/s\n",
      "[4700] loss=6.358 lr=2.63e-04 | β=0.221 g=0.566 state=15.7 gate=0.60 | 18,199 tok/s\n",
      "[4800] loss=6.438 lr=2.62e-04 | β=0.227 g=0.562 state=12.9 gate=0.61 | 18,208 tok/s\n",
      "[4900] loss=6.310 lr=2.60e-04 | β=0.226 g=0.574 state=10.5 gate=0.62 | 18,217 tok/s\n",
      "NIAH (untrained): 0.1452x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@5000: 0.15x random [FAIL]\n",
      "[5000] loss=6.377 lr=2.59e-04 | β=0.215 g=0.578 state=14.4 gate=0.62 | 18,210 tok/s\n",
      "[5100] loss=6.314 lr=2.57e-04 | β=0.213 g=0.566 state=12.4 gate=0.61 | 18,221 tok/s\n",
      "[5200] loss=6.349 lr=2.55e-04 | β=0.220 g=0.574 state=12.6 gate=0.61 | 18,230 tok/s\n",
      "[5300] loss=6.397 lr=2.54e-04 | β=0.217 g=0.582 state=14.1 gate=0.61 | 18,175 tok/s\n",
      "[5400] loss=6.341 lr=2.52e-04 | β=0.209 g=0.582 state=13.4 gate=0.61 | 18,185 tok/s\n",
      "NIAH (untrained): 0.1148x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@5500: 0.11x random [FAIL]\n",
      "[5500] loss=6.319 lr=2.50e-04 | β=0.212 g=0.570 state=12.7 gate=0.63 | 18,178 tok/s\n",
      "[5600] loss=6.300 lr=2.48e-04 | β=0.198 g=0.559 state=14.0 gate=0.60 | 18,187 tok/s\n",
      "[5700] loss=6.286 lr=2.46e-04 | β=0.207 g=0.574 state=15.0 gate=0.62 | 18,189 tok/s\n",
      "[5800] loss=6.371 lr=2.45e-04 | β=0.210 g=0.590 state=14.0 gate=0.59 | 18,202 tok/s\n",
      "[5900] loss=6.327 lr=2.43e-04 | β=0.194 g=0.578 state=11.2 gate=0.61 | 18,209 tok/s\n",
      "NIAH (untrained): 0.1026x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@6000: 0.10x random [FAIL]\n",
      "[6000] loss=6.340 lr=2.41e-04 | β=0.202 g=0.582 state=10.8 gate=0.61 | 18,201 tok/s\n",
      "[6100] loss=6.260 lr=2.39e-04 | β=0.191 g=0.582 state=11.7 gate=0.62 | 18,208 tok/s\n",
      "[6200] loss=6.223 lr=2.37e-04 | β=0.199 g=0.570 state=12.2 gate=0.62 | 18,215 tok/s\n",
      "[6300] loss=6.298 lr=2.35e-04 | β=0.192 g=0.574 state=12.8 gate=0.59 | 18,221 tok/s\n",
      "[6400] loss=6.209 lr=2.33e-04 | β=0.196 g=0.578 state=12.2 gate=0.61 | 18,173 tok/s\n",
      "NIAH (untrained): 0.1006x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@6500: 0.10x random [FAIL]\n",
      "[6500] loss=6.329 lr=2.31e-04 | β=0.190 g=0.570 state=9.1 gate=0.61 | 18,166 tok/s\n",
      "[6600] loss=6.278 lr=2.29e-04 | β=0.188 g=0.559 state=11.2 gate=0.62 | 18,174 tok/s\n",
      "[6700] loss=6.232 lr=2.27e-04 | β=0.193 g=0.566 state=7.7 gate=0.63 | 18,181 tok/s\n",
      "[6800] loss=6.209 lr=2.25e-04 | β=0.194 g=0.586 state=11.4 gate=0.60 | 18,184 tok/s\n",
      "[6900] loss=6.293 lr=2.23e-04 | β=0.194 g=0.570 state=10.1 gate=0.61 | 18,180 tok/s\n",
      "NIAH (untrained): 0.1775x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@7000: 0.18x random [FAIL]\n",
      "[7000] loss=6.249 lr=2.21e-04 | β=0.192 g=0.586 state=12.1 gate=0.61 | 18,163 tok/s\n",
      "[7100] loss=6.277 lr=2.19e-04 | β=0.186 g=0.574 state=11.3 gate=0.61 | 18,170 tok/s\n",
      "[7200] loss=6.196 lr=2.17e-04 | β=0.187 g=0.578 state=13.6 gate=0.60 | 18,178 tok/s\n",
      "[7300] loss=6.273 lr=2.14e-04 | β=0.178 g=0.559 state=10.2 gate=0.63 | 18,189 tok/s\n",
      "[7400] loss=6.202 lr=2.12e-04 | β=0.179 g=0.570 state=7.8 gate=0.61 | 18,198 tok/s\n",
      "NIAH (untrained): 0.2024x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@7500: 0.20x random [FAIL]\n",
      "[7500] loss=6.229 lr=2.10e-04 | β=0.183 g=0.566 state=9.1 gate=0.59 | 18,190 tok/s\n",
      "[7600] loss=6.244 lr=2.08e-04 | β=0.185 g=0.574 state=12.3 gate=0.61 | 18,151 tok/s\n",
      "[7700] loss=6.219 lr=2.06e-04 | β=0.188 g=0.578 state=12.1 gate=0.61 | 18,161 tok/s\n",
      "[7800] loss=6.262 lr=2.04e-04 | β=0.184 g=0.582 state=9.5 gate=0.58 | 18,167 tok/s\n",
      "[7900] loss=6.179 lr=2.01e-04 | β=0.187 g=0.586 state=11.0 gate=0.62 | 18,176 tok/s\n",
      "NIAH (untrained): 0.0913x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@8000: 0.09x random [FAIL]\n",
      "[8000] loss=6.199 lr=1.99e-04 | β=0.186 g=0.586 state=9.4 gate=0.61 | 18,169 tok/s\n",
      "[8100] loss=6.136 lr=1.97e-04 | β=0.180 g=0.562 state=12.2 gate=0.62 | 18,178 tok/s\n",
      "[8200] loss=6.230 lr=1.95e-04 | β=0.188 g=0.578 state=10.2 gate=0.59 | 18,184 tok/s\n",
      "[8300] loss=6.231 lr=1.92e-04 | β=0.180 g=0.574 state=9.2 gate=0.60 | 18,188 tok/s\n",
      "[8400] loss=6.146 lr=1.90e-04 | β=0.188 g=0.574 state=12.9 gate=0.59 | 18,194 tok/s\n",
      "NIAH (untrained): 0.1091x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@8500: 0.11x random [FAIL]\n",
      "[8500] loss=6.160 lr=1.88e-04 | β=0.181 g=0.570 state=9.7 gate=0.60 | 18,191 tok/s\n",
      "[8600] loss=6.188 lr=1.85e-04 | β=0.181 g=0.562 state=7.7 gate=0.62 | 18,197 tok/s\n",
      "[8700] loss=6.204 lr=1.83e-04 | β=0.190 g=0.574 state=11.6 gate=0.61 | 18,165 tok/s\n",
      "[8800] loss=6.199 lr=1.81e-04 | β=0.188 g=0.586 state=11.4 gate=0.62 | 18,171 tok/s\n",
      "[8900] loss=6.123 lr=1.78e-04 | β=0.195 g=0.594 state=10.3 gate=0.60 | 18,178 tok/s\n",
      "NIAH (untrained): 0.1714x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@9000: 0.17x random [FAIL]\n",
      "[9000] loss=6.165 lr=1.76e-04 | β=0.187 g=0.566 state=8.7 gate=0.61 | 18,172 tok/s\n",
      "[9100] loss=6.170 lr=1.74e-04 | β=0.189 g=0.590 state=11.2 gate=0.61 | 18,177 tok/s\n",
      "[9200] loss=6.112 lr=1.71e-04 | β=0.188 g=0.570 state=10.5 gate=0.62 | 18,184 tok/s\n",
      "[9300] loss=6.187 lr=1.69e-04 | β=0.188 g=0.578 state=10.2 gate=0.59 | 18,190 tok/s\n",
      "[9400] loss=6.111 lr=1.67e-04 | β=0.194 g=0.582 state=11.2 gate=0.62 | 18,198 tok/s\n",
      "NIAH (untrained): 0.1723x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@9500: 0.17x random [FAIL]\n",
      "[9500] loss=6.195 lr=1.64e-04 | β=0.189 g=0.570 state=11.9 gate=0.61 | 18,194 tok/s\n",
      "[9600] loss=6.066 lr=1.62e-04 | β=0.197 g=0.578 state=10.9 gate=0.59 | 18,197 tok/s\n",
      "[9700] loss=6.129 lr=1.60e-04 | β=0.200 g=0.578 state=12.9 gate=0.60 | 18,205 tok/s\n",
      "[9800] loss=6.066 lr=1.57e-04 | β=0.198 g=0.578 state=11.8 gate=0.58 | 18,210 tok/s\n",
      "[9900] loss=6.144 lr=1.55e-04 | β=0.194 g=0.582 state=9.4 gate=0.59 | 18,182 tok/s\n",
      "NIAH (untrained): 0.1396x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@10000: 0.14x random [FAIL]\n",
      "[10000] loss=6.079 lr=1.52e-04 | β=0.191 g=0.570 state=13.7 gate=0.61 | 18,180 tok/s\n",
      "[10100] loss=6.094 lr=1.50e-04 | β=0.195 g=0.586 state=9.9 gate=0.60 | 18,189 tok/s\n",
      "[10200] loss=6.152 lr=1.48e-04 | β=0.200 g=0.578 state=11.8 gate=0.62 | 18,193 tok/s\n",
      "[10300] loss=6.138 lr=1.45e-04 | β=0.204 g=0.582 state=11.1 gate=0.61 | 18,196 tok/s\n",
      "[10400] loss=6.053 lr=1.43e-04 | β=0.200 g=0.586 state=11.5 gate=0.62 | 18,200 tok/s\n",
      "NIAH (untrained): 0.1666x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@10500: 0.17x random [FAIL]\n",
      "[10500] loss=6.094 lr=1.40e-04 | β=0.196 g=0.574 state=10.9 gate=0.62 | 18,198 tok/s\n",
      "[10600] loss=6.188 lr=1.38e-04 | β=0.208 g=0.598 state=11.4 gate=0.61 | 18,204 tok/s\n",
      "[10700] loss=6.055 lr=1.36e-04 | β=0.203 g=0.574 state=9.3 gate=0.62 | 18,207 tok/s\n",
      "[10800] loss=6.051 lr=1.33e-04 | β=0.205 g=0.586 state=9.7 gate=0.61 | 18,212 tok/s\n",
      "[10900] loss=6.075 lr=1.31e-04 | β=0.210 g=0.598 state=10.8 gate=0.62 | 18,215 tok/s\n",
      "NIAH (untrained): 0.2259x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@11000: 0.23x random [FAIL]\n",
      "[11000] loss=6.109 lr=1.29e-04 | β=0.201 g=0.586 state=10.6 gate=0.59 | 18,211 tok/s\n",
      "[11100] loss=6.046 lr=1.26e-04 | β=0.210 g=0.586 state=11.1 gate=0.61 | 18,184 tok/s\n",
      "[11200] loss=6.093 lr=1.24e-04 | β=0.207 g=0.582 state=13.7 gate=0.61 | 18,183 tok/s\n",
      "[11300] loss=6.198 lr=1.22e-04 | β=0.204 g=0.566 state=11.9 gate=0.62 | 18,188 tok/s\n",
      "[11400] loss=6.017 lr=1.19e-04 | β=0.198 g=0.574 state=10.0 gate=0.61 | 18,194 tok/s\n",
      "NIAH (untrained): 0.2076x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@11500: 0.21x random [FAIL]\n",
      "[11500] loss=6.024 lr=1.17e-04 | β=0.207 g=0.590 state=11.1 gate=0.62 | 18,189 tok/s\n",
      "[11600] loss=6.047 lr=1.15e-04 | β=0.206 g=0.578 state=11.4 gate=0.61 | 18,170 tok/s\n",
      "[11700] loss=6.037 lr=1.12e-04 | β=0.211 g=0.582 state=12.9 gate=0.61 | 18,162 tok/s\n",
      "[11800] loss=6.116 lr=1.10e-04 | β=0.210 g=0.586 state=12.3 gate=0.62 | 18,159 tok/s\n",
      "[11900] loss=6.098 lr=1.08e-04 | β=0.207 g=0.594 state=12.5 gate=0.61 | 18,158 tok/s\n",
      "NIAH (untrained): 0.2128x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@12000: 0.21x random [FAIL]\n",
      "[12000] loss=6.098 lr=1.05e-04 | β=0.206 g=0.578 state=12.0 gate=0.60 | 18,151 tok/s\n",
      "[12100] loss=6.009 lr=1.03e-04 | β=0.207 g=0.574 state=8.9 gate=0.60 | 18,151 tok/s\n",
      "[12200] loss=6.005 lr=1.01e-04 | β=0.213 g=0.594 state=13.1 gate=0.61 | 18,131 tok/s\n",
      "[12300] loss=6.105 lr=9.87e-05 | β=0.211 g=0.582 state=10.2 gate=0.61 | 18,135 tok/s\n",
      "[12400] loss=6.066 lr=9.65e-05 | β=0.212 g=0.570 state=14.2 gate=0.61 | 18,140 tok/s\n",
      "NIAH (untrained): 0.2281x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@12500: 0.23x random [FAIL]\n",
      "[12500] loss=6.083 lr=9.43e-05 | β=0.208 g=0.570 state=10.0 gate=0.59 | 18,137 tok/s\n",
      "[12600] loss=5.969 lr=9.20e-05 | β=0.210 g=0.578 state=11.2 gate=0.61 | 18,140 tok/s\n",
      "[12700] loss=6.071 lr=8.99e-05 | β=0.211 g=0.574 state=13.6 gate=0.61 | 18,144 tok/s\n",
      "[12800] loss=5.994 lr=8.77e-05 | β=0.218 g=0.590 state=10.6 gate=0.61 | 18,144 tok/s\n",
      "[12900] loss=6.048 lr=8.55e-05 | β=0.213 g=0.586 state=10.0 gate=0.60 | 18,137 tok/s\n",
      "NIAH (untrained): 0.0967x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@13000: 0.10x random [FAIL]\n",
      "[13000] loss=5.971 lr=8.34e-05 | β=0.222 g=0.586 state=12.0 gate=0.62 | 18,125 tok/s\n",
      "[13100] loss=6.075 lr=8.13e-05 | β=0.217 g=0.582 state=11.2 gate=0.61 | 18,121 tok/s\n",
      "[13200] loss=5.944 lr=7.92e-05 | β=0.223 g=0.598 state=11.0 gate=0.61 | 18,118 tok/s\n",
      "[13300] loss=6.054 lr=7.71e-05 | β=0.215 g=0.578 state=11.2 gate=0.61 | 18,088 tok/s\n",
      "[13400] loss=6.015 lr=7.50e-05 | β=0.221 g=0.574 state=11.3 gate=0.63 | 18,089 tok/s\n",
      "NIAH (untrained): 0.1347x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@13500: 0.13x random [FAIL]\n",
      "[13500] loss=6.036 lr=7.29e-05 | β=0.219 g=0.590 state=12.9 gate=0.60 | 18,080 tok/s\n",
      "[13600] loss=6.015 lr=7.09e-05 | β=0.219 g=0.578 state=11.6 gate=0.62 | 18,076 tok/s\n",
      "[13700] loss=5.974 lr=6.89e-05 | β=0.218 g=0.574 state=10.1 gate=0.61 | 18,075 tok/s\n",
      "[13800] loss=6.107 lr=6.69e-05 | β=0.218 g=0.582 state=9.5 gate=0.61 | 18,063 tok/s\n",
      "[13900] loss=5.999 lr=6.49e-05 | β=0.215 g=0.598 state=13.0 gate=0.61 | 18,061 tok/s\n",
      "NIAH (untrained): 0.2339x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@14000: 0.23x random [FAIL]\n",
      "[14000] loss=6.044 lr=6.30e-05 | β=0.219 g=0.582 state=10.1 gate=0.61 | 18,047 tok/s\n",
      "[14100] loss=6.014 lr=6.11e-05 | β=0.219 g=0.582 state=9.8 gate=0.62 | 18,043 tok/s\n",
      "[14200] loss=5.954 lr=5.92e-05 | β=0.215 g=0.574 state=11.7 gate=0.61 | 18,042 tok/s\n",
      "[14300] loss=5.987 lr=5.73e-05 | β=0.222 g=0.598 state=14.0 gate=0.62 | 18,038 tok/s\n",
      "[14400] loss=6.061 lr=5.54e-05 | β=0.222 g=0.594 state=12.1 gate=0.62 | 18,012 tok/s\n",
      "NIAH (untrained): 0.1462x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@14500: 0.15x random [FAIL]\n",
      "[14500] loss=6.046 lr=5.36e-05 | β=0.216 g=0.570 state=10.6 gate=0.61 | 18,002 tok/s\n",
      "[14600] loss=5.978 lr=5.18e-05 | β=0.216 g=0.582 state=13.4 gate=0.59 | 17,997 tok/s\n",
      "[14700] loss=5.911 lr=5.00e-05 | β=0.226 g=0.582 state=11.9 gate=0.62 | 17,985 tok/s\n",
      "[14800] loss=6.000 lr=4.82e-05 | β=0.225 g=0.582 state=10.0 gate=0.62 | 17,979 tok/s\n",
      "[14900] loss=5.976 lr=4.65e-05 | β=0.220 g=0.590 state=9.6 gate=0.62 | 17,976 tok/s\n",
      "NIAH (untrained): 0.1885x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@15000: 0.19x random [FAIL]\n",
      "[15000] loss=6.008 lr=4.48e-05 | β=0.214 g=0.586 state=9.7 gate=0.60 | 17,966 tok/s\n",
      "[15100] loss=6.027 lr=4.31e-05 | β=0.225 g=0.586 state=13.9 gate=0.61 | 17,958 tok/s\n",
      "[15200] loss=6.031 lr=4.14e-05 | β=0.222 g=0.578 state=11.9 gate=0.61 | 17,949 tok/s\n",
      "[15300] loss=6.062 lr=3.98e-05 | β=0.217 g=0.586 state=11.4 gate=0.61 | 17,936 tok/s\n",
      "[15400] loss=6.066 lr=3.82e-05 | β=0.223 g=0.574 state=10.3 gate=0.61 | 17,918 tok/s\n",
      "NIAH (untrained): 0.1609x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@15500: 0.16x random [FAIL]\n",
      "[15500] loss=6.018 lr=3.66e-05 | β=0.221 g=0.590 state=12.5 gate=0.60 | 17,889 tok/s\n",
      "[15600] loss=6.004 lr=3.51e-05 | β=0.224 g=0.578 state=13.4 gate=0.62 | 17,886 tok/s\n",
      "[15700] loss=5.993 lr=3.36e-05 | β=0.220 g=0.578 state=13.0 gate=0.61 | 17,885 tok/s\n",
      "[15800] loss=6.064 lr=3.21e-05 | β=0.222 g=0.590 state=11.9 gate=0.61 | 17,883 tok/s\n",
      "[15900] loss=5.912 lr=3.06e-05 | β=0.226 g=0.594 state=12.7 gate=0.62 | 17,882 tok/s\n",
      "NIAH (untrained): 0.1714x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@16000: 0.17x random [FAIL]\n",
      "[16000] loss=6.009 lr=2.92e-05 | β=0.223 g=0.578 state=11.2 gate=0.63 | 17,874 tok/s\n",
      "[16100] loss=6.022 lr=2.78e-05 | β=0.226 g=0.590 state=10.9 gate=0.61 | 17,872 tok/s\n",
      "[16200] loss=6.053 lr=2.64e-05 | β=0.222 g=0.578 state=11.4 gate=0.61 | 17,873 tok/s\n",
      "[16300] loss=5.953 lr=2.51e-05 | β=0.233 g=0.586 state=11.3 gate=0.61 | 17,871 tok/s\n",
      "[16400] loss=5.923 lr=2.38e-05 | β=0.228 g=0.586 state=13.4 gate=0.62 | 17,869 tok/s\n",
      "NIAH (untrained): 0.1549x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@16500: 0.15x random [FAIL]\n",
      "[16500] loss=6.034 lr=2.25e-05 | β=0.221 g=0.578 state=13.0 gate=0.62 | 17,864 tok/s\n",
      "[16600] loss=6.024 lr=2.13e-05 | β=0.221 g=0.590 state=8.8 gate=0.62 | 17,845 tok/s\n",
      "[16700] loss=6.048 lr=2.01e-05 | β=0.218 g=0.578 state=13.2 gate=0.61 | 17,849 tok/s\n",
      "[16800] loss=6.029 lr=1.89e-05 | β=0.221 g=0.578 state=14.0 gate=0.63 | 17,849 tok/s\n",
      "[16900] loss=6.002 lr=1.78e-05 | β=0.230 g=0.586 state=11.2 gate=0.62 | 17,850 tok/s\n",
      "NIAH (untrained): 0.1523x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@17000: 0.15x random [FAIL]\n",
      "[17000] loss=5.992 lr=1.67e-05 | β=0.221 g=0.590 state=11.6 gate=0.59 | 17,845 tok/s\n",
      "[17100] loss=5.957 lr=1.56e-05 | β=0.220 g=0.586 state=12.2 gate=0.62 | 17,845 tok/s\n",
      "[17200] loss=5.983 lr=1.46e-05 | β=0.223 g=0.590 state=9.7 gate=0.61 | 17,846 tok/s\n",
      "[17300] loss=5.992 lr=1.36e-05 | β=0.224 g=0.582 state=13.5 gate=0.62 | 17,849 tok/s\n",
      "[17400] loss=5.987 lr=1.26e-05 | β=0.231 g=0.586 state=10.8 gate=0.62 | 17,850 tok/s\n",
      "NIAH (untrained): 0.1473x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@17500: 0.15x random [FAIL]\n",
      "[17500] loss=6.054 lr=1.16e-05 | β=0.219 g=0.586 state=10.2 gate=0.61 | 17,847 tok/s\n",
      "[17600] loss=5.971 lr=1.07e-05 | β=0.229 g=0.590 state=11.8 gate=0.63 | 17,846 tok/s\n",
      "[17700] loss=6.028 lr=9.88e-06 | β=0.222 g=0.574 state=11.5 gate=0.60 | 17,828 tok/s\n",
      "[17800] loss=6.064 lr=9.05e-06 | β=0.222 g=0.586 state=12.8 gate=0.61 | 17,830 tok/s\n",
      "[17900] loss=5.999 lr=8.25e-06 | β=0.220 g=0.586 state=13.0 gate=0.62 | 17,832 tok/s\n",
      "NIAH (untrained): 0.1633x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@18000: 0.16x random [FAIL]\n",
      "[18000] loss=6.024 lr=7.49e-06 | β=0.222 g=0.570 state=14.6 gate=0.62 | 17,826 tok/s\n",
      "[18100] loss=6.033 lr=6.76e-06 | β=0.226 g=0.574 state=11.5 gate=0.62 | 17,822 tok/s\n",
      "[18200] loss=6.004 lr=6.08e-06 | β=0.228 g=0.586 state=11.1 gate=0.62 | 17,821 tok/s\n",
      "[18300] loss=5.960 lr=5.42e-06 | β=0.222 g=0.582 state=10.7 gate=0.62 | 17,818 tok/s\n",
      "[18400] loss=5.931 lr=4.81e-06 | β=0.229 g=0.590 state=13.4 gate=0.62 | 17,818 tok/s\n",
      "NIAH (untrained): 0.2233x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@18500: 0.22x random [FAIL]\n",
      "[18500] loss=5.916 lr=4.23e-06 | β=0.227 g=0.586 state=12.6 gate=0.61 | 17,811 tok/s\n",
      "[18600] loss=5.985 lr=3.69e-06 | β=0.223 g=0.570 state=12.1 gate=0.62 | 17,811 tok/s\n",
      "[18700] loss=5.979 lr=3.18e-06 | β=0.229 g=0.586 state=12.7 gate=0.62 | 17,813 tok/s\n",
      "[18800] loss=6.026 lr=2.71e-06 | β=0.226 g=0.574 state=12.7 gate=0.62 | 17,796 tok/s\n",
      "[18900] loss=6.089 lr=2.28e-06 | β=0.222 g=0.570 state=13.2 gate=0.63 | 17,795 tok/s\n",
      "NIAH (untrained): 0.1398x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@19000: 0.14x random [FAIL]\n",
      "[19000] loss=5.949 lr=1.88e-06 | β=0.221 g=0.590 state=10.9 gate=0.62 | 17,790 tok/s\n",
      "[19100] loss=6.062 lr=1.53e-06 | β=0.229 g=0.582 state=10.5 gate=0.62 | 17,791 tok/s\n",
      "[19200] loss=6.003 lr=1.21e-06 | β=0.222 g=0.574 state=12.7 gate=0.63 | 17,786 tok/s\n",
      "[19300] loss=5.931 lr=9.24e-07 | β=0.224 g=0.586 state=14.8 gate=0.60 | 17,784 tok/s\n",
      "[19400] loss=6.049 lr=6.79e-07 | β=0.227 g=0.605 state=10.6 gate=0.60 | 17,785 tok/s\n",
      "NIAH (untrained): 0.1618x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@19500: 0.16x random [FAIL]\n",
      "[19500] loss=6.067 lr=4.72e-07 | β=0.225 g=0.590 state=12.7 gate=0.61 | 17,779 tok/s\n",
      "[19600] loss=6.043 lr=3.02e-07 | β=0.215 g=0.566 state=12.6 gate=0.63 | 17,778 tok/s\n",
      "[19700] loss=5.994 lr=1.70e-07 | β=0.221 g=0.590 state=12.6 gate=0.62 | 17,778 tok/s\n",
      "[19800] loss=6.071 lr=7.55e-08 | β=0.224 g=0.590 state=12.5 gate=0.62 | 17,778 tok/s\n",
      "[19900] loss=6.068 lr=1.89e-08 | β=0.228 g=0.586 state=11.0 gate=0.61 | 17,764 tok/s\n",
      "NIAH (untrained): 0.1871x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  >>> NIAH@20000: 0.19x random [FAIL]\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "Time: 9.6 min\n",
      "Loss: 10.94 -> 6.03\n",
      "NIAH trajectory: [(500, 0.017143921357378862), (1000, 0.08830697408167794), (1500, 0.08302063198924353), (2000, 0.0996810168259638), (2500, 0.13633083911787197), (3000, 0.15034655938394698), (3500, 0.17673673593545705), (4000, 0.11597252845881674), (4500, 0.11243931436490015), (5000, 0.145170783188694), (5500, 0.11477431772842503), (6000, 0.1025593658095616), (6500, 0.10060570080342188), (7000, 0.17753489404457432), (7500, 0.20239036173676367), (8000, 0.09129999332166108), (8500, 0.1090889310770469), (9000, 0.17143871544663464), (9500, 0.17234831498449238), (10000, 0.13955109099572288), (10500, 0.16657527576127318), (11000, 0.22590206477288746), (11500, 0.20758776473042484), (12000, 0.21277723994615533), (12500, 0.22807158683915532), (13000, 0.09665765959732488), (13500, 0.13472570582999216), (14000, 0.23389035005943792), (14500, 0.14617791335221283), (15000, 0.1884968895971042), (15500, 0.16085366738108842), (16000, 0.17135661609743616), (16500, 0.15494880246324527), (17000, 0.15230751727253514), (17500, 0.14728914769861562), (18000, 0.16331783334919928), (18500, 0.22330132718964724), (19000, 0.13977114029078888), (19500, 0.16183940878736772), (20000, 0.18714742554650124)]\n",
      "\n",
      "Final NIAH at different needle positions:\n",
      "NIAH (untrained): 0.1546x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  needle@16: 0.15x random\n",
      "NIAH (untrained): 0.1963x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  needle@32: 0.20x random\n",
      "NIAH (untrained): 0.1942x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  needle@64: 0.19x random\n",
      "NIAH (untrained): 0.2220x random\n",
      "  (>1.0 means model finds needle, <1.0 means no retrieval)\n",
      "  needle@96: 0.22x random\n"
     ]
    }
   ],
   "source": [
    "# TRAINING WITH MONITORING\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Data setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading data...\")\n",
    "ds = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\", split=\"train\", streaming=True)\n",
    "\n",
    "# Buffer tokens\n",
    "token_buffer = []\n",
    "target_tokens = 2_000_000  # 2M for quick test\n",
    "\n",
    "for doc in ds:\n",
    "    toks = tokenizer.encode(doc['text'])\n",
    "    token_buffer.extend(toks)\n",
    "    if len(token_buffer) >= target_tokens:\n",
    "        break\n",
    "\n",
    "token_tensor = torch.tensor(token_buffer[:target_tokens], device='cuda')\n",
    "print(f\"Loaded {len(token_tensor):,} tokens\")\n",
    "\n",
    "def get_batch(batch_size=4, seq_len=128):\n",
    "    ix = torch.randint(0, len(token_tensor) - seq_len - 1, (batch_size,))\n",
    "    x = torch.stack([token_tensor[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([token_tensor[i+1:i+seq_len+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Training config\n",
    "STEPS = 20000\n",
    "LR = 3e-4\n",
    "BATCH = 4\n",
    "SEQ_LEN = 128\n",
    "LOG_EVERY = 100\n",
    "NIAH_EVERY = 500\n",
    "\n",
    "# Optimizer\n",
    "opt = AdamW(model.parameters(), lr=LR, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "\n",
    "# Tracking\n",
    "history = {\n",
    "    'loss': [],\n",
    "    'niah_ratio': [],\n",
    "    'gdn_beta': [],\n",
    "    'gdn_g': [],\n",
    "    'state_norm': [],\n",
    "    'swa_state_contrib': [],\n",
    "}\n",
    "\n",
    "print(f\"\\nTraining {STEPS} steps, batch={BATCH}, seq_len={SEQ_LEN}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.train()\n",
    "start = time.time()\n",
    "\n",
    "for step in range(STEPS):\n",
    "    # LR schedule: linear warmup then cosine\n",
    "    if step < 200:\n",
    "        lr = LR * (step + 1) / 200\n",
    "    else:\n",
    "        progress = (step - 200) / (STEPS - 200)\n",
    "        lr = LR * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    for pg in opt.param_groups:\n",
    "        pg['lr'] = lr\n",
    "    \n",
    "    # Forward\n",
    "    x, y = get_batch(BATCH, SEQ_LEN)\n",
    "    logits, loss, diags, state = model(x, y, return_diagnostics=True)\n",
    "    \n",
    "    # Backward\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    opt.step()\n",
    "    \n",
    "    # Track\n",
    "    history['loss'].append(loss.item())\n",
    "    \n",
    "    # Extract diagnostics\n",
    "    gdn_diag = [d for d in diags if d['layer_type'] == 'GDN'][0]\n",
    "    swa_diag = [d for d in diags if d['layer_type'] == 'SWA'][0]\n",
    "    \n",
    "    history['gdn_beta'].append(gdn_diag['beta_mean'])\n",
    "    history['gdn_g'].append(gdn_diag['g_mean'])\n",
    "    history['state_norm'].append(gdn_diag['state_norm'])\n",
    "    history['swa_state_contrib'].append(swa_diag['gate_mean'])\n",
    "    # Log\n",
    "    if step % LOG_EVERY == 0:\n",
    "        elapsed = time.time() - start\n",
    "        tps = (step + 1) * BATCH * SEQ_LEN / elapsed\n",
    "        avg_loss = sum(history['loss'][-50:]) / min(50, len(history['loss']))\n",
    "        \n",
    "        print(f\"[{step:4d}] loss={avg_loss:.3f} lr={lr:.2e} | \"\n",
    "              f\"β={gdn_diag['beta_mean']:.3f} g={gdn_diag['g_mean']:.3f} \"\n",
    "              f\"state={gdn_diag['state_norm']:.1f} gate={swa_diag['gate_mean']:.2f} | \"\n",
    "              f\"{tps:,.0f} tok/s\")\n",
    "    \n",
    "    # NIAH check\n",
    "    if (step + 1) % NIAH_EVERY == 0:\n",
    "        model.eval()\n",
    "        niah = simple_niah(model, seq_len=SEQ_LEN, needle_pos=32, n_trials=30)\n",
    "        avg_ratio = sum(r['ratio'] for r in niah) / len(niah)\n",
    "        history['niah_ratio'].append((step + 1, avg_ratio))\n",
    "        status = \"PASS\" if avg_ratio > 1.0 else \"FAIL\"\n",
    "        print(f\"  >>> NIAH@{step+1}: {avg_ratio:.2f}x random [{status}]\")\n",
    "        model.train()\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "elapsed = time.time() - start\n",
    "print(f\"Time: {elapsed/60:.1f} min\")\n",
    "print(f\"Loss: {history['loss'][0]:.2f} -> {sum(history['loss'][-50:])/50:.2f}\")\n",
    "print(f\"NIAH trajectory: {history['niah_ratio']}\")\n",
    "\n",
    "# Final NIAH at multiple positions\n",
    "print(\"\\nFinal NIAH at different needle positions:\")\n",
    "model.eval()\n",
    "for needle_pos in [16, 32, 64, 96]:\n",
    "    niah = simple_niah(model, seq_len=128, needle_pos=needle_pos, n_trials=30)\n",
    "    avg = sum(r['ratio'] for r in niah) / len(niah)\n",
    "    print(f\"  needle@{needle_pos}: {avg:.2f}x random\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
