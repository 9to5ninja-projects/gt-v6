{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# GroundThink v6 Hybrid Architecture\n",
    "## GDN (Triton) + SWA (flash_attn) with Complete Analysis Suite\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "**GatedDeltaNet (GDN):**\n",
    "- TRUE Delta Rule: `S_t = g_t * S_{t-1} + β_t * (v_t - S_{t-1}·k_t) ⊗ k_t`\n",
    "- Error correction prevents redundant writes and state explosion\n",
    "- Triton kernels for forward + backward (validated)\n",
    "\n",
    "**SlidingWindowAttention (SWA):**\n",
    "- Local attention via flash_attn with window_size parameter\n",
    "- **State retrieval**: Queries GDN state for global context (enables NIAH)\n",
    "- PyTorch fallback if flash_attn unavailable\n",
    "\n",
    "**Hybrid Information Flow:**\n",
    "```\n",
    "GDN layers: Compress sequence into state S_t [H, K, V]\n",
    "     ↓ (state flows)\n",
    "SWA layers: Query state for retrieval + local attention\n",
    "```\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "| Cell | Content |\n",
    "|------|---------|\n",
    "| 0 | Environment & Imports |\n",
    "| 1 | Configuration |\n",
    "| 2 | Triton Kernels (Forward + Backward) |\n",
    "| 3 | Model Components |\n",
    "| 4 | Model Assembly |\n",
    "| 5 | Data Loading |\n",
    "| 6 | NIAH Testing Suite |\n",
    "| 7 | Delta Rule Validation Suite |\n",
    "| 8 | Training Infrastructure |\n",
    "| 9 | Gradient Analysis |\n",
    "| 10 | Performance Profiling |\n",
    "| 11 | Triton Cache Management |\n",
    "| 12 | Quick Start / Validation |\n",
    "| 13 | Training Execution |\n",
    "| 14 | Post-Training Evaluation |\n",
    "\n",
    "---\n",
    "\n",
    "*** Recent edits: ***\n",
    "### Cell 13: Training Execution\n",
    "\n",
    "```pythonpython\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)  # adjusted from 1.0 to 2.0\n",
    "```  \n",
    "### Cell 1: Configuration\n",
    "\n",
    "```pythonpython\n",
    "    g_bias: float = 4.0       # high retention  # adjusted from +2.0 to +4.0\n",
    "```\n",
    "\n",
    "### Cell 13: Training Execution\n",
    "\n",
    "```pythonpython\n",
    "    lr=2e-4, # adjusted from 3e-4 to 2e-4 to prevent collapse in mixed phase, study curriculum learning literature for details\n",
    "```\n",
    "### Both changes below made at the same time:\n",
    "### Cell 2: Triton Kernels (Backward Pass)\n",
    "\n",
    "```pythonpython\n",
    "         # Reconstruct state_prev via division (numerical stability floor)\n",
    "         safe_denom changed from 1e-6 to 1e-3\n",
    "         safe_g changed from 1e-6 to 1e-3\n",
    "```\n",
    "### Cell 3: Model Components (RMSNorm)\n",
    "\n",
    "```pythonpython\n",
    "    def __init__(self, dim: int, eps: float = 1e-3):  # adjusted from 1e-6 to 1e-3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "cell-0-env",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.11.0.dev20260128+cu128\n",
      "CUDA: 12.8\n",
      "Triton: 3.6.0\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "GPU Memory: 6.4 GB\n",
      "flash_attn: ✓ Available\n",
      "FLA: ✓ Available (for profiling comparison)\n",
      "\n",
      "✓ Environment ready. Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 0: Environment & Imports\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment check\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "print(f\"Triton: {triton.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Check flash_attn\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    FLASH_ATTN_AVAILABLE = True\n",
    "    print(\"flash_attn: ✓ Available\")\n",
    "except ImportError:\n",
    "    FLASH_ATTN_AVAILABLE = False\n",
    "    print(\"flash_attn: ✗ Using PyTorch fallback\")\n",
    "\n",
    "# Check FLA (for comparison)\n",
    "try:\n",
    "    from fla.ops.gated_delta_rule import chunk_gated_delta_rule\n",
    "    HAS_FLA = True\n",
    "    print(\"FLA: ✓ Available (for profiling comparison)\")\n",
    "except ImportError:\n",
    "    HAS_FLA = False\n",
    "    print(\"FLA: Not available\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\n✓ Environment ready. Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "cell-1-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridConfig: GS (1 GDN + 1 SWA)\n",
      "  d_model=256, n_heads=8\n",
      "  GDN state: [8, 32, 64]\n",
      "  SWA window: 64\n",
      "  State capacity/head: 2048 floats\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Configuration\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class HybridConfig:\n",
    "    \"\"\"\n",
    "    Configuration for TransparentHybrid model.\n",
    "    \n",
    "    Key Parameters:\n",
    "        layer_pattern: String of 'G' (GDN) and 'S' (SWA)\n",
    "            Examples: \"GS\", \"GGS\", \"GGSG\", \"GGGSGGGS\"\n",
    "        \n",
    "        head_dim (K): Key dimension for GDN state matrix\n",
    "        value_dim (V): Value dimension for GDN state matrix\n",
    "        \n",
    "        State matrix shape: [B, H, K, V]\n",
    "        Theoretical capacity: K * V floats per head\n",
    "        Effective capacity: Much less with random keys (interference)\n",
    "    \n",
    "    Gate Initialization (CRITICAL):\n",
    "        beta_bias=-2.0: sigmoid(-2) ≈ 0.12 → sparse writes (gatekeeper)\n",
    "        g_bias=+2.0:    sigmoid(+2) ≈ 0.88 → high retention\n",
    "    \"\"\"\n",
    "    # Model dimensions\n",
    "    d_model: int = 256\n",
    "    n_heads: int = 8\n",
    "    head_dim: int = 32        # K dimension for GDN\n",
    "    value_dim: int = 64       # V dimension for GDN (typically 2x head_dim)\n",
    "    vocab_size: int = 50257\n",
    "    \n",
    "    # Layer pattern\n",
    "    layer_pattern: str = \"GS\"\n",
    "    \n",
    "    # SWA config\n",
    "    window_size: int = 64\n",
    "    \n",
    "    # Initialization\n",
    "    init_std: float = 0.02\n",
    "    \n",
    "    # Gate biases (GDN)\n",
    "    beta_bias: float = -2.0   # sparse writes\n",
    "    g_bias: float = 4.0       # high retention ### adjusted from +2.0 to +4.0\n",
    "    \n",
    "    # Special tokens for NIAH testing\n",
    "    marker_token: int = 50251\n",
    "    cue_token: int = 50250\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.head_dim is None:\n",
    "            self.head_dim = self.d_model // self.n_heads\n",
    "        if self.value_dim is None:\n",
    "            self.value_dim = self.head_dim * 2\n",
    "    \n",
    "    @property\n",
    "    def n_layers(self) -> int:\n",
    "        return len(self.layer_pattern)\n",
    "    \n",
    "    @property\n",
    "    def state_capacity(self) -> int:\n",
    "        \"\"\"Theoretical state capacity per head (K * V floats).\"\"\"\n",
    "        return self.head_dim * self.value_dim\n",
    "    \n",
    "    def describe(self) -> str:\n",
    "        gdn = sum(1 for c in self.layer_pattern if c == 'G')\n",
    "        swa = sum(1 for c in self.layer_pattern if c == 'S')\n",
    "        return (f\"HybridConfig: {self.layer_pattern} ({gdn} GDN + {swa} SWA)\\n\"\n",
    "                f\"  d_model={self.d_model}, n_heads={self.n_heads}\\n\"\n",
    "                f\"  GDN state: [{self.n_heads}, {self.head_dim}, {self.value_dim}]\\n\"\n",
    "                f\"  SWA window: {self.window_size}\\n\"\n",
    "                f\"  State capacity/head: {self.state_capacity} floats\")\n",
    "\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "# Default config for quick testing\n",
    "DEFAULT_CFG = HybridConfig(d_model=256, n_heads=8, layer_pattern=\"GS\")\n",
    "print(DEFAULT_CFG.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "cell-2-triton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton kernels defined (forward + backward).\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Triton Kernels (Forward + Backward)\n",
    "# =============================================================================\n",
    "#\n",
    "# DESIGN NOTES:\n",
    "#   1. Forward and backward are SEPARATE kernels (not fused)\n",
    "#      - Explicit state mutation semantics\n",
    "#      - Cleaner autograd integration\n",
    "#      - Triton compiles/caches separately anyway\n",
    "#\n",
    "#   2. State reconstruction in backward via division:\n",
    "#      state_prev = (state - β * outer) / g\n",
    "#      - Unstable when g ≈ 0, using safe_denom floor\n",
    "#      - Better approach: checkpoint forward states (future work)\n",
    "#\n",
    "#   3. Memory layout: Always .contiguous().float() at entry\n",
    "#      - Non-contiguous tensors cause silent errors\n",
    "#\n",
    "#   4. Grid size: (B, H) - one program per batch×head\n",
    "#      - May hit limits if B*H > 1024 (use tiling for larger)\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "@triton.jit\n",
    "def delta_rule_fwd_kernel(\n",
    "    K_ptr, V_ptr, Beta_ptr, G_ptr, State_ptr, Out_ptr,\n",
    "    stride_k_b, stride_k_t, stride_k_h, stride_k_d,\n",
    "    stride_v_b, stride_v_t, stride_v_h, stride_v_d,\n",
    "    stride_bg_b, stride_bg_t, stride_bg_h,\n",
    "    stride_s_b, stride_s_h, stride_s_k, stride_s_v,\n",
    "    stride_o_b, stride_o_t, stride_o_h, stride_o_v,\n",
    "    T,\n",
    "    K_DIM: tl.constexpr,\n",
    "    V_DIM: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    TRUE Delta Rule forward kernel.\n",
    "    \n",
    "    For each timestep t:\n",
    "        1. pred = S_{t-1} · k_t           (what we'd retrieve)\n",
    "        2. error = v_t - pred              (correction needed)\n",
    "        3. outer = k_t ⊗ error            (rank-1 update)\n",
    "        4. S_t = g_t * S_{t-1} + β_t * outer\n",
    "        5. out_t = S_t · k_t              (output)\n",
    "    \"\"\"\n",
    "    pid_b = tl.program_id(0)\n",
    "    pid_h = tl.program_id(1)\n",
    "    \n",
    "    k_offs = tl.arange(0, K_DIM)\n",
    "    v_offs = tl.arange(0, V_DIM)\n",
    "    \n",
    "    # Load initial state\n",
    "    state_base = State_ptr + pid_b * stride_s_b + pid_h * stride_s_h\n",
    "    state_ptrs = state_base + k_offs[:, None] * stride_s_k + v_offs[None, :] * stride_s_v\n",
    "    state = tl.load(state_ptrs).to(tl.float32)\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Load k_t, v_t, beta_t, g_t\n",
    "        k_base = K_ptr + pid_b * stride_k_b + t * stride_k_t + pid_h * stride_k_h\n",
    "        k_t = tl.load(k_base + k_offs * stride_k_d).to(tl.float32)\n",
    "        \n",
    "        v_base = V_ptr + pid_b * stride_v_b + t * stride_v_t + pid_h * stride_v_h\n",
    "        v_t = tl.load(v_base + v_offs * stride_v_d).to(tl.float32)\n",
    "        \n",
    "        bg_offset = pid_b * stride_bg_b + t * stride_bg_t + pid_h * stride_bg_h\n",
    "        beta_t = tl.load(Beta_ptr + bg_offset).to(tl.float32)\n",
    "        g_t = tl.load(G_ptr + bg_offset).to(tl.float32)\n",
    "        \n",
    "        # TRUE Delta Rule\n",
    "        pred = tl.sum(state * k_t[:, None], axis=0)  # S·k\n",
    "        error = v_t - pred                            # error correction\n",
    "        outer = k_t[:, None] * error[None, :]        # k ⊗ error\n",
    "        state = g_t * state + beta_t * outer         # update\n",
    "        \n",
    "        # Output\n",
    "        out_t = tl.sum(state * k_t[:, None], axis=0)\n",
    "        out_base = Out_ptr + pid_b * stride_o_b + t * stride_o_t + pid_h * stride_o_h\n",
    "        tl.store(out_base + v_offs * stride_o_v, out_t)\n",
    "    \n",
    "    # Store final state\n",
    "    tl.store(state_ptrs, state)\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def delta_rule_bwd_kernel(\n",
    "    K_ptr, V_ptr, Beta_ptr, G_ptr, State_in_ptr,\n",
    "    dOut_ptr, dState_out_ptr,\n",
    "    dK_ptr, dV_ptr, dBeta_ptr, dG_ptr, dState_in_ptr,\n",
    "    stride_k_b, stride_k_t, stride_k_h, stride_k_d,\n",
    "    stride_v_b, stride_v_t, stride_v_h, stride_v_d,\n",
    "    stride_bg_b, stride_bg_t, stride_bg_h,\n",
    "    stride_s_b, stride_s_h, stride_s_k, stride_s_v,\n",
    "    stride_o_b, stride_o_t, stride_o_h, stride_o_v,\n",
    "    stride_dk_b, stride_dk_t, stride_dk_h, stride_dk_d,\n",
    "    stride_dv_b, stride_dv_t, stride_dv_h, stride_dv_d,\n",
    "    stride_dbg_b, stride_dbg_t, stride_dbg_h,\n",
    "    stride_ds_b, stride_ds_h, stride_ds_k, stride_ds_v,\n",
    "    T,\n",
    "    K_DIM: tl.constexpr,\n",
    "    V_DIM: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Backward kernel using adjoint method with state reconstruction.\n",
    "    \n",
    "    Gradient terms for dk (3 terms - CRITICAL):\n",
    "        1. dk_from_output = S · dout        (from out = S·k)\n",
    "        2. dk_from_outer  = β * dS · error  (from outer = k ⊗ error)\n",
    "        3. dk_from_pred   = -β * S_prev · (dS·k)  (from error = v - S·k)\n",
    "    \n",
    "    The third term was missing in initial version (caused 4.7% gradient error).\n",
    "    \"\"\"\n",
    "    pid_b = tl.program_id(0)\n",
    "    pid_h = tl.program_id(1)\n",
    "    \n",
    "    k_offs = tl.arange(0, K_DIM)\n",
    "    v_offs = tl.arange(0, V_DIM)\n",
    "    \n",
    "    # Phase 1: Forward pass to get final state\n",
    "    state_base = State_in_ptr + pid_b * stride_s_b + pid_h * stride_s_h\n",
    "    state_ptrs = state_base + k_offs[:, None] * stride_s_k + v_offs[None, :] * stride_s_v\n",
    "    state = tl.load(state_ptrs).to(tl.float32)\n",
    "    \n",
    "    for t in range(T):\n",
    "        k_base = K_ptr + pid_b * stride_k_b + t * stride_k_t + pid_h * stride_k_h\n",
    "        k_t = tl.load(k_base + k_offs * stride_k_d).to(tl.float32)\n",
    "        v_base = V_ptr + pid_b * stride_v_b + t * stride_v_t + pid_h * stride_v_h\n",
    "        v_t = tl.load(v_base + v_offs * stride_v_d).to(tl.float32)\n",
    "        bg_offset = pid_b * stride_bg_b + t * stride_bg_t + pid_h * stride_bg_h\n",
    "        beta_t = tl.load(Beta_ptr + bg_offset).to(tl.float32)\n",
    "        g_t = tl.load(G_ptr + bg_offset).to(tl.float32)\n",
    "        \n",
    "        pred = tl.sum(state * k_t[:, None], axis=0)\n",
    "        error = v_t - pred\n",
    "        outer = k_t[:, None] * error[None, :]\n",
    "        state = g_t * state + beta_t * outer\n",
    "    \n",
    "    # Phase 2: Backward pass (reverse time)\n",
    "    dstate_out_base = dState_out_ptr + pid_b * stride_ds_b + pid_h * stride_ds_h\n",
    "    dstate_out_ptrs = dstate_out_base + k_offs[:, None] * stride_ds_k + v_offs[None, :] * stride_ds_v\n",
    "    dstate = tl.load(dstate_out_ptrs).to(tl.float32)\n",
    "    \n",
    "    for t_rev in range(T):\n",
    "        t = T - 1 - t_rev\n",
    "        \n",
    "        # Reload inputs for timestep t\n",
    "        k_base = K_ptr + pid_b * stride_k_b + t * stride_k_t + pid_h * stride_k_h\n",
    "        k_t = tl.load(k_base + k_offs * stride_k_d).to(tl.float32)\n",
    "        v_base = V_ptr + pid_b * stride_v_b + t * stride_v_t + pid_h * stride_v_h\n",
    "        v_t = tl.load(v_base + v_offs * stride_v_d).to(tl.float32)\n",
    "        bg_offset = pid_b * stride_bg_b + t * stride_bg_t + pid_h * stride_bg_h\n",
    "        beta_t = tl.load(Beta_ptr + bg_offset).to(tl.float32)\n",
    "        g_t = tl.load(G_ptr + bg_offset).to(tl.float32)\n",
    "        \n",
    "        dout_base = dOut_ptr + pid_b * stride_o_b + t * stride_o_t + pid_h * stride_o_h\n",
    "        dout_t = tl.load(dout_base + v_offs * stride_o_v).to(tl.float32)\n",
    "        \n",
    "        # Reconstruct state_prev via division (numerical stability floor)\n",
    "        out_t = tl.sum(state * k_t[:, None], axis=0)\n",
    "        denom = g_t - beta_t\n",
    "        safe_denom = tl.where(tl.abs(denom) > 1e-3, denom, 1e-3) ### Reduced from 1e-6 to 1e-3\n",
    "        pred_before = (out_t - beta_t * v_t) / safe_denom        ### Also see RMSNorm eps change\n",
    "        error_t = v_t - pred_before                              ### in Cell 2. \n",
    "        outer_t = k_t[:, None] * error_t[None, :]\n",
    "        safe_g = tl.where(g_t > 1e-3, g_t, 1e-3) ### Reduced from 1e-6 to 1e-3\n",
    "        state_prev = (state - beta_t * outer_t) / safe_g\n",
    "        \n",
    "        # Accumulate dstate from output gradient\n",
    "        dstate = dstate + k_t[:, None] * dout_t[None, :]\n",
    "        \n",
    "        # Gradient for v: dv = β * (dS · k)\n",
    "        dv_t = beta_t * tl.sum(dstate * k_t[:, None], axis=0)\n",
    "        dv_base = dV_ptr + pid_b * stride_dv_b + t * stride_dv_t + pid_h * stride_dv_h\n",
    "        tl.store(dv_base + v_offs * stride_dv_d, dv_t)\n",
    "        \n",
    "        # Gradient for beta: dβ = sum(dS * outer)\n",
    "        dbeta_t = tl.sum(dstate * outer_t)\n",
    "        dbeta_base = dBeta_ptr + pid_b * stride_dbg_b + t * stride_dbg_t + pid_h * stride_dbg_h\n",
    "        tl.store(dbeta_base, dbeta_t)\n",
    "        \n",
    "        # Gradient for g: dg = sum(dS * S_prev)\n",
    "        dg_t = tl.sum(dstate * state_prev)\n",
    "        dg_base = dG_ptr + pid_b * stride_dbg_b + t * stride_dbg_t + pid_h * stride_dbg_h\n",
    "        tl.store(dg_base, dg_t)\n",
    "        \n",
    "        # Gradient for k: ALL 3 TERMS\n",
    "        dk_from_output = tl.sum(state * dout_t[None, :], axis=1)\n",
    "        dk_from_outer = beta_t * tl.sum(dstate * error_t[None, :], axis=1)\n",
    "        dstate_dot_k = tl.sum(dstate * k_t[:, None], axis=0)\n",
    "        dk_from_pred = -beta_t * tl.sum(state_prev * dstate_dot_k[None, :], axis=1)\n",
    "        dk_t = dk_from_output + dk_from_outer + dk_from_pred\n",
    "        dk_base = dK_ptr + pid_b * stride_dk_b + t * stride_dk_t + pid_h * stride_dk_h\n",
    "        tl.store(dk_base + k_offs * stride_dk_d, dk_t)\n",
    "        \n",
    "        # Propagate dstate backward\n",
    "        dstate_k = tl.sum(dstate * k_t[:, None], axis=0)\n",
    "        dstate = g_t * dstate - beta_t * k_t[:, None] * dstate_k[None, :]\n",
    "        state = state_prev\n",
    "    \n",
    "    # Store gradient w.r.t initial state\n",
    "    dstate_in_base = dState_in_ptr + pid_b * stride_ds_b + pid_h * stride_ds_h\n",
    "    dstate_in_ptrs = dstate_in_base + k_offs[:, None] * stride_ds_k + v_offs[None, :] * stride_ds_v\n",
    "    tl.store(dstate_in_ptrs, dstate)\n",
    "\n",
    "\n",
    "print(\"Triton kernels defined (forward + backward).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "cell-2b-wrappers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton wrappers + autograd ready.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2b: Triton Wrappers & Autograd\n",
    "# =============================================================================\n",
    "\n",
    "def triton_delta_rule(k, v, beta, g, initial_state=None):\n",
    "    \"\"\"Forward pass wrapper. Always works in float32 internally.\"\"\"\n",
    "    B, T, H, K_DIM = k.shape\n",
    "    V_DIM = v.shape[-1]\n",
    "    device = k.device\n",
    "    orig_dtype = k.dtype\n",
    "    \n",
    "    # CRITICAL: contiguous + float32\n",
    "    k = k.contiguous().float()\n",
    "    v = v.contiguous().float()\n",
    "    beta = beta.contiguous().float()\n",
    "    g = g.contiguous().float()\n",
    "    \n",
    "    if initial_state is None:\n",
    "        state = torch.zeros(B, H, K_DIM, V_DIM, device=device, dtype=torch.float32)\n",
    "    else:\n",
    "        state = initial_state.contiguous().float().clone()\n",
    "    \n",
    "    out = torch.empty(B, T, H, V_DIM, device=device, dtype=torch.float32)\n",
    "    \n",
    "    # Launch kernel\n",
    "    delta_rule_fwd_kernel[(B, H)](\n",
    "        k, v, beta, g, state, out,\n",
    "        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
    "        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
    "        beta.stride(0), beta.stride(1), beta.stride(2),\n",
    "        state.stride(0), state.stride(1), state.stride(2), state.stride(3),\n",
    "        out.stride(0), out.stride(1), out.stride(2), out.stride(3),\n",
    "        T, K_DIM, V_DIM,\n",
    "    )\n",
    "    \n",
    "    return out.to(orig_dtype), state.to(orig_dtype)\n",
    "\n",
    "\n",
    "def triton_delta_rule_backward(k, v, beta, g, initial_state, d_out, d_state_out):\n",
    "    \"\"\"Backward pass wrapper.\"\"\"\n",
    "    B, T, H, K_DIM = k.shape\n",
    "    V_DIM = v.shape[-1]\n",
    "    \n",
    "    k = k.contiguous().float()\n",
    "    v = v.contiguous().float()\n",
    "    beta = beta.contiguous().float()\n",
    "    g = g.contiguous().float()\n",
    "    initial_state = initial_state.contiguous().float()\n",
    "    d_out = d_out.contiguous().float()\n",
    "    d_state_out = d_state_out.contiguous().float()\n",
    "    \n",
    "    d_k = torch.empty_like(k)\n",
    "    d_v = torch.empty_like(v)\n",
    "    d_beta = torch.empty_like(beta)\n",
    "    d_g = torch.empty_like(g)\n",
    "    d_state_in = torch.empty_like(initial_state)\n",
    "    \n",
    "    delta_rule_bwd_kernel[(B, H)](\n",
    "        k, v, beta, g, initial_state,\n",
    "        d_out, d_state_out,\n",
    "        d_k, d_v, d_beta, d_g, d_state_in,\n",
    "        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
    "        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
    "        beta.stride(0), beta.stride(1), beta.stride(2),\n",
    "        initial_state.stride(0), initial_state.stride(1), initial_state.stride(2), initial_state.stride(3),\n",
    "        d_out.stride(0), d_out.stride(1), d_out.stride(2), d_out.stride(3),\n",
    "        d_k.stride(0), d_k.stride(1), d_k.stride(2), d_k.stride(3),\n",
    "        d_v.stride(0), d_v.stride(1), d_v.stride(2), d_v.stride(3),\n",
    "        d_beta.stride(0), d_beta.stride(1), d_beta.stride(2),\n",
    "        d_state_in.stride(0), d_state_in.stride(1), d_state_in.stride(2), d_state_in.stride(3),\n",
    "        T, K_DIM, V_DIM,\n",
    "    )\n",
    "    \n",
    "    return d_k, d_v, d_beta, d_g, d_state_in\n",
    "\n",
    "\n",
    "class DeltaRuleFunction(torch.autograd.Function):\n",
    "    \"\"\"Autograd wrapper connecting Triton forward + backward.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, k, v, beta, g, initial_state):\n",
    "        ctx.save_for_backward(k, v, beta, g, initial_state)\n",
    "        output, final_state = triton_delta_rule(k, v, beta, g, initial_state)\n",
    "        return output, final_state\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output, d_final_state):\n",
    "        k, v, beta, g, initial_state = ctx.saved_tensors\n",
    "        d_k, d_v, d_beta, d_g, d_initial_state = triton_delta_rule_backward(\n",
    "            k, v, beta, g, initial_state, d_output, d_final_state\n",
    "        )\n",
    "        return d_k, d_v, d_beta, d_g, d_initial_state\n",
    "\n",
    "\n",
    "print(\"Triton wrappers + autograd ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "cell-3-components",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model components loaded (GDN, SWA, FFN, RMSNorm).\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Model Components\n",
    "# =============================================================================\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x / rms * self.weight\n",
    "\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    \"\"\"SwiGLU Feed-Forward Network.\"\"\"\n",
    "    def __init__(self, d_model: int, expansion: float = 8/3):\n",
    "        super().__init__()\n",
    "        hidden = ((int(d_model * expansion) + 63) // 64) * 64  # Round to 64\n",
    "        self.w1 = nn.Linear(d_model, hidden, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, hidden, bias=False)\n",
    "        self.w2 = nn.Linear(hidden, d_model, bias=False)\n",
    "        self.norm = RMSNorm(d_model)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.norm(x)\n",
    "        return x + self.w2(F.silu(self.w1(h)) * self.w3(h))\n",
    "\n",
    "\n",
    "class GatedDeltaNetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    GDN layer with Triton kernels.\n",
    "    \n",
    "    TRUE Delta Rule: S_t = g_t * S_{t-1} + β_t * (v_t - S_{t-1}·k_t) ⊗ k_t\n",
    "    \n",
    "    Returns:\n",
    "        output: [B, T, D] with residual connection\n",
    "        state: [B, H, K, V] final state\n",
    "        diag: dict with diagnostic info\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: HybridConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.layer_idx = layer_idx\n",
    "        H, K, V = cfg.n_heads, cfg.head_dim, cfg.value_dim\n",
    "        \n",
    "        # Projections\n",
    "        self.k_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        self.v_proj = nn.Linear(cfg.d_model, H * V, bias=False)\n",
    "        self.o_proj = nn.Linear(H * V, cfg.d_model, bias=False)\n",
    "        \n",
    "        # Gates with biased initialization\n",
    "        self.beta_proj = nn.Linear(cfg.d_model, H, bias=True)\n",
    "        nn.init.constant_(self.beta_proj.bias, cfg.beta_bias)  # sparse writes\n",
    "        \n",
    "        self.g_proj = nn.Linear(cfg.d_model, H, bias=True)\n",
    "        nn.init.constant_(self.g_proj.bias, cfg.g_bias)  # high retention\n",
    "        \n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        \n",
    "    def forward(self, x, initial_state=None):\n",
    "        B, T, D = x.shape\n",
    "        H, K, V = self.cfg.n_heads, self.cfg.head_dim, self.cfg.value_dim\n",
    "        \n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        # Projections\n",
    "        k = self.k_proj(x_norm).view(B, T, H, K)\n",
    "        v = self.v_proj(x_norm).view(B, T, H, V)\n",
    "        \n",
    "        # CRITICAL: L2 normalize keys\n",
    "        k = F.normalize(k.float(), p=2, dim=-1).to(x.dtype)\n",
    "        \n",
    "        # Gates\n",
    "        beta = torch.sigmoid(self.beta_proj(x_norm))  # [B, T, H]\n",
    "        g = torch.sigmoid(self.g_proj(x_norm))        # [B, T, H]\n",
    "        \n",
    "        # Initialize state\n",
    "        if initial_state is None:\n",
    "            state = torch.zeros(B, H, K, V, device=x.device, dtype=x.dtype)\n",
    "        else:\n",
    "            state = initial_state.to(x.dtype)\n",
    "        \n",
    "        # Triton kernel\n",
    "        out, new_state = DeltaRuleFunction.apply(k, v, beta, g, state)\n",
    "        \n",
    "        # Output projection + residual\n",
    "        output = out.to(x.dtype).reshape(B, T, H * V)\n",
    "        output = x + self.o_proj(output)\n",
    "        \n",
    "        # Diagnostics\n",
    "        diag = {\n",
    "            'beta_mean': beta.mean().item(),\n",
    "            'beta_max': beta.max().item(),\n",
    "            'g_mean': g.mean().item(),\n",
    "            'state_norm': new_state.norm().item(),\n",
    "            'state_max': new_state.abs().max().item(),\n",
    "        }\n",
    "        \n",
    "        return output, new_state, diag\n",
    "\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    SWA with state retrieval from GDN.\n",
    "    \n",
    "    Two pathways:\n",
    "        1. Local attention (flash_attn or PyTorch fallback)\n",
    "        2. State retrieval: queries GDN state for global context\n",
    "    \n",
    "    This is the key feature enabling NIAH - the SWA can \"see\" information\n",
    "    stored in GDN state from earlier in the sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: HybridConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        H, K, V = cfg.n_heads, cfg.head_dim, cfg.value_dim\n",
    "        self.head_dim = cfg.d_model // cfg.n_heads\n",
    "        self.scale = K ** -0.5\n",
    "        \n",
    "        # Local attention projections\n",
    "        self.q_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        \n",
    "        # State retrieval (dedicated projection)\n",
    "        self.global_q_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        nn.init.normal_(self.global_q_proj.weight, std=cfg.init_std)\n",
    "        self.retrieval_o_proj = nn.Linear(H * V, cfg.d_model, bias=False)\n",
    "        \n",
    "        # Retrieval gate (starts open)\n",
    "        self.gate_proj = nn.Linear(cfg.d_model, H, bias=True)\n",
    "        nn.init.constant_(self.gate_proj.bias, 1.0)\n",
    "        \n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        \n",
    "    def forward(self, x, gdn_state=None):\n",
    "        B, T, D = x.shape\n",
    "        H = self.cfg.n_heads\n",
    "        K, V, W = self.cfg.head_dim, self.cfg.value_dim, self.cfg.window_size\n",
    "        \n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        # === Local Attention ===\n",
    "        q = self.q_proj(x_norm).view(B, T, H, self.head_dim)\n",
    "        k = self.k_proj(x_norm).view(B, T, H, self.head_dim)\n",
    "        v = self.v_proj(x_norm).view(B, T, H, self.head_dim)\n",
    "        \n",
    "        if FLASH_ATTN_AVAILABLE:\n",
    "            local_out = flash_attn_func(q, k, v, causal=True, window_size=(W, 0))\n",
    "            local_out = local_out.reshape(B, T, D)\n",
    "        else:\n",
    "            # PyTorch fallback\n",
    "            q = q.transpose(1, 2)  # [B, H, T, D]\n",
    "            k = k.transpose(1, 2)\n",
    "            v = v.transpose(1, 2)\n",
    "            \n",
    "            mask = torch.ones(T, T, device=x.device, dtype=torch.bool).triu(1)\n",
    "            mask |= torch.ones(T, T, device=x.device, dtype=torch.bool).tril(-W - 1)\n",
    "            \n",
    "            attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "            attn = attn.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "            local_out = (F.softmax(attn, dim=-1) @ v).transpose(1, 2).reshape(B, T, D)\n",
    "        \n",
    "        local_out = self.o_proj(local_out)\n",
    "        \n",
    "        # === State Retrieval ===\n",
    "        retrieval_out = torch.zeros_like(x)\n",
    "        gate_mean = 0.0\n",
    "        \n",
    "        if gdn_state is not None:\n",
    "            # Query GDN state for global retrieval\n",
    "            q_g = self.global_q_proj(x_norm).view(B, T, H, K).transpose(1, 2)  # [B, H, T, K]\n",
    "            q_g = F.relu(q_g)  # Sparse queries (only positive activations retrieve)\n",
    "            \n",
    "            # Retrieve from state: [B, H, K, V] @ [B, H, T, K]^T -> [B, H, T, V]\n",
    "            retrieved = torch.einsum('bhkv,bhtk->bhtv', gdn_state.to(x.dtype), q_g)\n",
    "            retrieved = retrieved.transpose(1, 2).reshape(B, T, H * V)\n",
    "            retrieval_out = self.retrieval_o_proj(retrieved)\n",
    "            \n",
    "            # Gate modulates retrieval\n",
    "            gate = torch.sigmoid(self.gate_proj(x_norm))  # [B, T, H]\n",
    "            gate_mean = gate.mean().item()\n",
    "            retrieval_out = gate.mean(dim=-1, keepdim=True) * retrieval_out\n",
    "        \n",
    "        out = x + local_out + retrieval_out\n",
    "        \n",
    "        diag = {\n",
    "            'gate_mean': gate_mean,\n",
    "            'local_norm': local_out.norm().item(),\n",
    "            'retrieval_norm': retrieval_out.norm().item() if gdn_state is not None else 0.0,\n",
    "        }\n",
    "        \n",
    "        return out, diag\n",
    "\n",
    "\n",
    "print(\"Model components loaded (GDN, SWA, FFN, RMSNorm).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "cell-4-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransparentHybrid model ready.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Model Assembly - TransparentHybrid\n",
    "# =============================================================================\n",
    "\n",
    "class TransparentHybrid(nn.Module):\n",
    "    \"\"\"\n",
    "    GDN + SWA hybrid model with full visibility into state flow.\n",
    "    \n",
    "    Information Flow:\n",
    "        - GDN layers compress sequence into state S_t [H, K, V]\n",
    "        - State flows to subsequent SWA layers for retrieval\n",
    "        - SWA provides precision retrieval (window + global via state)\n",
    "    \n",
    "    Layer pattern examples:\n",
    "        \"GS\"       - 2 layers: GDN, SWA (minimal)\n",
    "        \"GGS\"      - 3 layers: 2 GDN, 1 SWA\n",
    "        \"GGSG\"     - 4 layers: GDN, GDN, SWA, GDN\n",
    "        \"GGGSGGGS\" - 8 layers: sparse SWA placement\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: HybridConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Token embedding\n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        nn.init.normal_(self.embed.weight, std=cfg.init_std)\n",
    "        \n",
    "        # Build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        \n",
    "        for i, lt in enumerate(cfg.layer_pattern):\n",
    "            if lt == 'G':\n",
    "                self.layers.append(GatedDeltaNetLayer(cfg, i))\n",
    "            elif lt == 'S':\n",
    "                self.layers.append(SlidingWindowAttention(cfg, i))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {lt}\")\n",
    "            self.ffns.append(SwiGLUFFN(cfg.d_model))\n",
    "        \n",
    "        # Output\n",
    "        self.norm_f = RMSNorm(cfg.d_model)\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying\n",
    "        self.lm_head.weight = self.embed.weight\n",
    "        \n",
    "    class TransparentHybrid(nn.Module):\n",
    "        \"\"\"\n",
    "        GDN + SWA hybrid model with full visibility into state flow.\n",
    "        Includes memory-safe diagnostic collection.\n",
    "        \"\"\"\n",
    "    def __init__(self, cfg: HybridConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Token embedding\n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        nn.init.normal_(self.embed.weight, std=cfg.init_std)\n",
    "        \n",
    "        # Build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        \n",
    "        for i, lt in enumerate(cfg.layer_pattern):\n",
    "            if lt == 'G':\n",
    "                self.layers.append(GatedDeltaNetLayer(cfg, i))\n",
    "            elif lt == 'S':\n",
    "                self.layers.append(SlidingWindowAttention(cfg, i))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {lt}\")\n",
    "            self.ffns.append(SwiGLUFFN(cfg.d_model))\n",
    "        \n",
    "        # Output\n",
    "        self.norm_f = RMSNorm(cfg.d_model)\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying\n",
    "        self.lm_head.weight = self.embed.weight\n",
    "        \n",
    "    def forward(self, input_ids, targets=None, return_diagnostics=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [B, T] token indices\n",
    "            targets: [B, T] optional target indices for loss\n",
    "            return_diagnostics: whether to return layer diagnostics\n",
    "            \n",
    "        Returns:\n",
    "            logits: [B, T, vocab_size]\n",
    "            loss: scalar if targets provided\n",
    "            all_diag: list of layer diagnostics (DETACHED from graph)\n",
    "            state: final GDN state (DETACHED from graph)\n",
    "        \"\"\"\n",
    "        x = self.embed(input_ids)\n",
    "        state = None\n",
    "        all_diag = []\n",
    "        \n",
    "        # Track the \"current\" GDN state for SWA layers to use\n",
    "        current_gdn_state = None\n",
    "        \n",
    "        for i, (layer, ffn) in enumerate(zip(self.layers, self.ffns)):\n",
    "            lt = self.cfg.layer_pattern[i]\n",
    "            if lt == 'G':\n",
    "                x, state, diag = layer(x, initial_state=state)\n",
    "                current_gdn_state = state # Keep attached for SWA next step\n",
    "                \n",
    "                # FIXED: Copy ALL keys required by run_full_diagnostic\n",
    "                if return_diagnostics:\n",
    "                    safe_diag = {\n",
    "                        'layer': lt,\n",
    "                        'layer_idx': i,\n",
    "                        'beta_mean': diag['beta_mean'],\n",
    "                        'beta_max': diag.get('beta_max', 0.0),   # ADDED\n",
    "                        'g_mean': diag['g_mean'],\n",
    "                        'state_norm': diag['state_norm'],\n",
    "                        'state_max': diag.get('state_max', 0.0)  # ADDED\n",
    "                    }\n",
    "                    all_diag.append(safe_diag)\n",
    "                \n",
    "            else: # SWA\n",
    "                x, diag = layer(x, gdn_state=current_gdn_state)\n",
    "                \n",
    "                if return_diagnostics:\n",
    "                    safe_diag = {\n",
    "                        'layer': lt,\n",
    "                        'layer_idx': i,\n",
    "                        'gate_mean': diag['gate_mean'],\n",
    "                        'local_norm': diag['local_norm'],\n",
    "                        'retrieval_norm': diag['retrieval_norm']\n",
    "                    }\n",
    "                    all_diag.append(safe_diag)\n",
    "            \n",
    "            x = ffn(x)\n",
    "        \n",
    "        x = self.norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), \n",
    "                targets.view(-1), \n",
    "                ignore_index=-100\n",
    "            )\n",
    "        \n",
    "        # Return detached state to stop memory leaks\n",
    "        return logits, loss, all_diag, state.detach() if state is not None else None\n",
    "\n",
    "print(\"TransparentHybrid model ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "cell-5-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading utilities ready.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Data Loading\n",
    "# =============================================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Simple dataset for language modeling.\"\"\"\n",
    "    def __init__(self, tokens, seq_len=128):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.tokens) - 1) // self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_len\n",
    "        return torch.tensor(self.tokens[start:start + self.seq_len + 1], dtype=torch.long)\n",
    "\n",
    "\n",
    "def load_data(n_tokens=500_000, seq_len=128, batch_size=16):\n",
    "    \"\"\"Load wikitext data for training.\"\"\"\n",
    "    print(f\"Loading {n_tokens:,} tokens from wikitext-103...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train')\n",
    "    \n",
    "    all_tokens = []\n",
    "    for item in dataset:\n",
    "        if item['text'].strip():\n",
    "            all_tokens.extend(tokenizer.encode(item['text']))\n",
    "            if len(all_tokens) >= n_tokens:\n",
    "                break\n",
    "    \n",
    "    all_tokens = all_tokens[:n_tokens]\n",
    "    print(f\"Loaded {len(all_tokens):,} tokens\")\n",
    "    \n",
    "    ds = TextDataset(all_tokens, seq_len)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "# Uncomment to load data:\n",
    "# data_loader = load_data(n_tokens=500_000, seq_len=128, batch_size=16)\n",
    "print(\"Data loading utilities ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "cell-6-niah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIAH testing suite loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: NIAH Testing Suite\n",
    "# =============================================================================\n",
    "#\n",
    "# Needle-In-A-Haystack tests measure the model's ability to:\n",
    "#   1. Store information at an early position (MARKER + NEEDLE)\n",
    "#   2. Retrieve it when cued at a later position (CUE → predict NEEDLE)\n",
    "#\n",
    "# This tests the GDN → SWA retrieval pathway.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "def proper_niah_test(model, seq_len=128, needle_pos=32, n_trials=30):\n",
    "    \"\"\"NIAH test with MARKER + CUE tokens.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    \n",
    "    correct = 0\n",
    "    for _ in range(n_trials):\n",
    "        needle_id = cfg.vocab_size - 3  # Use a specific token as needle\n",
    "        seq = torch.randint(0, cfg.vocab_size - 100, (1, seq_len), device=device)\n",
    "        \n",
    "        # Place MARKER + NEEDLE at needle_pos\n",
    "        seq[0, needle_pos] = cfg.marker_token\n",
    "        seq[0, needle_pos + 1] = needle_id\n",
    "        \n",
    "        # Place CUE at end\n",
    "        seq[0, -1] = cfg.cue_token\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _, _, _ = model(seq)\n",
    "        \n",
    "        # Check if model predicts needle_id after CUE\n",
    "        pred = logits[0, -1].argmax().item()\n",
    "        if pred == needle_id:\n",
    "            correct += 1\n",
    "    \n",
    "    acc = correct / n_trials\n",
    "    print(f\"  Accuracy: {acc*100:.1f}% ({correct}/{n_trials})\")\n",
    "    return {'accuracy': acc, 'correct': correct, 'total': n_trials}\n",
    "\n",
    "\n",
    "def test_niah_by_distance(model, distances=[5, 10, 20, 40, 60, 95], n_trials=20, seq_len=128):\n",
    "    \"\"\"\n",
    "    Test retrieval across varying distances.\n",
    "    \n",
    "    Distance = (seq_len - 1) - needle_pos = how far back the needle is from CUE\n",
    "    \"\"\"\n",
    "    print(f\"\\nNIAH by Distance (seq_len={seq_len}):\")\n",
    "    results = {}\n",
    "    \n",
    "    for dist in distances:\n",
    "        needle_pos = max(2, seq_len - dist - 2)\n",
    "        print(f\"  Distance {dist:3d} (pos={needle_pos:3d}): \", end=\"\")\n",
    "        result = proper_niah_test(model, seq_len=seq_len, needle_pos=needle_pos, n_trials=n_trials)\n",
    "        results[dist] = result\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_full_diagnostic(model, seq_len=128, needle_pos=32):\n",
    "    \"\"\"\n",
    "    Comprehensive diagnostic with state health check.\n",
    "    \n",
    "    Checks:\n",
    "        - State norm (should be bounded, not exploding)\n",
    "        - Gate values (β and g activation patterns)\n",
    "        - Retrieval pathway activation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    \n",
    "    # Create test sequence\n",
    "    needle_id = cfg.vocab_size - 3\n",
    "    seq = torch.randint(0, cfg.vocab_size - 100, (1, seq_len), device=device)\n",
    "    seq[0, needle_pos] = cfg.marker_token\n",
    "    seq[0, needle_pos + 1] = needle_id\n",
    "    seq[0, -1] = cfg.cue_token\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, _, diags, state = model(seq, return_diagnostics=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DIAGNOSTIC REPORT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # State health\n",
    "    print(f\"\\nState Health:\")\n",
    "    print(f\"  State norm: {state.norm().item():.2f}\")\n",
    "    print(f\"  State max:  {state.abs().max().item():.2f}\")\n",
    "    \n",
    "    if state.abs().max().item() < 10:\n",
    "        print(f\"  ✓ State bounded - Delta Rule working!\")\n",
    "    elif state.abs().max().item() < 100:\n",
    "        print(f\"  ⚠ State moderately large - monitor during training\")\n",
    "    else:\n",
    "        print(f\"  ✗ State explosion detected - check Delta Rule!\")\n",
    "    \n",
    "    # Layer diagnostics\n",
    "    print(f\"\\nLayer Diagnostics:\")\n",
    "    for i, d in enumerate(diags):\n",
    "        if d['layer'] == 'G':\n",
    "            print(f\"  Layer {i} (GDN): β={d['beta_mean']:.3f} (max={d['beta_max']:.3f}), \"\n",
    "                  f\"g={d['g_mean']:.3f}, state_norm={d['state_norm']:.2f}\")\n",
    "        else:\n",
    "            print(f\"  Layer {i} (SWA): gate={d['gate_mean']:.3f}, \"\n",
    "                  f\"local={d['local_norm']:.2f}, retrieval={d['retrieval_norm']:.2f}\")\n",
    "    \n",
    "    # Prediction check\n",
    "    pred = logits[0, -1].argmax().item()\n",
    "    print(f\"\\nPrediction: {pred} (target: {needle_id}) - {'✓' if pred == needle_id else '✗'}\")\n",
    "    \n",
    "    return state, diags\n",
    "\n",
    "\n",
    "print(\"NIAH testing suite loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "cell-7-delta-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta Rule validation suite loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Delta Rule Validation Suite\n",
    "# =============================================================================\n",
    "#\n",
    "# These tests validate the correctness of the TRUE Delta Rule implementation.\n",
    "# Understanding these behaviors is CRITICAL for debugging and tuning.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "def test_identical_tokens():\n",
    "    \"\"\"\n",
    "    TEST: Identical tokens should produce zero error on second write.\n",
    "    \n",
    "    This is the KEY property of Delta Rule vs naive outer product.\n",
    "    If this fails, you don't have TRUE Delta Rule.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST: Identical Tokens (Redundancy Suppression)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    B, H, K, V = 1, 4, 32, 64\n",
    "    device = DEVICE\n",
    "    \n",
    "    state = torch.zeros(B, H, K, V, device=device)\n",
    "    k = F.normalize(torch.randn(B, H, K, device=device), dim=-1)\n",
    "    v = torch.randn(B, H, V, device=device)\n",
    "    \n",
    "    # First write\n",
    "    pred1 = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "    error1 = v - pred1\n",
    "    update1 = torch.einsum('bhv,bhk->bhkv', error1, k)\n",
    "    state = state + update1\n",
    "    norm1 = state.norm().item()\n",
    "    \n",
    "    # Second write (SAME k, v)\n",
    "    pred2 = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "    error2 = v - pred2\n",
    "    update2 = torch.einsum('bhv,bhk->bhkv', error2, k)\n",
    "    state = state + update2\n",
    "    norm2 = state.norm().item()\n",
    "    \n",
    "    error_ratio = error2.norm().item() / (error1.norm().item() + 1e-8)\n",
    "    growth = norm2 / norm1\n",
    "    \n",
    "    print(f\"  Error1 norm: {error1.norm().item():.4f}\")\n",
    "    print(f\"  Error2 norm: {error2.norm().item():.6f} (should be ~0)\")\n",
    "    print(f\"  Error ratio: {error_ratio:.6f}\")\n",
    "    print(f\"  State growth: {growth:.4f}x (should be ~1.0)\")\n",
    "    \n",
    "    passed = error_ratio < 0.001 and growth < 1.01\n",
    "    print(f\"  → {'✓ PASS' if passed else '✗ FAIL'}\")\n",
    "    return passed\n",
    "\n",
    "\n",
    "def test_orthogonal_keys():\n",
    "    \"\"\"\n",
    "    TEST: Orthogonal keys should store independently without interference.\n",
    "    \n",
    "    This tests the theoretical best case for associative memory.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST: Orthogonal Keys (Independent Storage)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    B, H, K, V = 1, 1, 32, 64\n",
    "    device = DEVICE\n",
    "    \n",
    "    state = torch.zeros(B, H, K, V, device=device)\n",
    "    \n",
    "    # Two orthogonal keys (unit vectors)\n",
    "    k1 = torch.zeros(B, H, K, device=device)\n",
    "    k1[0, 0, 0] = 1.0\n",
    "    k2 = torch.zeros(B, H, K, device=device)\n",
    "    k2[0, 0, 1] = 1.0\n",
    "    \n",
    "    v1 = torch.randn(B, H, V, device=device)\n",
    "    v2 = torch.randn(B, H, V, device=device)\n",
    "    \n",
    "    # Write v1 at k1\n",
    "    pred = torch.einsum('bhkv,bhk->bhv', state, k1)\n",
    "    error = v1 - pred\n",
    "    update = torch.einsum('bhv,bhk->bhkv', error, k1)\n",
    "    state = state + update\n",
    "    \n",
    "    # Write v2 at k2\n",
    "    pred = torch.einsum('bhkv,bhk->bhv', state, k2)\n",
    "    error = v2 - pred\n",
    "    update = torch.einsum('bhv,bhk->bhkv', error, k2)\n",
    "    state = state + update\n",
    "    \n",
    "    # Retrieve\n",
    "    retrieved_v1 = torch.einsum('bhkv,bhk->bhv', state, k1)\n",
    "    retrieved_v2 = torch.einsum('bhkv,bhk->bhv', state, k2)\n",
    "    \n",
    "    error_v1 = (retrieved_v1 - v1).norm().item() / v1.norm().item()\n",
    "    error_v2 = (retrieved_v2 - v2).norm().item() / v2.norm().item()\n",
    "    \n",
    "    print(f\"  v1 retrieval error: {error_v1:.6f} (should be ~0)\")\n",
    "    print(f\"  v2 retrieval error: {error_v2:.6f} (should be ~0)\")\n",
    "    \n",
    "    passed = error_v1 < 0.001 and error_v2 < 0.001\n",
    "    print(f\"  → {'✓ PASS' if passed else '✗ FAIL'}\")\n",
    "    return passed\n",
    "\n",
    "\n",
    "def test_interference():\n",
    "    \"\"\"\n",
    "    TEST: Similar (non-orthogonal) keys cause interference.\n",
    "    \n",
    "    This documents EXPECTED behavior - not a failure!\n",
    "    Understanding interference is key to capacity planning.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST: Key Interference (Expected Behavior)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    B, H, K, V = 1, 1, 32, 64\n",
    "    device = DEVICE\n",
    "    \n",
    "    state = torch.zeros(B, H, K, V, device=device)\n",
    "    \n",
    "    # Two similar keys (high dot product)\n",
    "    k1 = F.normalize(torch.randn(B, H, K, device=device), dim=-1)\n",
    "    k2 = k1 + 0.1 * F.normalize(torch.randn(B, H, K, device=device), dim=-1)\n",
    "    k2 = F.normalize(k2, dim=-1)\n",
    "    \n",
    "    dot_product = (k1 * k2).sum().item()\n",
    "    \n",
    "    v1 = torch.randn(B, H, V, device=device)\n",
    "    v2 = torch.randn(B, H, V, device=device)\n",
    "    \n",
    "    # Write v1 at k1\n",
    "    pred = torch.einsum('bhkv,bhk->bhv', state, k1)\n",
    "    error = v1 - pred\n",
    "    update = torch.einsum('bhv,bhk->bhkv', error, k1)\n",
    "    state = state + update\n",
    "    \n",
    "    # Retrieve v1 BEFORE writing v2\n",
    "    retrieved_v1_before = torch.einsum('bhkv,bhk->bhv', state, k1)\n",
    "    error_before = (retrieved_v1_before - v1).norm().item() / v1.norm().item()\n",
    "    \n",
    "    # Write v2 at k2 (similar key)\n",
    "    pred = torch.einsum('bhkv,bhk->bhv', state, k2)\n",
    "    error = v2 - pred\n",
    "    update = torch.einsum('bhv,bhk->bhkv', error, k2)\n",
    "    state = state + update\n",
    "    \n",
    "    # Retrieve v1 AFTER writing v2\n",
    "    retrieved_v1_after = torch.einsum('bhkv,bhk->bhv', state, k1)\n",
    "    error_after = (retrieved_v1_after - v1).norm().item() / v1.norm().item()\n",
    "    \n",
    "    print(f\"  Key similarity (dot product): {dot_product:.4f}\")\n",
    "    print(f\"  v1 error BEFORE v2 write: {error_before:.6f}\")\n",
    "    print(f\"  v1 error AFTER v2 write:  {error_after:.4f}\")\n",
    "    print(f\"  Interference occurred: {error_after > error_before}\")\n",
    "    print(f\"  → This is EXPECTED: Similar keys interfere\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_capacity_limit():\n",
    "    \"\"\"\n",
    "    TEST: State behavior under many writes.\n",
    "    \n",
    "    Shows how state norm grows and early items degrade.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST: Capacity Limit (State Saturation)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    B, H, K, V = 1, 1, 32, 64\n",
    "    device = DEVICE\n",
    "    \n",
    "    n_writes = [10, 50, 100, 200]\n",
    "    results = []\n",
    "    \n",
    "    for n in n_writes:\n",
    "        state = torch.zeros(B, H, K, V, device=device)\n",
    "        keys, values = [], []\n",
    "        \n",
    "        for i in range(n):\n",
    "            k = F.normalize(torch.randn(B, H, K, device=device), dim=-1)\n",
    "            v = torch.randn(B, H, V, device=device)\n",
    "            keys.append(k)\n",
    "            values.append(v)\n",
    "            \n",
    "            pred = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "            error = v - pred\n",
    "            update = torch.einsum('bhv,bhk->bhkv', error, k)\n",
    "            state = state + update\n",
    "        \n",
    "        # Test retrieval of first and last items\n",
    "        retrieved_first = torch.einsum('bhkv,bhk->bhv', state, keys[0])\n",
    "        error_first = (retrieved_first - values[0]).norm().item() / values[0].norm().item()\n",
    "        \n",
    "        retrieved_last = torch.einsum('bhkv,bhk->bhv', state, keys[-1])\n",
    "        error_last = (retrieved_last - values[-1]).norm().item() / values[-1].norm().item()\n",
    "        \n",
    "        print(f\"  n={n:3d}: state_norm={state.norm().item():.2f}, \"\n",
    "              f\"first_err={error_first:.4f}, last_err={error_last:.4f}\")\n",
    "        \n",
    "        results.append({'n': n, 'state_norm': state.norm().item(),\n",
    "                       'first_error': error_first, 'last_error': error_last})\n",
    "    \n",
    "    print(f\"\\n  → State grows with writes, early items degrade (expected)\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def test_forget_gate():\n",
    "    \"\"\"\n",
    "    TEST: Forget gate controls decay.\n",
    "    \n",
    "    Shows how different g values affect information retention.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST: Forget Gate Effect\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    B, H, K, V = 1, 1, 32, 64\n",
    "    device = DEVICE\n",
    "    \n",
    "    k = F.normalize(torch.randn(B, H, K, device=device), dim=-1)\n",
    "    v = torch.randn(B, H, V, device=device)\n",
    "    \n",
    "    for g_val in [1.0, 0.9, 0.5, 0.1]:\n",
    "        state = torch.zeros(B, H, K, V, device=device)\n",
    "        g = torch.full((B, H), g_val, device=device)\n",
    "        \n",
    "        # Write once\n",
    "        pred = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "        error = v - pred\n",
    "        update = torch.einsum('bhv,bhk->bhkv', error, k)\n",
    "        state = g.unsqueeze(-1).unsqueeze(-1) * state + update\n",
    "        \n",
    "        # Apply 10 more \"noise\" steps\n",
    "        k_noise = F.normalize(torch.randn(B, H, K, device=device), dim=-1)\n",
    "        v_zero = torch.zeros(B, H, V, device=device)\n",
    "        \n",
    "        for _ in range(10):\n",
    "            pred = torch.einsum('bhkv,bhk->bhv', state, k_noise)\n",
    "            error = v_zero - pred\n",
    "            update = torch.einsum('bhv,bhk->bhkv', error, k_noise)\n",
    "            state = g.unsqueeze(-1).unsqueeze(-1) * state + update\n",
    "        \n",
    "        # Try to retrieve original\n",
    "        retrieved = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "        retention = (retrieved * v).sum().item() / (v.norm().item() ** 2)\n",
    "        \n",
    "        print(f\"  g={g_val:.1f}: retention after 10 steps = {retention:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  → Lower g = faster decay\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def run_all_validations():\n",
    "    \"\"\"Run complete Delta Rule validation suite.\"\"\"\n",
    "    print(\"\\n\" + \"#\" * 70)\n",
    "    print(\"# DELTA RULE VALIDATION SUITE\")\n",
    "    print(\"#\" * 70)\n",
    "    \n",
    "    results = {\n",
    "        'identical_tokens': test_identical_tokens(),\n",
    "        'orthogonal_keys': test_orthogonal_keys(),\n",
    "        'interference': test_interference(),\n",
    "        'capacity': test_capacity_limit(),\n",
    "        'forget_gate': test_forget_gate(),\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VALIDATION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    for name, passed in results.items():\n",
    "        if isinstance(passed, bool):\n",
    "            status = \"✓ PASS\" if passed else \"✗ FAIL\"\n",
    "        else:\n",
    "            status = \"INFO\"\n",
    "        print(f\"  {name}: {status}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Delta Rule validation suite loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "cell-8-training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training infrastructure loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Training Infrastructure\n",
    "# =============================================================================\n",
    "#\n",
    "# CURRICULUM LEARNING:\n",
    "#   Phase 1 (warmup): Pure retrieval loss\n",
    "#       - Teaches the model to use MARKER/CUE pattern\n",
    "#       - Establishes GDN → SWA retrieval pathway\n",
    "#   \n",
    "#   Phase 2 (mixed): LM loss + weighted retrieval loss\n",
    "#       - Maintains retrieval capability\n",
    "#       - Adds language modeling objective\n",
    "#\n",
    "# This two-phase approach prevents the model from \"forgetting\" how to \n",
    "# retrieve once LM training begins.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "def compute_retrieval_loss(model, seq_len=128, batch_size=4):\n",
    "    \"\"\"\n",
    "    Synthetic retrieval task for gradient signal.\n",
    "    \n",
    "    Creates sequences with MARKER + NEEDLE → CUE pattern.\n",
    "    Loss is only computed at the CUE position.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    \n",
    "    needle_id = cfg.vocab_size - 3\n",
    "    tokens = torch.randint(0, cfg.vocab_size - 100, (batch_size, seq_len), device=device)\n",
    "    \n",
    "    # Place MARKER + NEEDLE at random positions\n",
    "    for i in range(batch_size):\n",
    "        pos = torch.randint(5, seq_len - 10, (1,)).item()\n",
    "        tokens[i, pos] = cfg.marker_token\n",
    "        tokens[i, pos + 1] = needle_id\n",
    "    \n",
    "    # Place CUE at end\n",
    "    tokens[:, -1] = cfg.cue_token\n",
    "    \n",
    "    # Target: predict needle_id after CUE\n",
    "    targets = torch.full((batch_size, seq_len), -100, device=device)  # -100 = ignore\n",
    "    targets[:, -1] = needle_id\n",
    "    \n",
    "    _, loss, _, _ = model(tokens, targets=targets)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_curriculum(model, data_loader, steps=1000, warmup_steps=200,\n",
    "                     lr=3e-4, retrieval_weight=2.0, log_interval=100):\n",
    "    \"\"\"\n",
    "    Curriculum training: retrieval warmup → mixed LM/retrieval.\n",
    "    \n",
    "    Args:\n",
    "        model: TransparentHybrid\n",
    "        data_loader: DataLoader for LM data\n",
    "        steps: total training steps\n",
    "        warmup_steps: steps for pure retrieval training\n",
    "        lr: learning rate\n",
    "        retrieval_weight: weight for retrieval loss in mixed phase\n",
    "        log_interval: steps between log messages\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    lm_iter = iter(data_loader)\n",
    "    history = {'step': [], 'lm': [], 'ret': [], 'phase': []}\n",
    "    \n",
    "    print(f\"Training {steps} steps ({warmup_steps} warmup)\")\n",
    "    print(f\"  LR: {lr}, Retrieval weight: {retrieval_weight}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Phase 1: Pure retrieval (warmup)\n",
    "        if step < warmup_steps:\n",
    "            ret_loss = compute_retrieval_loss(model)\n",
    "            ret_loss.backward()\n",
    "            history['ret'].append(ret_loss.item())\n",
    "            history['lm'].append(0)\n",
    "            history['phase'].append('warmup')\n",
    "        \n",
    "        # Phase 2: Mixed LM + retrieval\n",
    "        else:\n",
    "            try:\n",
    "                batch = next(lm_iter)\n",
    "            except StopIteration:\n",
    "                lm_iter = iter(data_loader)\n",
    "                batch = next(lm_iter)\n",
    "            \n",
    "            input_ids = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "            _, lm_loss, _, _ = model(input_ids, targets)\n",
    "            \n",
    "            ret_loss = compute_retrieval_loss(model)\n",
    "            \n",
    "            total = lm_loss + retrieval_weight * ret_loss\n",
    "            total.backward()\n",
    "            \n",
    "            history['lm'].append(lm_loss.item())\n",
    "            history['ret'].append(ret_loss.item())\n",
    "            history['phase'].append('mixed')\n",
    "        \n",
    "        history['step'].append(step)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0) ### adjusted from 1.0 to 2.0\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        if step % log_interval == 0:\n",
    "            phase = \"WARMUP\" if step < warmup_steps else \"MIXED\"\n",
    "            lm = history['lm'][-1]\n",
    "            ret = history['ret'][-1]\n",
    "            elapsed = time.time() - start_time\n",
    "            steps_per_sec = (step + 1) / elapsed\n",
    "            print(f\"[{phase:6s}] Step {step:5d}: LM={lm:6.3f} RET={ret:6.3f} ({steps_per_sec:.1f} steps/s)\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Training complete: {steps} steps in {total_time:.1f}s ({steps/total_time:.1f} steps/s)\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"Training infrastructure loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "cell-9-gradients",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient analysis loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Gradient Analysis\n",
    "# =============================================================================\n",
    "#\n",
    "# Understanding gradient flow is CRITICAL for debugging training issues.\n",
    "#\n",
    "# Watch for:\n",
    "#   - NaN/Inf in gradients (numerical instability)\n",
    "#   - Very small gradients in early layers (vanishing)\n",
    "#   - Very large gradients in specific components (exploding)\n",
    "#   - Zero gradients in expected-trainable parameters\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_gradients(model, seq_len=64, verbose=True):\n",
    "    \"\"\"\n",
    "    Analyze gradient flow through the model.\n",
    "    \n",
    "    Returns dict of gradient norms by parameter name.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.train()\n",
    "    \n",
    "    # Create retrieval task input\n",
    "    x = torch.randint(0, model.cfg.vocab_size - 100, (2, seq_len), device=device)\n",
    "    x[:, 10] = model.cfg.marker_token\n",
    "    x[:, 11] = model.cfg.vocab_size - 3  # needle\n",
    "    x[:, -1] = model.cfg.cue_token\n",
    "    \n",
    "    targets = torch.full((2, seq_len), -100, device=device)\n",
    "    targets[:, -1] = model.cfg.vocab_size - 3\n",
    "    \n",
    "    # Forward + backward\n",
    "    model.zero_grad()\n",
    "    _, loss, _, _ = model(x, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Collect gradient info\n",
    "    grad_info = {}\n",
    "    has_nan = False\n",
    "    has_inf = False\n",
    "    has_zero = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            grad_info[name] = grad_norm\n",
    "            \n",
    "            if torch.isnan(param.grad).any():\n",
    "                has_nan = True\n",
    "            if torch.isinf(param.grad).any():\n",
    "                has_inf = True\n",
    "            if grad_norm < 1e-10:\n",
    "                has_zero.append(name)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"GRADIENT ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nLoss: {loss.item():.4f}\")\n",
    "        print(f\"\\nGradient norms by component:\")\n",
    "        \n",
    "        # Group by layer\n",
    "        for name, norm in sorted(grad_info.items()):\n",
    "            if 'proj' in name or 'embed' in name or 'lm_head' in name:\n",
    "                print(f\"  {name:40s}: {norm:.6f}\")\n",
    "        \n",
    "        print(f\"\\n  NaN in gradients: {'✗ YES' if has_nan else '✓ NO'}\")\n",
    "        print(f\"  Inf in gradients: {'✗ YES' if has_inf else '✓ NO'}\")\n",
    "        \n",
    "        if has_zero:\n",
    "            print(f\"  Zero gradients: {len(has_zero)} parameters\")\n",
    "            for name in has_zero[:5]:\n",
    "                print(f\"    - {name}\")\n",
    "        else:\n",
    "            print(f\"  Zero gradients: ✓ NONE\")\n",
    "    \n",
    "    return {\n",
    "        'grad_norms': grad_info,\n",
    "        'has_nan': has_nan,\n",
    "        'has_inf': has_inf,\n",
    "        'has_zero': has_zero,\n",
    "        'loss': loss.item(),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Gradient analysis loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "cell-10-profiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance profiling loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: Performance Profiling\n",
    "# =============================================================================\n",
    "#\n",
    "# Profile both:\n",
    "#   1. Triton kernel alone\n",
    "#   2. Full model forward pass\n",
    "#   3. Comparison with FLA (if available)\n",
    "#\n",
    "# Key metrics:\n",
    "#   - ms per forward pass\n",
    "#   - tokens per second\n",
    "#   - speedup vs PyTorch/FLA\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "def profile_triton_kernel(batch_sizes=[1, 4, 8], seq_lens=[64, 128, 256, 512],\n",
    "                          n_heads=8, head_dim=32, value_dim=64,\n",
    "                          n_warmup=3, n_runs=10):\n",
    "    \"\"\"Profile Triton Delta Rule kernel.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"TRITON KERNEL PROFILING\")\n",
    "    print(f\"Config: H={n_heads}, K={head_dim}, V={value_dim}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    device = DEVICE\n",
    "    results = {}\n",
    "    \n",
    "    for B in batch_sizes:\n",
    "        for T in seq_lens:\n",
    "            k = F.normalize(torch.randn(B, T, n_heads, head_dim, device=device), dim=-1)\n",
    "            v = torch.randn(B, T, n_heads, value_dim, device=device)\n",
    "            beta = torch.sigmoid(torch.randn(B, T, n_heads, device=device))\n",
    "            g = torch.sigmoid(torch.randn(B, T, n_heads, device=device))\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(n_warmup):\n",
    "                triton_delta_rule(k, v, beta, g)\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Timed runs\n",
    "            times = []\n",
    "            for _ in range(n_runs):\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.perf_counter()\n",
    "                triton_delta_rule(k, v, beta, g)\n",
    "                torch.cuda.synchronize()\n",
    "                times.append(time.perf_counter() - start)\n",
    "            \n",
    "            avg_ms = sum(times) / len(times) * 1000\n",
    "            tokens_per_sec = (B * T) / (sum(times) / len(times))\n",
    "            \n",
    "            results[(B, T)] = {'avg_ms': avg_ms, 'tokens_per_sec': tokens_per_sec}\n",
    "            print(f\"B={B:2d}, T={T:4d}: {avg_ms:8.2f} ms | {tokens_per_sec:>12,.0f} tok/s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def profile_fla_kernel(batch_sizes=[1, 4, 8], seq_lens=[64, 128, 256, 512],\n",
    "                       n_heads=8, head_dim=32, value_dim=64,\n",
    "                       n_warmup=3, n_runs=10):\n",
    "    \"\"\"Profile FLA chunked kernel for comparison.\"\"\"\n",
    "    if not HAS_FLA:\n",
    "        print(\"FLA not available for comparison\")\n",
    "        return {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FLA CHUNKED KERNEL (for comparison)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    device = DEVICE\n",
    "    results = {}\n",
    "    \n",
    "    for B in batch_sizes:\n",
    "        for T in seq_lens:\n",
    "            q = torch.randn(B, T, n_heads, head_dim, device=device)\n",
    "            k = F.normalize(torch.randn(B, T, n_heads, head_dim, device=device), dim=-1)\n",
    "            v = torch.randn(B, T, n_heads, value_dim, device=device)\n",
    "            beta = torch.sigmoid(torch.randn(B, T, n_heads, device=device))\n",
    "            g = torch.sigmoid(torch.randn(B, T, n_heads, device=device))\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(n_warmup):\n",
    "                _, _ = chunk_gated_delta_rule(q, k, v, g, beta, output_final_state=True)\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Timed\n",
    "            times = []\n",
    "            for _ in range(n_runs):\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.perf_counter()\n",
    "                _, _ = chunk_gated_delta_rule(q, k, v, g, beta, output_final_state=True)\n",
    "                torch.cuda.synchronize()\n",
    "                times.append(time.perf_counter() - start)\n",
    "            \n",
    "            avg_ms = sum(times) / len(times) * 1000\n",
    "            tokens_per_sec = (B * T) / (sum(times) / len(times))\n",
    "            \n",
    "            results[(B, T)] = {'avg_ms': avg_ms, 'tokens_per_sec': tokens_per_sec}\n",
    "            print(f\"B={B:2d}, T={T:4d}: {avg_ms:8.2f} ms | {tokens_per_sec:>12,.0f} tok/s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def profile_full_model(model, batch_sizes=[1, 4], seq_lens=[64, 128, 256],\n",
    "                       n_warmup=3, n_runs=10):\n",
    "    \"\"\"Profile full model forward pass.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FULL MODEL PROFILING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    for B in batch_sizes:\n",
    "        for T in seq_lens:\n",
    "            x = torch.randint(0, model.cfg.vocab_size, (B, T), device=device)\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(n_warmup):\n",
    "                with torch.no_grad():\n",
    "                    model(x)\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Timed\n",
    "            times = []\n",
    "            for _ in range(n_runs):\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.perf_counter()\n",
    "                with torch.no_grad():\n",
    "                    model(x)\n",
    "                torch.cuda.synchronize()\n",
    "                times.append(time.perf_counter() - start)\n",
    "            \n",
    "            avg_ms = sum(times) / len(times) * 1000\n",
    "            tokens_per_sec = (B * T) / (sum(times) / len(times))\n",
    "            \n",
    "            results[(B, T)] = {'avg_ms': avg_ms, 'tokens_per_sec': tokens_per_sec}\n",
    "            print(f\"B={B:2d}, T={T:4d}: {avg_ms:8.2f} ms | {tokens_per_sec:>12,.0f} tok/s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Performance profiling loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "cell-11-triton-cache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton cache management loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: Triton Cache Management\n",
    "# =============================================================================\n",
    "#\n",
    "# TRITON JIT COMPILATION:\n",
    "#   - Kernels are compiled at runtime targeting active GPU architecture\n",
    "#   - First launch incurs ~100-500ms compile time\n",
    "#   - Compiled kernels are cached in ~/.triton/cache/\n",
    "#\n",
    "# CACHE INVALIDATION TRIGGERS:\n",
    "#   - Triton version change\n",
    "#   - CUDA version change\n",
    "#   - GPU architecture change\n",
    "#   - Kernel source change (even whitespace!)\n",
    "#\n",
    "# DEPLOYMENT STRATEGY:\n",
    "#   1. Run warmup script to compile all needed configs\n",
    "#   2. Ship cache with project: export TRITON_CACHE_DIR=/path/to/.triton_cache\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "def warmup_triton_cache(configs=None):\n",
    "    \"\"\"\n",
    "    Pre-compile Triton kernels for common configurations.\n",
    "    \n",
    "    Run this before training to avoid JIT compilation delays.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"TRITON CACHE WARMUP\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if configs is None:\n",
    "        configs = [\n",
    "            (1, 8, 8, 32, 64),    # Single sample, short\n",
    "            (1, 64, 8, 32, 64),   # Single sample, short\n",
    "            (1, 128, 8, 32, 64),  # Single sample, typical\n",
    "            (4, 128, 8, 32, 64),  # Small batch\n",
    "            (8, 128, 8, 32, 64),  # Medium batch\n",
    "            (16, 128, 8, 32, 64), # Large batch\n",
    "            (4, 256, 8, 32, 64),  # Longer sequence\n",
    "            (4, 512, 8, 32, 64),  # Long sequence\n",
    "        ]\n",
    "    \n",
    "    device = DEVICE\n",
    "    \n",
    "    for B, T, H, K, V in configs:\n",
    "        print(f\"  Compiling B={B}, T={T}, H={H}, K={K}, V={V}...\", end=\" \")\n",
    "        \n",
    "        k = F.normalize(torch.randn(B, T, H, K, device=device), dim=-1)\n",
    "        v = torch.randn(B, T, H, V, device=device)\n",
    "        beta = torch.sigmoid(torch.randn(B, T, H, device=device))\n",
    "        g = torch.sigmoid(torch.randn(B, T, H, device=device))\n",
    "        state = torch.zeros(B, H, K, V, device=device)\n",
    "        \n",
    "        # Forward\n",
    "        out, final_state = triton_delta_rule(k, v, beta, g, state)\n",
    "        \n",
    "        # Backward\n",
    "        d_out = torch.randn_like(out)\n",
    "        d_state = torch.randn_like(final_state)\n",
    "        triton_delta_rule_backward(k, v, beta, g, state, d_out, d_state)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        print(\"✓\")\n",
    "    \n",
    "    print(\"\\nCache warmed.\")\n",
    "    print(\"\\nTriton cache location: ~/.triton/cache/\")\n",
    "    print(\"\\nTo ship cache with project:\")\n",
    "    print(\"  export TRITON_CACHE_DIR=/path/to/project/.triton_cache\")\n",
    "\n",
    "\n",
    "def check_triton_cache():\n",
    "    \"\"\"Check Triton cache status.\"\"\"\n",
    "    import os\n",
    "    cache_dir = os.path.expanduser(\"~/.triton/cache\")\n",
    "    \n",
    "    if os.path.exists(cache_dir):\n",
    "        files = list(Path(cache_dir).rglob(\"*\"))\n",
    "        total_size = sum(f.stat().st_size for f in files if f.is_file())\n",
    "        print(f\"Triton cache: {len(files)} files, {total_size/1e6:.1f} MB\")\n",
    "        print(f\"Location: {cache_dir}\")\n",
    "    else:\n",
    "        print(f\"Triton cache not found at {cache_dir}\")\n",
    "\n",
    "\n",
    "print(\"Triton cache management loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "cell-12-quickstart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GROUNDTHINK V6 - QUICK START VALIDATION\n",
      "======================================================================\n",
      "\n",
      "1. Model Created\n",
      "   Pattern: GS\n",
      "   Parameters: 14,741,016\n",
      "\n",
      "2. Forward Pass Test\n",
      "   Output shape: torch.Size([2, 64, 50257])\n",
      "   State norm: 6.4062\n",
      "   GDN: β=0.124, g=0.980\n",
      "   ✓ Forward pass OK\n",
      "\n",
      "3. Backward Pass Test\n",
      "   Loss: 10.8125\n",
      "   ✓ Backward pass OK\n",
      "\n",
      "4. Delta Rule Test\n",
      "\n",
      "============================================================\n",
      "TEST: Identical Tokens (Redundancy Suppression)\n",
      "============================================================\n",
      "  Error1 norm: 15.9750\n",
      "  Error2 norm: 0.000001 (should be ~0)\n",
      "  Error ratio: 0.000000\n",
      "  State growth: 1.0000x (should be ~1.0)\n",
      "  → ✓ PASS\n",
      "\n",
      "5. NIAH Test (untrained model)\n",
      "  Accuracy: 0.0% (0/10)\n",
      "\n",
      "======================================================================\n",
      "✓ QUICK START COMPLETE - Model ready for training\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: Quick Start / Validation\n",
    "# =============================================================================\n",
    "\n",
    "def quick_start():\n",
    "    \"\"\"Quick validation that everything works.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GROUNDTHINK V6 - QUICK START VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Create model\n",
    "    cfg = HybridConfig(d_model=256, n_heads=8, layer_pattern=\"GS\")\n",
    "    model = TransparentHybrid(cfg).to(DEVICE).bfloat16()\n",
    "    \n",
    "    print(f\"\\n1. Model Created\")\n",
    "    print(f\"   Pattern: {cfg.layer_pattern}\")\n",
    "    print(f\"   Parameters: {count_params(model):,}\")\n",
    "    \n",
    "    # 2. Test forward pass\n",
    "    print(f\"\\n2. Forward Pass Test\")\n",
    "    x = torch.randint(0, 1000, (2, 64), device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits, _, diags, state = model(x, return_diagnostics=True)\n",
    "    print(f\"   Output shape: {logits.shape}\")\n",
    "    print(f\"   State norm: {state.norm().item():.4f}\")\n",
    "    print(f\"   GDN: β={diags[0]['beta_mean']:.3f}, g={diags[0]['g_mean']:.3f}\")\n",
    "    print(f\"   ✓ Forward pass OK\")\n",
    "    \n",
    "    # 3. Test backward pass\n",
    "    print(f\"\\n3. Backward Pass Test\")\n",
    "    model.train()\n",
    "    x = torch.randint(0, 1000, (2, 64), device=DEVICE)\n",
    "    y = torch.randint(0, 1000, (2, 64), device=DEVICE)\n",
    "    _, loss, _, _ = model(x, y)\n",
    "    loss.backward()\n",
    "    print(f\"   Loss: {loss.item():.4f}\")\n",
    "    print(f\"   ✓ Backward pass OK\")\n",
    "    \n",
    "    # 4. Delta Rule validation\n",
    "    print(f\"\\n4. Delta Rule Test\")\n",
    "    passed = test_identical_tokens()\n",
    "    \n",
    "    # 5. NIAH test (untrained)\n",
    "    print(f\"\\n5. NIAH Test (untrained model)\")\n",
    "    model.eval()\n",
    "    proper_niah_test(model, seq_len=64, needle_pos=20, n_trials=10)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ QUICK START COMPLETE - Model ready for training\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return model, cfg\n",
    "\n",
    "\n",
    "# Run quick start\n",
    "model, cfg = quick_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "cell-13-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 500,000 tokens from wikitext-103...\n",
      "Loaded 500,000 tokens\n",
      "\n",
      "Model: GS\n",
      "Parameters: 14,741,016\n",
      "\n",
      "--- Pre-training State ---\n",
      "Initial state norm: 4.7188\n",
      "GDN: β=0.123, g=0.980\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: Training Execution\n",
    "# =============================================================================\n",
    "\n",
    "# Load data\n",
    "data_loader = load_data(n_tokens=500_000, seq_len=128, batch_size=16)\n",
    "\n",
    "# Create fresh model for training\n",
    "cfg = HybridConfig(d_model=256, n_heads=8, layer_pattern=\"GS\")\n",
    "model = TransparentHybrid(cfg).to(DEVICE).bfloat16()\n",
    "\n",
    "print(f\"\\nModel: {cfg.layer_pattern}\")\n",
    "print(f\"Parameters: {count_params(model):,}\")\n",
    "\n",
    "# Pre-training check\n",
    "print(\"\\n--- Pre-training State ---\")\n",
    "x = torch.randint(0, 1000, (1, 128), device=DEVICE)\n",
    "with torch.no_grad():\n",
    "    _, _, diags, state = model(x, return_diagnostics=True)\n",
    "print(f\"Initial state norm: {state.norm().item():.4f}\")\n",
    "print(f\"GDN: β={diags[0]['beta_mean']:.3f}, g={diags[0]['g_mean']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "01b9d07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Priming Triton Kernels (Burn-in Phase)...\n",
      "  Warmup Iteration 1/5 Complete\n",
      "  Warmup Iteration 2/5 Complete\n",
      "  Warmup Iteration 3/5 Complete\n",
      "  Warmup Iteration 4/5 Complete\n",
      "  Warmup Iteration 5/5 Complete\n",
      "Status: Kernels hot. Ready for training.\n"
     ]
    }
   ],
   "source": [
    "def warmup_kernels(model, cfg):\n",
    "    print(\"Status: Priming Triton Kernels (Burn-in Phase)...\")\n",
    "    model.train() \n",
    "    # Ensure DEVICE is set to \"cuda\"\n",
    "    dummy_x = torch.randint(0, cfg.vocab_size, (1, 64)).cuda()\n",
    "    \n",
    "    for i in range(5):\n",
    "        # Flexible Unpacking: Take the first element (logits), ignore the rest\n",
    "        outputs = model(dummy_x)\n",
    "        logits = outputs[0] if isinstance(outputs, (tuple, list)) else outputs\n",
    "        \n",
    "        loss = logits.sum()\n",
    "        loss.backward()\n",
    "        model.zero_grad()\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"  Warmup Iteration {i+1}/5 Complete\")\n",
    "    \n",
    "    print(\"Status: Kernels hot. Ready for training.\")\n",
    "\n",
    "# Execute\n",
    "warmup_kernels(model, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "cell-13b-train-exec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2000 steps (200 warmup)\n",
      "  LR: 0.0002, Retrieval weight: 2.0\n",
      "============================================================\n",
      "[WARMUP] Step     0: LM= 0.000 RET=10.750 (20.4 steps/s)\n",
      "[WARMUP] Step   100: LM= 0.000 RET= 0.178 (49.8 steps/s)\n",
      "[MIXED ] Step   200: LM=13.500 RET= 0.029 (50.4 steps/s)\n",
      "[MIXED ] Step   300: LM= 7.531 RET= 0.014 (33.1 steps/s)\n",
      "[MIXED ] Step   400: LM= 7.438 RET= 0.014 (28.4 steps/s)\n",
      "[MIXED ] Step   500: LM= 7.219 RET= 0.013 (26.3 steps/s)\n",
      "[MIXED ] Step   600: LM= 7.188 RET= 0.013 (26.2 steps/s)\n",
      "[MIXED ] Step   700: LM= 7.344 RET= 0.014 (25.1 steps/s)\n",
      "[MIXED ] Step   800: LM= 7.281 RET= 0.014 (24.3 steps/s)\n",
      "[MIXED ] Step   900: LM= 7.281 RET= 0.014 (23.7 steps/s)\n",
      "[MIXED ] Step  1000: LM= 7.281 RET= 0.013 (23.2 steps/s)\n",
      "[MIXED ] Step  1100: LM= 7.312 RET= 0.014 (22.8 steps/s)\n",
      "[MIXED ] Step  1200: LM= 7.312 RET= 0.013 (22.9 steps/s)\n",
      "[MIXED ] Step  1300: LM= 7.281 RET= 0.013 (22.5 steps/s)\n",
      "[MIXED ] Step  1400: LM= 7.438 RET= 0.014 (22.3 steps/s)\n",
      "[MIXED ] Step  1500: LM= 7.438 RET= 0.014 (22.1 steps/s)\n",
      "[MIXED ] Step  1600: LM= 7.062 RET= 0.014 (22.0 steps/s)\n",
      "[MIXED ] Step  1700: LM= 7.188 RET= 0.013 (21.9 steps/s)\n",
      "[MIXED ] Step  1800: LM= 7.406 RET= 0.013 (22.0 steps/s)\n",
      "[MIXED ] Step  1900: LM= 7.281 RET= 0.014 (21.9 steps/s)\n",
      "============================================================\n",
      "Training complete: 2000 steps in 91.7s (21.8 steps/s)\n"
     ]
    }
   ],
   "source": [
    "# Train!\n",
    "history = train_curriculum(\n",
    "    model, \n",
    "    data_loader, \n",
    "    steps=2000, \n",
    "    warmup_steps=200,\n",
    "    lr=2e-4, # adjusted from 3e-4 to 2e-4 to prevent collapse in mixed phase, study curriculum learning literature for details\n",
    "    retrieval_weight=2.0,\n",
    "    log_interval=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "cell-14-eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "POST-TRAINING EVALUATION\n",
      "============================================================\n",
      "\n",
      "1. NIAH Accuracy:\n",
      "  Accuracy: 100.0% (30/30)\n",
      "\n",
      "2. NIAH by Distance:\n",
      "\n",
      "NIAH by Distance (seq_len=128):\n",
      "  Distance   5 (pos=121):   Accuracy: 100.0% (20/20)\n",
      "  Distance  10 (pos=116):   Accuracy: 100.0% (20/20)\n",
      "  Distance  20 (pos=106):   Accuracy: 100.0% (20/20)\n",
      "  Distance  40 (pos= 86):   Accuracy: 100.0% (20/20)\n",
      "  Distance  60 (pos= 66):   Accuracy: 100.0% (20/20)\n",
      "  Distance  95 (pos= 31):   Accuracy: 100.0% (20/20)\n",
      "\n",
      "3. State Health:\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC REPORT\n",
      "============================================================\n",
      "\n",
      "State Health:\n",
      "  State norm: 7.75\n",
      "  State max:  0.50\n",
      "  ✓ State bounded - Delta Rule working!\n",
      "\n",
      "Layer Diagnostics:\n",
      "  Layer 0 (GDN): β=0.112 (max=0.365), g=0.980, state_norm=7.75\n",
      "  Layer 1 (SWA): gate=0.922, local=81.50, retrieval=48.50\n",
      "\n",
      "Prediction: 50254 (target: 50254) - ✓\n",
      "\n",
      "4. Delta Rule Validation:\n",
      "\n",
      "============================================================\n",
      "TEST: Identical Tokens (Redundancy Suppression)\n",
      "============================================================\n",
      "  Error1 norm: 15.1082\n",
      "  Error2 norm: 0.000001 (should be ~0)\n",
      "  Error ratio: 0.000000\n",
      "  State growth: 1.0000x (should be ~1.0)\n",
      "  → ✓ PASS\n",
      "\n",
      "5. Gradient Analysis:\n",
      "\n",
      "============================================================\n",
      "GRADIENT ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Loss: 0.0139\n",
      "\n",
      "Gradient norms by component:\n",
      "  embed.weight                            : 0.222656\n",
      "  layers.0.beta_proj.bias                 : 0.000153\n",
      "  layers.0.beta_proj.weight               : 0.001183\n",
      "  layers.0.g_proj.bias                    : 0.000092\n",
      "  layers.0.g_proj.weight                  : 0.000557\n",
      "  layers.0.k_proj.weight                  : 0.012573\n",
      "  layers.0.o_proj.weight                  : 0.004059\n",
      "  layers.0.v_proj.weight                  : 0.002808\n",
      "  layers.1.gate_proj.bias                 : 0.000012\n",
      "  layers.1.gate_proj.weight               : 0.000186\n",
      "  layers.1.global_q_proj.weight           : 0.001259\n",
      "  layers.1.k_proj.weight                  : 0.000904\n",
      "  layers.1.o_proj.weight                  : 0.004608\n",
      "  layers.1.q_proj.weight                  : 0.001045\n",
      "  layers.1.retrieval_o_proj.weight        : 0.000298\n",
      "  layers.1.v_proj.weight                  : 0.005035\n",
      "\n",
      "  NaN in gradients: ✓ NO\n",
      "  Inf in gradients: ✓ NO\n",
      "  Zero gradients: ✓ NONE\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: Post-Training Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"POST-TRAINING EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. NIAH Accuracy\n",
    "print(\"\\n1. NIAH Accuracy:\")\n",
    "proper_niah_test(model, seq_len=128, n_trials=30)\n",
    "\n",
    "# 2. NIAH by Distance\n",
    "print(\"\\n2. NIAH by Distance:\")\n",
    "test_niah_by_distance(model, seq_len=128)\n",
    "\n",
    "# 3. Full Diagnostic\n",
    "print(\"\\n3. State Health:\")\n",
    "run_full_diagnostic(model, seq_len=128)\n",
    "\n",
    "# 4. Delta Rule Validation (post-training)\n",
    "print(\"\\n4. Delta Rule Validation:\")\n",
    "test_identical_tokens()\n",
    "\n",
    "# 5. Gradient Analysis\n",
    "print(\"\\n5. Gradient Analysis:\")\n",
    "analyze_gradients(model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "cell-15-profiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRITON KERNEL PROFILING\n",
      "Config: H=8, K=32, V=64\n",
      "======================================================================\n",
      "B= 1, T=  64:     0.13 ms |      493,280 tok/s\n",
      "B= 1, T= 128:     0.26 ms |      496,505 tok/s\n",
      "B= 1, T= 256:     0.64 ms |      397,564 tok/s\n",
      "B= 1, T= 512:     0.53 ms |      970,825 tok/s\n",
      "B= 4, T=  64:     0.24 ms |    1,055,400 tok/s\n",
      "B= 4, T= 128:     0.35 ms |    1,482,763 tok/s\n",
      "B= 4, T= 256:     0.44 ms |    2,305,276 tok/s\n",
      "B= 4, T= 512:     0.56 ms |    3,628,184 tok/s\n",
      "B= 8, T=  64:     0.16 ms |    3,117,177 tok/s\n",
      "B= 8, T= 128:     0.26 ms |    3,960,540 tok/s\n",
      "B= 8, T= 256:     0.52 ms |    3,946,534 tok/s\n",
      "B= 8, T= 512:     0.84 ms |    4,871,025 tok/s\n",
      "\n",
      "======================================================================\n",
      "FLA CHUNKED KERNEL (for comparison)\n",
      "======================================================================\n",
      "B= 1, T=  64:     0.40 ms |      158,239 tok/s\n",
      "B= 1, T= 128:     0.37 ms |      349,867 tok/s\n",
      "B= 1, T= 256:     0.35 ms |      731,251 tok/s\n",
      "B= 1, T= 512:     0.40 ms |    1,272,767 tok/s\n",
      "B= 4, T=  64:     0.36 ms |      709,553 tok/s\n",
      "B= 4, T= 128:     0.37 ms |    1,396,387 tok/s\n",
      "B= 4, T= 256:     0.38 ms |    2,687,615 tok/s\n",
      "B= 4, T= 512:     0.71 ms |    2,898,140 tok/s\n",
      "B= 8, T=  64:     0.39 ms |    1,300,533 tok/s\n",
      "B= 8, T= 128:     0.38 ms |    2,662,291 tok/s\n",
      "B= 8, T= 256:     0.67 ms |    3,077,323 tok/s\n",
      "B= 8, T= 512:     1.08 ms |    3,799,218 tok/s\n",
      "\n",
      "======================================================================\n",
      "FULL MODEL PROFILING\n",
      "======================================================================\n",
      "B= 1, T=  64:     2.86 ms |       22,363 tok/s\n",
      "B= 1, T= 128:     3.59 ms |       35,668 tok/s\n",
      "B= 1, T= 256:     3.73 ms |       68,710 tok/s\n",
      "B= 4, T=  64:     4.34 ms |       59,039 tok/s\n",
      "B= 4, T= 128:     4.65 ms |      110,033 tok/s\n",
      "B= 4, T= 256:     6.25 ms |      163,889 tok/s\n",
      "\n",
      "------------------------------------------------------------\n",
      "SPEEDUP: Triton vs FLA\n",
      "------------------------------------------------------------\n",
      "B= 1, T=  64: 3.12x\n",
      "B= 1, T= 128: 1.42x\n",
      "B= 1, T= 256: 0.54x\n",
      "B= 1, T= 512: 0.76x\n",
      "B= 4, T=  64: 1.49x\n",
      "B= 4, T= 128: 1.06x\n",
      "B= 4, T= 256: 0.86x\n",
      "B= 4, T= 512: 1.25x\n",
      "B= 8, T=  64: 2.40x\n",
      "B= 8, T= 128: 1.49x\n",
      "B= 8, T= 256: 1.28x\n",
      "B= 8, T= 512: 1.28x\n",
      "Profiling cell ready. Uncomment to run.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 15: Performance Profiling (Optional)\n",
    "# =============================================================================\n",
    "\n",
    "# Uncomment to run profiling:\n",
    "\n",
    "# # 1. Triton kernel profiling\n",
    "triton_results = profile_triton_kernel()\n",
    "\n",
    "# # 2. FLA comparison (if available)\n",
    "fla_results = profile_fla_kernel()\n",
    "\n",
    "# # 3. Full model profiling\n",
    "model_results = profile_full_model(model)\n",
    "\n",
    "# # 4. Speedup analysis\n",
    "if fla_results:\n",
    "     print(\"\\n\" + \"-\"*60)\n",
    "     print(\"SPEEDUP: Triton vs FLA\")\n",
    "     print(\"-\"*60)\n",
    "     for key in triton_results:\n",
    "         if key in fla_results:\n",
    "             speedup = fla_results[key]['avg_ms'] / triton_results[key]['avg_ms']\n",
    "             print(f\"B={key[0]:2d}, T={key[1]:4d}: {speedup:.2f}x\")\n",
    "\n",
    "print(\"Profiling cell ready. Uncomment to run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
