{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# GroundThink v6 Hybrid Architecture - v5\n",
    "## TRUE Delta Rule + Retrieval Training\n",
    "\n",
    "**Critical Fix in v5:**\n",
    "- FLA's `chunk_gated_delta_rule` processes chunks in parallel, breaking Delta correction\n",
    "- This version uses **pure PyTorch token-by-token** Delta Rule for correctness\n",
    "- State stays bounded (no more -55 or 66,958 signal strength)\n",
    "\n",
    "**Architecture:**\n",
    "1. GDN: True Delta Rule with error correction `S += β * (v - S·k) ⊗ k`\n",
    "2. SWA: Sparse retrieval with dedicated query projection\n",
    "3. Training: Curriculum learning (retrieval warmup → mixed)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-0-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 0: Configuration & Imports\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "import math\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class HybridConfig:\n",
    "    d_model: int = 256\n",
    "    n_heads: int = 8\n",
    "    head_dim: int = 32\n",
    "    expand_v: float = 2.0\n",
    "    vocab_size: int = 50257\n",
    "    layer_pattern: str = \"GS\"\n",
    "    window_size: int = 64\n",
    "    init_std: float = 0.02\n",
    "    state_accumulation: str = 'replace'\n",
    "    marker_token: int = 50251\n",
    "    cue_token: int = 50250\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.head_dim = self.d_model // self.n_heads\n",
    "        self.value_dim = int(self.head_dim * self.expand_v)\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1-components",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic components loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Basic Components\n",
    "# =============================================================================\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x / rms * self.weight\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, cfg: HybridConfig):\n",
    "        super().__init__()\n",
    "        hidden = int(cfg.d_model * 4)\n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        self.w1 = nn.Linear(cfg.d_model, hidden, bias=False)\n",
    "        self.w2 = nn.Linear(hidden, cfg.d_model, bias=False)\n",
    "        self.w3 = nn.Linear(cfg.d_model, hidden, bias=False)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.norm(x)\n",
    "        return x + self.w2(F.silu(self.w1(h)) * self.w3(h))\n",
    "\n",
    "print(\"Basic components loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2-gdn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GatedDeltaNetLayer loaded (TRUE DELTA RULE).\n",
      "  - Pure PyTorch, token-by-token\n",
      "  - Error correction: v - S·k\n",
      "  - Key normalization for stability\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: GatedDeltaNetLayer - TRUE DELTA RULE (PURE PYTORCH)\n",
    "# =============================================================================\n",
    "#\n",
    "# WHY NOT USE FLA's chunk_gated_delta_rule?\n",
    "# - It processes chunks in parallel for speed\n",
    "# - This breaks the error correction: each token's update MUST depend on\n",
    "#   the state AFTER all previous tokens, not the initial state\n",
    "#\n",
    "# TRUE DELTA RULE: S_t = g*S_{t-1} + β * (v - S_{t-1}·k) ⊗ k\n",
    "# - If we've already stored v at address k, error ≈ 0, no redundant write\n",
    "# - This keeps state bounded and memory efficient\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "class GatedDeltaNetLayer(nn.Module):\n",
    "    \"\"\"True Delta Rule GDN - token-by-token for correctness.\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: HybridConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.layer_idx = layer_idx\n",
    "        H, K, V = cfg.n_heads, cfg.head_dim, cfg.value_dim\n",
    "        \n",
    "        self.q_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        self.k_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        self.v_proj = nn.Linear(cfg.d_model, H * V, bias=False)\n",
    "        self.o_proj = nn.Linear(H * V, cfg.d_model, bias=False)\n",
    "        \n",
    "        # Beta gate (write strength) - gatekeeper init\n",
    "        self.beta_proj = nn.Linear(cfg.d_model, H, bias=True)\n",
    "        nn.init.constant_(self.beta_proj.bias, -2.0)  # sigmoid(-2) ≈ 0.12\n",
    "        \n",
    "        # Forget gate (retention) - high default\n",
    "        self.g_proj = nn.Linear(cfg.d_model, H, bias=True)\n",
    "        nn.init.constant_(self.g_proj.bias, 2.0)  # sigmoid(2) ≈ 0.88\n",
    "        \n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        \n",
    "    def forward(self, x, initial_state=None, output_state=True):\n",
    "        B, T, D = x.shape\n",
    "        H, K, V = self.cfg.n_heads, self.cfg.head_dim, self.cfg.value_dim\n",
    "        \n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        q = self.q_proj(x_norm).view(B, T, H, K)\n",
    "        k = self.k_proj(x_norm).view(B, T, H, K)\n",
    "        v = self.v_proj(x_norm).view(B, T, H, V)\n",
    "        \n",
    "        # CRITICAL: Normalize keys for stability\n",
    "        k = F.normalize(k.float(), p=2, dim=-1).to(x.dtype)\n",
    "        \n",
    "        beta = torch.sigmoid(self.beta_proj(x_norm))  # [B, T, H]\n",
    "        g = torch.sigmoid(self.g_proj(x_norm))        # [B, T, H]\n",
    "        \n",
    "        # Initialize state\n",
    "        if initial_state is None:\n",
    "            state = torch.zeros(B, H, K, V, device=x.device, dtype=x.dtype)\n",
    "        else:\n",
    "            state = initial_state.to(x.dtype)\n",
    "        \n",
    "        # Token-by-token TRUE Delta Rule\n",
    "        outputs = []\n",
    "        for t in range(T):\n",
    "            k_t = k[:, t]      # [B, H, K]\n",
    "            v_t = v[:, t]      # [B, H, V]\n",
    "            beta_t = beta[:, t]  # [B, H]\n",
    "            g_t = g[:, t]        # [B, H]\n",
    "            \n",
    "            # 1. Prediction from current state\n",
    "            prediction = torch.einsum('bhkv,bhk->bhv', state, k_t)\n",
    "            \n",
    "            # 2. Error (what we want - what we'd retrieve)\n",
    "            error = v_t - prediction\n",
    "            \n",
    "            # 3. Outer product update\n",
    "            update = torch.einsum('bhv,bhk->bhkv', error, k_t)\n",
    "            update = beta_t.unsqueeze(-1).unsqueeze(-1) * update\n",
    "            \n",
    "            # 4. Apply forget gate and update\n",
    "            state = g_t.unsqueeze(-1).unsqueeze(-1) * state + update\n",
    "            \n",
    "            # 5. Output: retrieve using q\n",
    "            q_t = q[:, t]  # [B, H, K]\n",
    "            out_t = torch.einsum('bhkv,bhk->bhv', state, q_t)\n",
    "            outputs.append(out_t)\n",
    "        \n",
    "        output = torch.stack(outputs, dim=1)  # [B, T, H, V]\n",
    "        output = output.reshape(B, T, H * V)\n",
    "        output = self.o_proj(output)\n",
    "        output = x + output\n",
    "        \n",
    "        diag = {\n",
    "            'beta_mean': beta.mean().item(),\n",
    "            'beta_max': beta.max().item(),\n",
    "            'g_mean': g.mean().item(),\n",
    "            'state_norm': state.norm().item(),\n",
    "            'state_max': state.abs().max().item(),\n",
    "        }\n",
    "        \n",
    "        return output, state, diag\n",
    "\n",
    "print(\"GatedDeltaNetLayer loaded (TRUE DELTA RULE).\")\n",
    "print(\"  - Pure PyTorch, token-by-token\")\n",
    "print(\"  - Error correction: v - S·k\")\n",
    "print(\"  - Key normalization for stability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-2b-validate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DELTA RULE VALIDATION\n",
      "============================================================\n",
      "\n",
      "First token:\n",
      "  Error norm: 10.1741\n",
      "  State norm: 10.1741\n",
      "\n",
      "Second token (SAME k, v):\n",
      "  Error norm: 0.000001  ← Should be ~0\n",
      "  State norm: 10.1741\n",
      "  State growth: 1.0000x  ← Should be ~1.0\n",
      "\n",
      "✓ [PASS] TRUE DELTA RULE: Redundant info suppressed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2b: DELTA RULE VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_delta_rule():\n",
    "    \"\"\"Verify Delta Rule suppresses redundant updates.\"\"\"\n",
    "    B, H, K, V = 1, 4, 16, 32\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"DELTA RULE VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    state = torch.zeros(B, H, K, V, device=device)\n",
    "    \n",
    "    # Normalized key, random value\n",
    "    k = torch.randn(B, H, K, device=device)\n",
    "    k = F.normalize(k, p=2, dim=-1)\n",
    "    v = torch.randn(B, H, V, device=device)\n",
    "    beta = torch.ones(B, H, device=device)\n",
    "    g = torch.ones(B, H, device=device)  # No decay\n",
    "    \n",
    "    # First update\n",
    "    pred_1 = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "    error_1 = v - pred_1\n",
    "    update_1 = torch.einsum('bhv,bhk->bhkv', error_1, k)\n",
    "    state = state + beta.unsqueeze(-1).unsqueeze(-1) * update_1\n",
    "    norm_1 = state.norm().item()\n",
    "    \n",
    "    print(f\"\\nFirst token:\")\n",
    "    print(f\"  Error norm: {error_1.norm().item():.4f}\")\n",
    "    print(f\"  State norm: {norm_1:.4f}\")\n",
    "    \n",
    "    # Second update (SAME k, v)\n",
    "    pred_2 = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "    error_2 = v - pred_2\n",
    "    update_2 = torch.einsum('bhv,bhk->bhkv', error_2, k)\n",
    "    state = state + beta.unsqueeze(-1).unsqueeze(-1) * update_2\n",
    "    norm_2 = state.norm().item()\n",
    "    \n",
    "    print(f\"\\nSecond token (SAME k, v):\")\n",
    "    print(f\"  Error norm: {error_2.norm().item():.6f}  ← Should be ~0\")\n",
    "    print(f\"  State norm: {norm_2:.4f}\")\n",
    "    print(f\"  State growth: {norm_2/norm_1:.4f}x  ← Should be ~1.0\")\n",
    "    \n",
    "    if error_2.norm().item() < 0.01 and norm_2 / norm_1 < 1.1:\n",
    "        print(f\"\\n✓ [PASS] TRUE DELTA RULE: Redundant info suppressed!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\n✗ [FAIL] NOT Delta Rule\")\n",
    "        return False\n",
    "\n",
    "# Run validation\n",
    "validate_delta_rule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-3-swa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlidingWindowAttention loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: SlidingWindowAttention with State Retrieval\n",
    "# =============================================================================\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    def __init__(self, cfg: HybridConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        H, K, V = cfg.n_heads, cfg.head_dim, cfg.value_dim\n",
    "        \n",
    "        # Local attention\n",
    "        self.q_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        self.k_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        self.v_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        self.o_proj = nn.Linear(H * K, cfg.d_model, bias=False)\n",
    "        \n",
    "        # State retrieval (dedicated projection)\n",
    "        self.global_q_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        nn.init.normal_(self.global_q_proj.weight, std=0.02)\n",
    "        self.retrieval_o_proj = nn.Linear(H * V, cfg.d_model, bias=False)\n",
    "        \n",
    "        # Gate (starts open for recall)\n",
    "        self.gate_proj = nn.Linear(cfg.d_model, H, bias=True)\n",
    "        nn.init.constant_(self.gate_proj.bias, 1.0)\n",
    "        \n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        self.scale = K ** -0.5\n",
    "        \n",
    "    def forward(self, x, gdn_state=None):\n",
    "        B, T, D = x.shape\n",
    "        H, K, V, W = self.cfg.n_heads, self.cfg.head_dim, self.cfg.value_dim, self.cfg.window_size\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        # Local attention\n",
    "        q = self.q_proj(x_norm).view(B, T, H, K).transpose(1, 2)\n",
    "        k = self.k_proj(x_norm).view(B, T, H, K).transpose(1, 2)\n",
    "        v = self.v_proj(x_norm).view(B, T, H, K).transpose(1, 2)\n",
    "        \n",
    "        mask = torch.ones(T, T, device=x.device, dtype=torch.bool).triu(1) | \\\n",
    "               torch.ones(T, T, device=x.device, dtype=torch.bool).tril(-W - 1)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        attn_w = F.softmax(attn, dim=-1)\n",
    "        local_out = (attn_w @ v).transpose(1, 2).reshape(B, T, H * K)\n",
    "        local_out = self.o_proj(local_out)\n",
    "        \n",
    "        # State retrieval\n",
    "        retrieval_out = torch.zeros_like(x)\n",
    "        gate_mean = 0.0\n",
    "        \n",
    "        if gdn_state is not None:\n",
    "            q_g = self.global_q_proj(x_norm).view(B, T, H, K).transpose(1, 2)\n",
    "            q_g = F.relu(q_g)  # Sparse queries\n",
    "            \n",
    "            retrieved = torch.einsum('bhkv,bhtk->bhtv', gdn_state.to(x.dtype), q_g)\n",
    "            retrieved = retrieved.transpose(1, 2).reshape(B, T, H * V)\n",
    "            retrieval_out = self.retrieval_o_proj(retrieved)\n",
    "            \n",
    "            gate = torch.sigmoid(self.gate_proj(x_norm))\n",
    "            gate_mean = gate.mean().item()\n",
    "            retrieval_out = gate.mean(dim=-1, keepdim=True) * retrieval_out\n",
    "        \n",
    "        out = x + local_out + retrieval_out\n",
    "        diag = {'gate_mean': gate_mean, 'local_norm': local_out.norm().item()}\n",
    "        return out, diag\n",
    "\n",
    "print(\"SlidingWindowAttention loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-4-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransparentHybrid loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: TransparentHybrid Model\n",
    "# =============================================================================\n",
    "\n",
    "class TransparentHybrid(nn.Module):\n",
    "    def __init__(self, cfg: HybridConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        nn.init.normal_(self.embed.weight, std=cfg.init_std)\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        \n",
    "        for i, lt in enumerate(cfg.layer_pattern):\n",
    "            if lt == 'G':\n",
    "                self.layers.append(GatedDeltaNetLayer(cfg, i))\n",
    "            elif lt == 'S':\n",
    "                self.layers.append(SlidingWindowAttention(cfg, i))\n",
    "            self.ffns.append(FFN(cfg))\n",
    "        \n",
    "        self.norm_f = RMSNorm(cfg.d_model)\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embed.weight\n",
    "        \n",
    "    def forward(self, input_ids, targets=None, return_diagnostics=False):\n",
    "        x = self.embed(input_ids)\n",
    "        state = None\n",
    "        all_diag = []\n",
    "        \n",
    "        for i, (layer, ffn) in enumerate(zip(self.layers, self.ffns)):\n",
    "            lt = self.cfg.layer_pattern[i]\n",
    "            if lt == 'G':\n",
    "                x, state, diag = layer(x, initial_state=state)\n",
    "            else:\n",
    "                x, diag = layer(x, gdn_state=state)\n",
    "            x = ffn(x)\n",
    "            diag['layer'] = lt\n",
    "            all_diag.append(diag)\n",
    "        \n",
    "        x = self.norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-100)\n",
    "        \n",
    "        return logits, loss, all_diag, state\n",
    "\n",
    "print(\"TransparentHybrid loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-5-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m_tes/groundthink/gt-v6/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500,000 tokens\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Data Loading\n",
    "# =============================================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len=128):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return (len(self.tokens) - 1) // self.seq_len\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_len\n",
    "        return torch.tensor(self.tokens[start:start + self.seq_len + 1], dtype=torch.long)\n",
    "\n",
    "def load_data(n_tokens=500_000, seq_len=128, batch_size=16):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train')\n",
    "    all_tokens = []\n",
    "    for item in dataset:\n",
    "        if item['text'].strip():\n",
    "            all_tokens.extend(tokenizer.encode(item['text']))\n",
    "            if len(all_tokens) >= n_tokens:\n",
    "                break\n",
    "    all_tokens = all_tokens[:n_tokens]\n",
    "    print(f\"Loaded {len(all_tokens):,} tokens\")\n",
    "    ds = TextDataset(all_tokens, seq_len)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader = load_data(n_tokens=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-6-retrieval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval testing loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Retrieval Testing\n",
    "# =============================================================================\n",
    "\n",
    "def proper_niah_test(model, seq_len=128, needle_pos=32, n_trials=30):\n",
    "    \"\"\"NIAH test with MARKER + CUE tokens.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    \n",
    "    correct = 0\n",
    "    for _ in range(n_trials):\n",
    "        needle_id = cfg.vocab_size - 3\n",
    "        seq = torch.randint(0, cfg.vocab_size - 100, (1, seq_len), device=device)\n",
    "        seq[0, needle_pos] = cfg.marker_token\n",
    "        seq[0, needle_pos + 1] = needle_id\n",
    "        seq[0, -1] = cfg.cue_token\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _, _, _ = model(seq)\n",
    "        \n",
    "        pred = logits[0, -1].argmax().item()\n",
    "        if pred == needle_id:\n",
    "            correct += 1\n",
    "    \n",
    "    acc = correct / n_trials\n",
    "    print(f\"  Accuracy: {acc*100:.1f}% ({correct}/{n_trials})\")\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "def test_niah_by_distance(model, distances=[5, 10, 20, 40, 60, 95], n_trials=20):\n",
    "    print(f\"Testing retrieval across distances: {distances}\")\n",
    "    for dist in distances:\n",
    "        needle_pos = max(2, 128 - dist - 2)\n",
    "        print(f\"  Distance {dist}:  \", end=\"\")\n",
    "        proper_niah_test(model, needle_pos=needle_pos, n_trials=n_trials)\n",
    "\n",
    "def run_full_diagnostic(model, seq_len=128, needle_pos=32):\n",
    "    \"\"\"Check SNR and state health.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    \n",
    "    needle_id = cfg.vocab_size - 3\n",
    "    seq = torch.randint(0, cfg.vocab_size - 100, (1, seq_len), device=device)\n",
    "    seq[0, needle_pos] = cfg.marker_token\n",
    "    seq[0, needle_pos + 1] = needle_id\n",
    "    seq[0, -1] = cfg.cue_token\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, _, diags, state = model(seq, return_diagnostics=True)\n",
    "    \n",
    "    print(f\"\\n--- Diagnostic ---\")\n",
    "    print(f\"  State norm: {state.norm().item():.2f}\")\n",
    "    print(f\"  State max:  {state.abs().max().item():.2f}\")\n",
    "    for d in diags:\n",
    "        if d['layer'] == 'G':\n",
    "            print(f\"  GDN β={d['beta_mean']:.3f}, g={d['g_mean']:.3f}\")\n",
    "        else:\n",
    "            print(f\"  SWA gate={d['gate_mean']:.3f}\")\n",
    "    \n",
    "    # Check if state is bounded (good Delta Rule)\n",
    "    if state.abs().max().item() < 10:\n",
    "        print(f\"  ✓ State bounded - Delta Rule working!\")\n",
    "    else:\n",
    "        print(f\"  ⚠ State large - check Delta Rule\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"Retrieval testing loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-7-training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training infrastructure loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Training Infrastructure\n",
    "# =============================================================================\n",
    "\n",
    "def compute_retrieval_loss(model, seq_len=128):\n",
    "    \"\"\"Synthetic retrieval task for gradient signal.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    batch_size = 4\n",
    "    \n",
    "    needle_id = cfg.vocab_size - 3\n",
    "    tokens = torch.randint(0, cfg.vocab_size - 100, (batch_size, seq_len), device=device)\n",
    "    \n",
    "    # Random needle positions\n",
    "    for i in range(batch_size):\n",
    "        pos = torch.randint(5, seq_len - 10, (1,)).item()\n",
    "        tokens[i, pos] = cfg.marker_token\n",
    "        tokens[i, pos + 1] = needle_id\n",
    "    tokens[:, -1] = cfg.cue_token\n",
    "    \n",
    "    targets = torch.full((batch_size, seq_len), -100, device=device)\n",
    "    targets[:, -1] = needle_id\n",
    "    \n",
    "    _, loss, _, _ = model(tokens, targets=targets)\n",
    "    return loss\n",
    "\n",
    "def train_curriculum(model, data_loader, steps=1000, warmup_steps=200):\n",
    "    \"\"\"Curriculum: retrieval warmup → mixed training.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    \n",
    "    lm_iter = iter(data_loader)\n",
    "    history = {'lm': [], 'ret': []}\n",
    "    \n",
    "    print(f\"Training {steps} steps ({warmup_steps} warmup)...\")\n",
    "    \n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Phase 1: Retrieval only\n",
    "        if step < warmup_steps:\n",
    "            ret_loss = compute_retrieval_loss(model)\n",
    "            ret_loss.backward()\n",
    "            history['ret'].append(ret_loss.item())\n",
    "            history['lm'].append(0)\n",
    "        # Phase 2: Mixed\n",
    "        else:\n",
    "            try:\n",
    "                batch = next(lm_iter)\n",
    "            except StopIteration:\n",
    "                lm_iter = iter(data_loader)\n",
    "                batch = next(lm_iter)\n",
    "            \n",
    "            input_ids = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "            _, lm_loss, _, _ = model(input_ids, targets)\n",
    "            \n",
    "            ret_loss = compute_retrieval_loss(model)\n",
    "            \n",
    "            total = lm_loss + 2.0 * ret_loss\n",
    "            total.backward()\n",
    "            \n",
    "            history['lm'].append(lm_loss.item())\n",
    "            history['ret'].append(ret_loss.item())\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            phase = \"WARMUP\" if step < warmup_steps else \"MIXED\"\n",
    "            lm = history['lm'][-1]\n",
    "            ret = history['ret'][-1]\n",
    "            print(f\"[{phase}] {step}: LM={lm:.3f} RET={ret:.3f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"Training infrastructure loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-8-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GS\n",
      "Parameters: 15,298,072\n",
      "\n",
      "--- Pre-training check ---\n",
      "Initial state norm: 4.3750\n",
      "GDN β=0.130, g=0.867\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Create Model & Validate\n",
    "# =============================================================================\n",
    "\n",
    "cfg = HybridConfig(d_model=256, n_heads=8, layer_pattern=\"GS\")\n",
    "model = TransparentHybrid(cfg).cuda().bfloat16()\n",
    "\n",
    "print(f\"Model: {cfg.layer_pattern}\")\n",
    "print(f\"Parameters: {count_params(model):,}\")\n",
    "\n",
    "# Quick validation\n",
    "print(\"\\n--- Pre-training check ---\")\n",
    "x = torch.randint(0, 1000, (1, 128), device='cuda')\n",
    "with torch.no_grad():\n",
    "    _, _, diags, state = model(x, return_diagnostics=True)\n",
    "print(f\"Initial state norm: {state.norm().item():.4f}\")\n",
    "print(f\"GDN β={diags[0]['beta_mean']:.3f}, g={diags[0]['g_mean']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-9-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1000 steps (200 warmup)...\n",
      "[WARMUP] 0: LM=0.000 RET=10.812\n",
      "[WARMUP] 100: LM=0.000 RET=0.013\n",
      "[MIXED] 200: LM=15.438 RET=0.007\n",
      "[MIXED] 300: LM=6.781 RET=0.003\n",
      "[MIXED] 400: LM=6.250 RET=0.002\n",
      "[MIXED] 500: LM=5.938 RET=0.002\n",
      "[MIXED] 600: LM=5.938 RET=0.002\n",
      "[MIXED] 700: LM=5.562 RET=0.002\n",
      "[MIXED] 800: LM=5.688 RET=0.002\n",
      "[MIXED] 900: LM=5.438 RET=0.002\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Train\n",
    "# =============================================================================\n",
    "\n",
    "history = train_curriculum(model, data_loader, steps=1000, warmup_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-10-eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "POST-TRAINING EVALUATION\n",
      "============================================================\n",
      "\n",
      "1. NIAH Accuracy:\n",
      "  Accuracy: 100.0% (30/30)\n",
      "\n",
      "2. NIAH by Distance:\n",
      "Testing retrieval across distances: [5, 10, 20, 40, 60, 95]\n",
      "  Distance 5:    Accuracy: 100.0% (20/20)\n",
      "  Distance 10:    Accuracy: 100.0% (20/20)\n",
      "  Distance 20:    Accuracy: 100.0% (20/20)\n",
      "  Distance 40:    Accuracy: 100.0% (20/20)\n",
      "  Distance 60:    Accuracy: 100.0% (20/20)\n",
      "  Distance 95:    Accuracy: 100.0% (20/20)\n",
      "\n",
      "3. State Health:\n",
      "\n",
      "--- Diagnostic ---\n",
      "  State norm: 5.66\n",
      "  State max:  0.29\n",
      "  GDN β=0.214, g=0.742\n",
      "  SWA gate=0.676\n",
      "  ✓ State bounded - Delta Rule working!\n",
      "\n",
      "4. Delta Rule Validation:\n",
      "============================================================\n",
      "DELTA RULE VALIDATION\n",
      "============================================================\n",
      "\n",
      "First token:\n",
      "  Error norm: 11.3580\n",
      "  State norm: 11.3580\n",
      "\n",
      "Second token (SAME k, v):\n",
      "  Error norm: 0.000001  ← Should be ~0\n",
      "  State norm: 11.3580\n",
      "  State growth: 1.0000x  ← Should be ~1.0\n",
      "\n",
      "✓ [PASS] TRUE DELTA RULE: Redundant info suppressed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: Evaluate\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. NIAH Accuracy:\")\n",
    "proper_niah_test(model, n_trials=30)\n",
    "\n",
    "print(\"\\n2. NIAH by Distance:\")\n",
    "test_niah_by_distance(model)\n",
    "\n",
    "print(\"\\n3. State Health:\")\n",
    "run_full_diagnostic(model)\n",
    "\n",
    "print(\"\\n4. Delta Rule Validation:\")\n",
    "validate_delta_rule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7de1e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPEED: Sequential Delta Rule (what we're using)\n",
      "============================================================\n",
      "  T=  64:   18.50 ms |     27,680 tok/s\n",
      "  T= 128:   35.69 ms |     28,689 tok/s\n",
      "  T= 256:   51.50 ms |     39,765 tok/s\n",
      "  T= 512:  108.66 ms |     37,696 tok/s\n",
      "\n",
      "============================================================\n",
      "SPEED: FLA Chunked Kernel (what we had before)\n",
      "============================================================\n",
      "  T=  64:    0.60 ms |    856,513 tok/s\n",
      "  T= 128:    1.08 ms |    949,515 tok/s\n",
      "  T= 256:    1.78 ms |  1,147,598 tok/s\n",
      "  T= 512:    3.36 ms |  1,219,835 tok/s\n",
      "\n",
      "============================================================\n",
      "VALIDATION: Delta Rule Correctness Tests\n",
      "============================================================\n",
      "\n",
      "--- Test 1: Identical Tokens ---\n",
      "  Error1: 15.2943\n",
      "  Error2: 0.000001 (should be ~0)\n",
      "  Growth: 1.0000x (should be ~1.0)\n",
      "  → ✓ PASS\n",
      "\n",
      "--- Test 2: Orthogonal Keys ---\n",
      "  v1 retrieval error: 0.000000\n",
      "  v2 retrieval error: 0.000000\n",
      "  → ✓ PASS\n",
      "\n",
      "--- Test 3: Capacity (100 random writes) ---\n",
      "  State norm after 100 writes: 44.62\n",
      "  First item error: 1.2811\n",
      "  Last item error:  0.0000\n",
      "  → First item degraded (expected with random keys)\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "\n",
      "Speed: Sequential loop is ~10-50x slower than chunked kernel\n",
      "       This may be acceptable for small T, but scales poorly\n",
      "\n",
      "Correctness:\n",
      "  ✓ Redundant writes suppressed (true Delta Rule)\n",
      "  ✓ Orthogonal keys stored independently  \n",
      "  ⚠ Random keys interfere (inherent to associative memory)\n",
      "  ⚠ Capacity limited by K*V state size\n",
      "\n",
      "Next steps:\n",
      "  1. Decide if speed is acceptable for your use case\n",
      "  2. Consider hybrid: chunked for training, sequential for inference\n",
      "  3. Consider custom CUDA kernel that does Delta Rule correctly in chunks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL: PROFILING & VALIDATION (Run this to understand what we built)\n",
    "# =============================================================================\n",
    "#\n",
    "# BEFORE scaling, answer these questions:\n",
    "# 1. How slow is sequential vs chunked?\n",
    "# 2. What edge cases does Delta Rule handle?\n",
    "# 3. Where does it break?\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SPEED TEST: Sequential Delta Rule\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def profile_sequential(T_values=[64, 128, 256, 512], B=8, H=8, K=32, V=64, n_runs=5):\n",
    "    \"\"\"Profile the sequential loop we're using.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SPEED: Sequential Delta Rule (what we're using)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    \n",
    "    for T in T_values:\n",
    "        k = F.normalize(torch.randn(B, T, H, K, device=device), dim=-1)\n",
    "        v = torch.randn(B, T, H, V, device=device)\n",
    "        beta = torch.sigmoid(torch.randn(B, T, H, device=device))\n",
    "        g = torch.sigmoid(torch.randn(B, T, H, device=device))\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(2):\n",
    "            state = torch.zeros(B, H, K, V, device=device)\n",
    "            for t in range(T):\n",
    "                pred = torch.einsum('bhkv,bhk->bhv', state, k[:, t])\n",
    "                error = v[:, t] - pred\n",
    "                update = torch.einsum('bhv,bhk->bhkv', error, k[:, t])\n",
    "                state = g[:, t].unsqueeze(-1).unsqueeze(-1) * state + \\\n",
    "                        beta[:, t].unsqueeze(-1).unsqueeze(-1) * update\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Timed\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n_runs):\n",
    "            state = torch.zeros(B, H, K, V, device=device)\n",
    "            for t in range(T):\n",
    "                pred = torch.einsum('bhkv,bhk->bhv', state, k[:, t])\n",
    "                error = v[:, t] - pred\n",
    "                update = torch.einsum('bhv,bhk->bhkv', error, k[:, t])\n",
    "                state = g[:, t].unsqueeze(-1).unsqueeze(-1) * state + \\\n",
    "                        beta[:, t].unsqueeze(-1).unsqueeze(-1) * update\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed = (time.perf_counter() - start) / n_runs\n",
    "        \n",
    "        toks_per_sec = (B * T) / elapsed\n",
    "        print(f\"  T={T:4d}: {elapsed*1000:7.2f} ms | {toks_per_sec:>10,.0f} tok/s\")\n",
    "\n",
    "profile_sequential()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SPEED TEST: FLA Chunked Kernel (for comparison)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "try:\n",
    "    from fla.ops.gated_delta_rule import chunk_gated_delta_rule\n",
    "    \n",
    "    def profile_fla(T_values=[64, 128, 256, 512], B=8, H=8, K=32, V=64, n_runs=5):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SPEED: FLA Chunked Kernel (what we had before)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        device = \"cuda\"\n",
    "        \n",
    "        for T in T_values:\n",
    "            q = torch.randn(B, T, H, K, device=device)\n",
    "            k = F.normalize(torch.randn(B, T, H, K, device=device), dim=-1)\n",
    "            v = torch.randn(B, T, H, V, device=device)\n",
    "            beta = torch.sigmoid(torch.randn(B, T, H, device=device))\n",
    "            g = torch.sigmoid(torch.randn(B, T, H, device=device))\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(2):\n",
    "                _, _ = chunk_gated_delta_rule(q, k, v, g, beta, output_final_state=True)\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Timed\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            for _ in range(n_runs):\n",
    "                _, _ = chunk_gated_delta_rule(q, k, v, g, beta, output_final_state=True)\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = (time.perf_counter() - start) / n_runs\n",
    "            \n",
    "            toks_per_sec = (B * T) / elapsed\n",
    "            print(f\"  T={T:4d}: {elapsed*1000:7.2f} ms | {toks_per_sec:>10,.0f} tok/s\")\n",
    "    \n",
    "    profile_fla()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nFLA not available for comparison\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# VALIDATION TESTS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION: Delta Rule Correctness Tests\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Identical tokens\n",
    "print(\"\\n--- Test 1: Identical Tokens ---\")\n",
    "B, H, K, V = 1, 4, 32, 64\n",
    "state = torch.zeros(B, H, K, V, device='cuda')\n",
    "k = F.normalize(torch.randn(B, H, K, device='cuda'), dim=-1)\n",
    "v = torch.randn(B, H, V, device='cuda')\n",
    "\n",
    "pred1 = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "error1 = v - pred1\n",
    "state = state + torch.einsum('bhv,bhk->bhkv', error1, k)\n",
    "norm1 = state.norm().item()\n",
    "\n",
    "pred2 = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "error2 = v - pred2\n",
    "state = state + torch.einsum('bhv,bhk->bhkv', error2, k)\n",
    "norm2 = state.norm().item()\n",
    "\n",
    "print(f\"  Error1: {error1.norm().item():.4f}\")\n",
    "print(f\"  Error2: {error2.norm().item():.6f} (should be ~0)\")\n",
    "print(f\"  Growth: {norm2/norm1:.4f}x (should be ~1.0)\")\n",
    "print(f\"  → {'✓ PASS' if error2.norm().item() < 0.001 else '✗ FAIL'}\")\n",
    "\n",
    "# Test 2: Orthogonal keys\n",
    "print(\"\\n--- Test 2: Orthogonal Keys ---\")\n",
    "state = torch.zeros(1, 1, 32, 64, device='cuda')\n",
    "k1 = torch.zeros(1, 1, 32, device='cuda'); k1[0,0,0] = 1.0\n",
    "k2 = torch.zeros(1, 1, 32, device='cuda'); k2[0,0,1] = 1.0\n",
    "v1 = torch.randn(1, 1, 64, device='cuda')\n",
    "v2 = torch.randn(1, 1, 64, device='cuda')\n",
    "\n",
    "# Write v1 at k1\n",
    "state = state + torch.einsum('bhv,bhk->bhkv', v1, k1)\n",
    "# Write v2 at k2\n",
    "state = state + torch.einsum('bhv,bhk->bhkv', v2, k2)\n",
    "\n",
    "ret1 = torch.einsum('bhkv,bhk->bhv', state, k1)\n",
    "ret2 = torch.einsum('bhkv,bhk->bhv', state, k2)\n",
    "err1 = (ret1 - v1).norm().item() / v1.norm().item()\n",
    "err2 = (ret2 - v2).norm().item() / v2.norm().item()\n",
    "\n",
    "print(f\"  v1 retrieval error: {err1:.6f}\")\n",
    "print(f\"  v2 retrieval error: {err2:.6f}\")\n",
    "print(f\"  → {'✓ PASS' if err1 < 0.001 and err2 < 0.001 else '✗ FAIL'}\")\n",
    "\n",
    "# Test 3: Capacity / interference\n",
    "print(\"\\n--- Test 3: Capacity (100 random writes) ---\")\n",
    "state = torch.zeros(1, 1, 32, 64, device='cuda')\n",
    "keys, values = [], []\n",
    "for i in range(100):\n",
    "    k = F.normalize(torch.randn(1, 1, 32, device='cuda'), dim=-1)\n",
    "    v = torch.randn(1, 1, 64, device='cuda')\n",
    "    keys.append(k); values.append(v)\n",
    "    pred = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "    error = v - pred\n",
    "    state = state + torch.einsum('bhv,bhk->bhkv', error, k)\n",
    "\n",
    "ret_first = torch.einsum('bhkv,bhk->bhv', state, keys[0])\n",
    "ret_last = torch.einsum('bhkv,bhk->bhv', state, keys[-1])\n",
    "err_first = (ret_first - values[0]).norm().item() / values[0].norm().item()\n",
    "err_last = (ret_last - values[-1]).norm().item() / values[-1].norm().item()\n",
    "\n",
    "print(f\"  State norm after 100 writes: {state.norm().item():.2f}\")\n",
    "print(f\"  First item error: {err_first:.4f}\")\n",
    "print(f\"  Last item error:  {err_last:.4f}\")\n",
    "print(f\"  → First item degraded (expected with random keys)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Speed: Sequential loop is ~10-50x slower than chunked kernel\n",
    "       This may be acceptable for small T, but scales poorly\n",
    "\n",
    "Correctness:\n",
    "  ✓ Redundant writes suppressed (true Delta Rule)\n",
    "  ✓ Orthogonal keys stored independently  \n",
    "  ⚠ Random keys interfere (inherent to associative memory)\n",
    "  ⚠ Capacity limited by K*V state size\n",
    "\n",
    "Next steps:\n",
    "  1. Decide if speed is acceptable for your use case\n",
    "  2. Consider hybrid: chunked for training, sequential for inference\n",
    "  3. Consider custom CUDA kernel that does Delta Rule correctly in chunks\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "490c6b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SPEED COMPARISON: Sequential vs Chunked\n",
      "======================================================================\n",
      "======================================================================\n",
      "SPEED PROFILING: Sequential Delta Rule\n",
      "======================================================================\n",
      "Config: H=8, K=32, V=64\n",
      "Warmup: 3, Runs: 10\n",
      "\n",
      "B= 1, T=  64:    10.94 ms | 5,850 tok/s\n",
      "B= 1, T= 128:    23.21 ms | 5,516 tok/s\n",
      "B= 1, T= 256:    46.63 ms | 5,491 tok/s\n",
      "B= 1, T= 512:    78.86 ms | 6,492 tok/s\n",
      "B= 8, T=  64:    13.97 ms | 36,663 tok/s\n",
      "B= 8, T= 128:    26.90 ms | 38,067 tok/s\n",
      "B= 8, T= 256:    60.47 ms | 33,871 tok/s\n",
      "B= 8, T= 512:   113.94 ms | 35,950 tok/s\n",
      "\n",
      "======================================================================\n",
      "SPEED PROFILING: FLA Chunked Kernel\n",
      "======================================================================\n",
      "B= 1, T=  64:     1.21 ms | 52,874 tok/s\n",
      "B= 1, T= 128:     1.06 ms | 120,433 tok/s\n",
      "B= 1, T= 256:     1.06 ms | 242,520 tok/s\n",
      "B= 1, T= 512:     0.81 ms | 630,671 tok/s\n",
      "B= 8, T=  64:     0.65 ms | 793,513 tok/s\n",
      "B= 8, T= 128:     0.95 ms | 1,077,741 tok/s\n",
      "B= 8, T= 256:     1.34 ms | 1,526,117 tok/s\n",
      "B= 8, T= 512:     1.93 ms | 2,119,349 tok/s\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "SLOWDOWN FACTOR (Sequential / Chunked)\n",
      "----------------------------------------------------------------------\n",
      "B= 1, T=  64:    9.0x slower\n",
      "B= 1, T= 128:   21.8x slower\n",
      "B= 1, T= 256:   44.2x slower\n",
      "B= 1, T= 512:   97.1x slower\n",
      "B= 8, T=  64:   21.6x slower\n",
      "B= 8, T= 128:   28.3x slower\n",
      "B= 8, T= 256:   45.1x slower\n",
      "B= 8, T= 512:   59.0x slower\n",
      "\n",
      "######################################################################\n",
      "# COMPLETE VALIDATION SUITE\n",
      "######################################################################\n",
      "\n",
      "============================================================\n",
      "TEST 1: Identical Tokens (Redundancy Suppression)\n",
      "============================================================\n",
      "  Error1 norm: 16.0865\n",
      "  Error2 norm: 0.000001\n",
      "  Error ratio: 0.000000 (should be ~0)\n",
      "  State growth: 1.0000x (should be ~1.0)\n",
      "  → ✓ PASS\n",
      "\n",
      "============================================================\n",
      "TEST 2: Orthogonal Keys (Independent Storage)\n",
      "============================================================\n",
      "  v1 retrieval error: 0.000000 (should be ~0)\n",
      "  v2 retrieval error: 0.000000 (should be ~0)\n",
      "  → ✓ PASS\n",
      "\n",
      "============================================================\n",
      "TEST 3: Key Interference (Expected Behavior)\n",
      "============================================================\n",
      "  Key similarity (dot product): 0.9952\n",
      "  v1 error BEFORE v2 write: 0.000000\n",
      "  v1 error AFTER v2 write:  1.3758\n",
      "  Interference occurred: True\n",
      "  → This is EXPECTED: Similar keys interfere\n",
      "\n",
      "============================================================\n",
      "TEST 4: Capacity Limit (State Saturation)\n",
      "============================================================\n",
      "  n= 10: state_norm=23.41, first_err=0.6475, last_err=0.0000\n",
      "  n= 50: state_norm=40.30, first_err=1.1868, last_err=0.0000\n",
      "  n=100: state_norm=44.60, first_err=1.2554, last_err=0.0000\n",
      "  n=200: state_norm=45.19, first_err=1.3642, last_err=0.0000\n",
      "\n",
      "  → State grows with writes, early items degrade\n",
      "\n",
      "============================================================\n",
      "TEST 5: Forget Gate Effect\n",
      "============================================================\n",
      "  g=1.0: retention after 10 steps = 0.9838\n",
      "  g=0.9: retention after 10 steps = 0.3285\n",
      "  g=0.5: retention after 10 steps = 0.0010\n",
      "  g=0.1: retention after 10 steps = 0.0004\n",
      "\n",
      "  → Lower g = faster decay\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "  identical_tokens: ✓ PASS\n",
      "  orthogonal_keys: ✓ PASS\n",
      "  interference: ✓ PASS\n",
      "  capacity: ✓ PASS\n",
      "  forget_gate: ✓ PASS\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRUE DELTA RULE: PROFILING & VALIDATION SUITE\n",
    "# =============================================================================\n",
    "#\n",
    "# Before scaling, we need to understand:\n",
    "# 1. HOW SLOW is the sequential loop vs chunked kernel?\n",
    "# 2. WHAT CASES does the Delta Rule handle correctly?\n",
    "# 3. WHERE does it break down?\n",
    "# 4. CAN we optimize without losing correctness?\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Try to import FLA for comparison\n",
    "try:\n",
    "    from fla.ops.gated_delta_rule import chunk_gated_delta_rule\n",
    "    HAS_FLA = True\n",
    "except ImportError:\n",
    "    HAS_FLA = False\n",
    "    print(\"FLA not available - skipping kernel comparison\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: SPEED PROFILING\n",
    "# =============================================================================\n",
    "\n",
    "def profile_sequential_delta(\n",
    "    batch_sizes: List[int] = [1, 4, 8, 16],\n",
    "    seq_lens: List[int] = [64, 128, 256, 512],\n",
    "    n_heads: int = 8,\n",
    "    head_dim: int = 32,\n",
    "    value_dim: int = 64,\n",
    "    n_warmup: int = 3,\n",
    "    n_runs: int = 10,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Profile the sequential Delta Rule implementation.\n",
    "    \n",
    "    This is the CRITICAL question: Is token-by-token viable?\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SPEED PROFILING: Sequential Delta Rule\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Config: H={n_heads}, K={head_dim}, V={value_dim}\")\n",
    "    print(f\"Warmup: {n_warmup}, Runs: {n_runs}\")\n",
    "    print()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for B in batch_sizes:\n",
    "        for T in seq_lens:\n",
    "            # Create inputs\n",
    "            k = torch.randn(B, T, n_heads, head_dim, device=device)\n",
    "            k = F.normalize(k, p=2, dim=-1)\n",
    "            v = torch.randn(B, T, n_heads, value_dim, device=device)\n",
    "            beta = torch.sigmoid(torch.randn(B, T, n_heads, device=device))\n",
    "            g = torch.sigmoid(torch.randn(B, T, n_heads, device=device))\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(n_warmup):\n",
    "                state = torch.zeros(B, n_heads, head_dim, value_dim, device=device)\n",
    "                for t in range(T):\n",
    "                    pred = torch.einsum('bhkv,bhk->bhv', state, k[:, t])\n",
    "                    error = v[:, t] - pred\n",
    "                    update = torch.einsum('bhv,bhk->bhkv', error, k[:, t])\n",
    "                    state = g[:, t].unsqueeze(-1).unsqueeze(-1) * state + \\\n",
    "                            beta[:, t].unsqueeze(-1).unsqueeze(-1) * update\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Timed runs\n",
    "            times = []\n",
    "            for _ in range(n_runs):\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.perf_counter()\n",
    "                \n",
    "                state = torch.zeros(B, n_heads, head_dim, value_dim, device=device)\n",
    "                for t in range(T):\n",
    "                    pred = torch.einsum('bhkv,bhk->bhv', state, k[:, t])\n",
    "                    error = v[:, t] - pred\n",
    "                    update = torch.einsum('bhv,bhk->bhkv', error, k[:, t])\n",
    "                    state = g[:, t].unsqueeze(-1).unsqueeze(-1) * state + \\\n",
    "                            beta[:, t].unsqueeze(-1).unsqueeze(-1) * update\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "                elapsed = time.perf_counter() - start\n",
    "                times.append(elapsed)\n",
    "            \n",
    "            avg_ms = sum(times) / len(times) * 1000\n",
    "            tokens_per_sec = (B * T) / (sum(times) / len(times))\n",
    "            \n",
    "            results[(B, T)] = {\n",
    "                'avg_ms': avg_ms,\n",
    "                'tokens_per_sec': tokens_per_sec,\n",
    "            }\n",
    "            \n",
    "            print(f\"B={B:2d}, T={T:4d}: {avg_ms:8.2f} ms | {tokens_per_sec:,.0f} tok/s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def profile_fla_kernel(\n",
    "    batch_sizes: List[int] = [1, 4, 8, 16],\n",
    "    seq_lens: List[int] = [64, 128, 256, 512],\n",
    "    n_heads: int = 8,\n",
    "    head_dim: int = 32,\n",
    "    value_dim: int = 64,\n",
    "    n_warmup: int = 3,\n",
    "    n_runs: int = 10,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"Profile FLA's chunked kernel for comparison.\"\"\"\n",
    "    if not HAS_FLA:\n",
    "        print(\"FLA not available\")\n",
    "        return {}\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"SPEED PROFILING: FLA Chunked Kernel\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for B in batch_sizes:\n",
    "        for T in seq_lens:\n",
    "            q = torch.randn(B, T, n_heads, head_dim, device=device)\n",
    "            k = torch.randn(B, T, n_heads, head_dim, device=device)\n",
    "            k = F.normalize(k, p=2, dim=-1)\n",
    "            v = torch.randn(B, T, n_heads, value_dim, device=device)\n",
    "            beta = torch.sigmoid(torch.randn(B, T, n_heads, device=device))\n",
    "            g = torch.sigmoid(torch.randn(B, T, n_heads, device=device))\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(n_warmup):\n",
    "                _, _ = chunk_gated_delta_rule(q, k, v, g, beta, output_final_state=True)\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Timed runs\n",
    "            times = []\n",
    "            for _ in range(n_runs):\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.perf_counter()\n",
    "                _, _ = chunk_gated_delta_rule(q, k, v, g, beta, output_final_state=True)\n",
    "                torch.cuda.synchronize()\n",
    "                elapsed = time.perf_counter() - start\n",
    "                times.append(elapsed)\n",
    "            \n",
    "            avg_ms = sum(times) / len(times) * 1000\n",
    "            tokens_per_sec = (B * T) / (sum(times) / len(times))\n",
    "            \n",
    "            results[(B, T)] = {\n",
    "                'avg_ms': avg_ms,\n",
    "                'tokens_per_sec': tokens_per_sec,\n",
    "            }\n",
    "            \n",
    "            print(f\"B={B:2d}, T={T:4d}: {avg_ms:8.2f} ms | {tokens_per_sec:,.0f} tok/s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_speeds():\n",
    "    \"\"\"Side-by-side comparison.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SPEED COMPARISON: Sequential vs Chunked\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    seq_results = profile_sequential_delta(\n",
    "        batch_sizes=[1, 8],\n",
    "        seq_lens=[64, 128, 256, 512],\n",
    "    )\n",
    "    \n",
    "    if HAS_FLA:\n",
    "        print()\n",
    "        fla_results = profile_fla_kernel(\n",
    "            batch_sizes=[1, 8],\n",
    "            seq_lens=[64, 128, 256, 512],\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"SLOWDOWN FACTOR (Sequential / Chunked)\")\n",
    "        print(\"-\" * 70)\n",
    "        for key in seq_results:\n",
    "            if key in fla_results:\n",
    "                slowdown = seq_results[key]['avg_ms'] / fla_results[key]['avg_ms']\n",
    "                print(f\"B={key[0]:2d}, T={key[1]:4d}: {slowdown:6.1f}x slower\")\n",
    "    \n",
    "    return seq_results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: CORRECTNESS VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def test_identical_tokens():\n",
    "    \"\"\"Test 1: Identical tokens should produce zero error on second write.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST 1: Identical Tokens (Redundancy Suppression)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    B, H, K, V = 1, 4, 32, 64\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    state = torch.zeros(B, H, K, V, device=device)\n",
    "    k = F.normalize(torch.randn(B, H, K, device=device), dim=-1)\n",
    "    v = torch.randn(B, H, V, device=device)\n",
    "    \n",
    "    # First write\n",
    "    pred1 = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "    error1 = v - pred1\n",
    "    update1 = torch.einsum('bhv,bhk->bhkv', error1, k)\n",
    "    state = state + update1\n",
    "    norm1 = state.norm().item()\n",
    "    \n",
    "    # Second write (same k, v)\n",
    "    pred2 = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "    error2 = v - pred2\n",
    "    update2 = torch.einsum('bhv,bhk->bhkv', error2, k)\n",
    "    state = state + update2\n",
    "    norm2 = state.norm().item()\n",
    "    \n",
    "    error_ratio = error2.norm().item() / (error1.norm().item() + 1e-8)\n",
    "    growth = norm2 / norm1\n",
    "    \n",
    "    print(f\"  Error1 norm: {error1.norm().item():.4f}\")\n",
    "    print(f\"  Error2 norm: {error2.norm().item():.6f}\")\n",
    "    print(f\"  Error ratio: {error_ratio:.6f} (should be ~0)\")\n",
    "    print(f\"  State growth: {growth:.4f}x (should be ~1.0)\")\n",
    "    \n",
    "    passed = error_ratio < 0.001 and growth < 1.01\n",
    "    print(f\"  → {'✓ PASS' if passed else '✗ FAIL'}\")\n",
    "    return passed\n",
    "\n",
    "\n",
    "def test_orthogonal_keys():\n",
    "    \"\"\"Test 2: Orthogonal keys should store independently without interference.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST 2: Orthogonal Keys (Independent Storage)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    B, H, K, V = 1, 1, 32, 64\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    state = torch.zeros(B, H, K, V, device=device)\n",
    "    \n",
    "    # Create two orthogonal keys\n",
    "    k1 = torch.zeros(B, H, K, device=device)\n",
    "    k1[0, 0, 0] = 1.0  # Unit vector along dim 0\n",
    "    \n",
    "    k2 = torch.zeros(B, H, K, device=device)\n",
    "    k2[0, 0, 1] = 1.0  # Unit vector along dim 1\n",
    "    \n",
    "    v1 = torch.randn(B, H, V, device=device)\n",
    "    v2 = torch.randn(B, H, V, device=device)\n",
    "    \n",
    "    # Write v1 at k1\n",
    "    pred = torch.einsum('bhkv,bhk->bhv', state, k1)\n",
    "    error = v1 - pred\n",
    "    update = torch.einsum('bhv,bhk->bhkv', error, k1)\n",
    "    state = state + update\n",
    "    \n",
    "    # Write v2 at k2\n",
    "    pred = torch.einsum('bhkv,bhk->bhv', state, k2)\n",
    "    error = v2 - pred\n",
    "    update = torch.einsum('bhv,bhk->bhkv', error, k2)\n",
    "    state = state + update\n",
    "    \n",
    "    # Retrieve v1 using k1\n",
    "    retrieved_v1 = torch.einsum('bhkv,bhk->bhv', state, k1)\n",
    "    # Retrieve v2 using k2\n",
    "    retrieved_v2 = torch.einsum('bhkv,bhk->bhv', state, k2)\n",
    "    \n",
    "    error_v1 = (retrieved_v1 - v1).norm().item() / v1.norm().item()\n",
    "    error_v2 = (retrieved_v2 - v2).norm().item() / v2.norm().item()\n",
    "    \n",
    "    print(f\"  v1 retrieval error: {error_v1:.6f} (should be ~0)\")\n",
    "    print(f\"  v2 retrieval error: {error_v2:.6f} (should be ~0)\")\n",
    "    \n",
    "    passed = error_v1 < 0.001 and error_v2 < 0.001\n",
    "    print(f\"  → {'✓ PASS' if passed else '✗ FAIL'}\")\n",
    "    return passed\n",
    "\n",
    "\n",
    "def test_interference():\n",
    "    \"\"\"Test 3: Similar (non-orthogonal) keys cause interference.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST 3: Key Interference (Expected Behavior)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    B, H, K, V = 1, 1, 32, 64\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    state = torch.zeros(B, H, K, V, device=device)\n",
    "    \n",
    "    # Two similar keys (high dot product)\n",
    "    k1 = F.normalize(torch.randn(B, H, K, device=device), dim=-1)\n",
    "    k2 = k1 + 0.1 * F.normalize(torch.randn(B, H, K, device=device), dim=-1)\n",
    "    k2 = F.normalize(k2, dim=-1)\n",
    "    \n",
    "    dot_product = (k1 * k2).sum().item()\n",
    "    \n",
    "    v1 = torch.randn(B, H, V, device=device)\n",
    "    v2 = torch.randn(B, H, V, device=device)\n",
    "    \n",
    "    # Write v1 at k1\n",
    "    pred = torch.einsum('bhkv,bhk->bhv', state, k1)\n",
    "    error = v1 - pred\n",
    "    update = torch.einsum('bhv,bhk->bhkv', error, k1)\n",
    "    state = state + update\n",
    "    \n",
    "    # Retrieve v1 BEFORE writing v2\n",
    "    retrieved_v1_before = torch.einsum('bhkv,bhk->bhv', state, k1)\n",
    "    error_before = (retrieved_v1_before - v1).norm().item() / v1.norm().item()\n",
    "    \n",
    "    # Write v2 at k2 (similar key)\n",
    "    pred = torch.einsum('bhkv,bhk->bhv', state, k2)\n",
    "    error = v2 - pred\n",
    "    update = torch.einsum('bhv,bhk->bhkv', error, k2)\n",
    "    state = state + update\n",
    "    \n",
    "    # Retrieve v1 AFTER writing v2\n",
    "    retrieved_v1_after = torch.einsum('bhkv,bhk->bhv', state, k1)\n",
    "    error_after = (retrieved_v1_after - v1).norm().item() / v1.norm().item()\n",
    "    \n",
    "    print(f\"  Key similarity (dot product): {dot_product:.4f}\")\n",
    "    print(f\"  v1 error BEFORE v2 write: {error_before:.6f}\")\n",
    "    print(f\"  v1 error AFTER v2 write:  {error_after:.4f}\")\n",
    "    print(f\"  Interference occurred: {error_after > error_before}\")\n",
    "    \n",
    "    # This test documents expected behavior, not a pass/fail\n",
    "    print(f\"  → This is EXPECTED: Similar keys interfere\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_capacity_limit():\n",
    "    \"\"\"Test 4: What happens when we write more items than state can hold?\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST 4: Capacity Limit (State Saturation)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    B, H, K, V = 1, 1, 32, 64\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    state = torch.zeros(B, H, K, V, device=device)\n",
    "    \n",
    "    # Theoretical capacity: K * V = 32 * 64 = 2048 floats\n",
    "    # But with random keys, effective capacity is much less\n",
    "    \n",
    "    n_writes = [10, 50, 100, 200]\n",
    "    results = []\n",
    "    \n",
    "    for n in n_writes:\n",
    "        state = torch.zeros(B, H, K, V, device=device)\n",
    "        keys = []\n",
    "        values = []\n",
    "        \n",
    "        # Write n random (k, v) pairs\n",
    "        for i in range(n):\n",
    "            k = F.normalize(torch.randn(B, H, K, device=device), dim=-1)\n",
    "            v = torch.randn(B, H, V, device=device)\n",
    "            keys.append(k)\n",
    "            values.append(v)\n",
    "            \n",
    "            pred = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "            error = v - pred\n",
    "            update = torch.einsum('bhv,bhk->bhkv', error, k)\n",
    "            state = state + update\n",
    "        \n",
    "        # Test retrieval of first item\n",
    "        retrieved_first = torch.einsum('bhkv,bhk->bhv', state, keys[0])\n",
    "        error_first = (retrieved_first - values[0]).norm().item() / values[0].norm().item()\n",
    "        \n",
    "        # Test retrieval of last item\n",
    "        retrieved_last = torch.einsum('bhkv,bhk->bhv', state, keys[-1])\n",
    "        error_last = (retrieved_last - values[-1]).norm().item() / values[-1].norm().item()\n",
    "        \n",
    "        results.append({\n",
    "            'n': n,\n",
    "            'state_norm': state.norm().item(),\n",
    "            'first_error': error_first,\n",
    "            'last_error': error_last,\n",
    "        })\n",
    "        \n",
    "        print(f\"  n={n:3d}: state_norm={state.norm().item():.2f}, \"\n",
    "              f\"first_err={error_first:.4f}, last_err={error_last:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  → State grows with writes, early items degrade\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def test_forget_gate():\n",
    "    \"\"\"Test 5: Forget gate controls information decay.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST 5: Forget Gate Effect\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    B, H, K, V = 1, 1, 32, 64\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    k = F.normalize(torch.randn(B, H, K, device=device), dim=-1)\n",
    "    v = torch.randn(B, H, V, device=device)\n",
    "    \n",
    "    for g_val in [1.0, 0.9, 0.5, 0.1]:\n",
    "        state = torch.zeros(B, H, K, V, device=device)\n",
    "        g = torch.full((B, H), g_val, device=device)\n",
    "        \n",
    "        # Write once\n",
    "        pred = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "        error = v - pred\n",
    "        update = torch.einsum('bhv,bhk->bhkv', error, k)\n",
    "        state = g.unsqueeze(-1).unsqueeze(-1) * state + update\n",
    "        \n",
    "        # Apply 10 more \"empty\" steps (just decay)\n",
    "        k_noise = F.normalize(torch.randn(B, H, K, device=device), dim=-1)\n",
    "        v_zero = torch.zeros(B, H, V, device=device)\n",
    "        \n",
    "        for _ in range(10):\n",
    "            pred = torch.einsum('bhkv,bhk->bhv', state, k_noise)\n",
    "            error = v_zero - pred\n",
    "            update = torch.einsum('bhv,bhk->bhkv', error, k_noise)\n",
    "            state = g.unsqueeze(-1).unsqueeze(-1) * state + update\n",
    "        \n",
    "        # Try to retrieve original\n",
    "        retrieved = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "        retention = (retrieved * v).sum().item() / (v.norm().item() ** 2)\n",
    "        \n",
    "        print(f\"  g={g_val:.1f}: retention after 10 steps = {retention:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  → Lower g = faster decay\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: RUN ALL TESTS\n",
    "# =============================================================================\n",
    "\n",
    "def run_all_validations():\n",
    "    \"\"\"Run complete validation suite.\"\"\"\n",
    "    print(\"\\n\" + \"#\" * 70)\n",
    "    print(\"# COMPLETE VALIDATION SUITE\")\n",
    "    print(\"#\" * 70)\n",
    "    \n",
    "    results = {\n",
    "        'identical_tokens': test_identical_tokens(),\n",
    "        'orthogonal_keys': test_orthogonal_keys(),\n",
    "        'interference': test_interference(),\n",
    "        'capacity': test_capacity_limit(),\n",
    "        'forget_gate': test_forget_gate(),\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    for name, passed in results.items():\n",
    "        status = \"✓ PASS\" if passed else \"✗ FAIL/INFO\"\n",
    "        print(f\"  {name}: {status}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run speed comparison\n",
    "    compare_speeds()\n",
    "    \n",
    "    # Run all validations\n",
    "    run_all_validations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
