{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# GroundThink v6 Hybrid Architecture\n",
    "## GDN + SWA with Retrieval Training\n",
    "\n",
    "**Key Fixes Applied:**\n",
    "1. **Gatekeeper GDN**: β bias=-2.0, no floor → sparse selective writes\n",
    "2. **Sparse SWA Retrieval**: Dedicated query projection with ReLU sparsity\n",
    "3. **Proper NIAH Test**: Uses MARKER + CUE tokens for retrieval signal\n",
    "4. **Mixed Training**: LM + synthetic retrieval tasks\n",
    "5. **Auxiliary Retrieval Loss**: Direct gradient for state→retrieval pathway\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-0-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 0: Configuration & Imports\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from fla.ops.gated_delta_rule import chunk_gated_delta_rule\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HybridConfig:\n",
    "    \"\"\"\n",
    "    Hybrid architecture configuration.\n",
    "    \n",
    "    Layer patterns: 'GS', 'GSG', 'GSGS', etc.\n",
    "    \"\"\"\n",
    "    d_model: int = 256\n",
    "    n_heads: int = 8\n",
    "    head_dim: int = 32\n",
    "    expand_v: float = 2.0\n",
    "    vocab_size: int = 50257\n",
    "    \n",
    "    layer_pattern: str = \"GS\"\n",
    "    window_size: int = 64\n",
    "    \n",
    "    init_std: float = 0.02\n",
    "    \n",
    "    state_accumulation: str = 'weighted'\n",
    "    state_weight_new: float = 0.5\n",
    "    \n",
    "    # Special tokens for retrieval\n",
    "    marker_token: int = 50251\n",
    "    cue_token: int = 50250\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.head_dim = self.d_model // self.n_heads\n",
    "        self.value_dim = int(self.head_dim * self.expand_v)\n",
    "        \n",
    "    @property\n",
    "    def n_layers(self) -> int:\n",
    "        return len(self.layer_pattern)\n",
    "    \n",
    "    @property\n",
    "    def gdn_indices(self) -> List[int]:\n",
    "        return [i for i, t in enumerate(self.layer_pattern) if t == 'G']\n",
    "    \n",
    "    @property\n",
    "    def swa_indices(self) -> List[int]:\n",
    "        return [i for i, t in enumerate(self.layer_pattern) if t == 'S']\n",
    "\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-1-components",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic components loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Basic Components (RMSNorm, FFN)\n",
    "# =============================================================================\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x / rms * self.weight\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, cfg: HybridConfig):\n",
    "        super().__init__()\n",
    "        hidden = int(cfg.d_model * 4)\n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        self.w1 = nn.Linear(cfg.d_model, hidden, bias=False)\n",
    "        self.w2 = nn.Linear(hidden, cfg.d_model, bias=False)\n",
    "        self.w3 = nn.Linear(cfg.d_model, hidden, bias=False)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.norm(x)\n",
    "        return x + self.w2(F.silu(self.w1(h)) * self.w3(h))\n",
    "\n",
    "\n",
    "print(\"Basic components loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-2-gdn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GatedDeltaNetLayer loaded (GATEKEEPER).\n",
      "  - β bias = -2.0 → default β ≈ 0.12\n",
      "  - No β floor → sparse selective writes\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: GatedDeltaNetLayer - GATEKEEPER INITIALIZATION\n",
    "# =============================================================================\n",
    "#\n",
    "# KEY FIX: β is LOW by default (bias=-2.0 → sigmoid ≈ 0.12)\n",
    "#          No floor on β → can reach near 0\n",
    "#          Model must LEARN to spike β for important tokens\n",
    "#\n",
    "# This prevents the state from being flooded with noise.\n",
    "# =============================================================================\n",
    "\n",
    "class GatedDeltaNetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    GDN with Gatekeeper initialization.\n",
    "    \n",
    "    Delta Rule: Sₜ = gₜ * Sₜ₋₁ + βₜ * (vₜ ⊗ kₜ)\n",
    "    \n",
    "    With β biased low, the model preserves state by default\n",
    "    and only writes when it learns something is important.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: HybridConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        H, K, V = cfg.n_heads, cfg.head_dim, cfg.value_dim\n",
    "        \n",
    "        # QKV projections\n",
    "        self.q_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        self.k_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        self.v_proj = nn.Linear(cfg.d_model, H * V, bias=False)\n",
    "        self.o_proj = nn.Linear(H * V, cfg.d_model, bias=False)\n",
    "        \n",
    "        # === GATEKEEPER BETA ===\n",
    "        self.beta_proj = nn.Linear(cfg.d_model, H, bias=True)\n",
    "        # CRITICAL: Negative bias = closed by default\n",
    "        # sigmoid(-2.0) ≈ 0.12, preserves 88% of old memory\n",
    "        nn.init.constant_(self.beta_proj.bias, -2.0)\n",
    "        \n",
    "        # Forget gate\n",
    "        self.g_proj = nn.Linear(cfg.d_model, H, bias=False)\n",
    "        \n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        initial_state: Optional[torch.Tensor] = None,\n",
    "        output_state: bool = True\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Dict[str, Any]]:\n",
    "        B, T, D = x.shape\n",
    "        H, K, V = self.cfg.n_heads, self.cfg.head_dim, self.cfg.value_dim\n",
    "        \n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.q_proj(x_norm).view(B, T, H, K)\n",
    "        k = self.k_proj(x_norm).view(B, T, H, K)\n",
    "        v = self.v_proj(x_norm).view(B, T, H, V)\n",
    "        \n",
    "        # Normalize K for stability\n",
    "        k = F.normalize(k.float(), p=2, dim=-1).to(x.dtype)\n",
    "        \n",
    "        # === GATEKEEPER BETA ===\n",
    "        # NO floor! β can reach near 0.\n",
    "        # With bias=-2.0, default β ≈ 0.12\n",
    "        beta = torch.sigmoid(self.beta_proj(x_norm))  # [B, T, H] in [0, 1]\n",
    "        \n",
    "        # Forget gate (log space for numerical stability)\n",
    "        g = F.logsigmoid(self.g_proj(x_norm))\n",
    "        \n",
    "        # Core delta rule\n",
    "        output, state = chunk_gated_delta_rule(\n",
    "            q, k, v, g, beta,\n",
    "            initial_state=initial_state,\n",
    "            output_final_state=output_state\n",
    "        )\n",
    "        \n",
    "        # Project and residual\n",
    "        output = output.reshape(B, T, H * V)\n",
    "        output = self.o_proj(output)\n",
    "        output = x + output\n",
    "        \n",
    "        # Diagnostics\n",
    "        diagnostics = {\n",
    "            'beta_mean': beta.mean().item(),\n",
    "            'beta_std': beta.std().item(),\n",
    "            'beta_min': beta.min().item(),\n",
    "            'beta_max': beta.max().item(),\n",
    "            'g_mean': g.exp().mean().item(),\n",
    "            'g_std': g.exp().std().item(),\n",
    "            'state_norm': state.norm().item() if state is not None else 0,\n",
    "            'state_shape': tuple(state.shape) if state is not None else None,\n",
    "        }\n",
    "        \n",
    "        return output, state, diagnostics\n",
    "\n",
    "\n",
    "print(\"GatedDeltaNetLayer loaded (GATEKEEPER).\")\n",
    "print(\"  - β bias = -2.0 → default β ≈ 0.12\")\n",
    "print(\"  - No β floor → sparse selective writes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-3-swa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlidingWindowAttention loaded (SPARSE RETRIEVAL).\n",
      "  - Dedicated global_q_proj for state retrieval\n",
      "  - ReLU sparsity on retrieval query\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: SlidingWindowAttention - DEDICATED SPARSE RETRIEVAL\n",
    "# =============================================================================\n",
    "#\n",
    "# KEY FIX: SWA has its OWN query projection for state retrieval.\n",
    "#          ReLU on retrieval query forces sparsity (pointy attention).\n",
    "#\n",
    "# This decouples GDN's write optimization from SWA's read optimization.\n",
    "# =============================================================================\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    SWA with dedicated sparse retrieval from GDN state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: HybridConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.layer_idx = layer_idx\n",
    "        H, K, V = cfg.n_heads, cfg.head_dim, cfg.value_dim\n",
    "        \n",
    "        # === LOCAL ATTENTION ===\n",
    "        self.q_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        self.k_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        self.v_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        self.o_proj = nn.Linear(H * K, cfg.d_model, bias=False)\n",
    "        \n",
    "        # === DEDICATED GLOBAL RETRIEVAL ===\n",
    "        self.global_q_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        nn.init.normal_(self.global_q_proj.weight, std=0.01)\n",
    "        \n",
    "        self.retrieval_o_proj = nn.Linear(H * V, cfg.d_model, bias=False)\n",
    "        nn.init.xavier_uniform_(self.retrieval_o_proj.weight, gain=0.5)\n",
    "        \n",
    "        # Gate for retrieval contribution\n",
    "        self.gate_proj = nn.Linear(cfg.d_model, H, bias=True)\n",
    "        nn.init.zeros_(self.gate_proj.weight)\n",
    "        nn.init.constant_(self.gate_proj.bias, 0.0)\n",
    "        \n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        self.scale = K ** -0.5\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor,\n",
    "        gdn_state: Optional[torch.Tensor] = None,\n",
    "        return_attn: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, Any], ...]:\n",
    "        B, T, D = x.shape\n",
    "        H = self.cfg.n_heads\n",
    "        K = self.cfg.head_dim\n",
    "        V = self.cfg.value_dim\n",
    "        W = self.cfg.window_size\n",
    "        \n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        # === LOCAL ATTENTION ===\n",
    "        q = self.q_proj(x_norm).view(B, T, H, K).transpose(1, 2)\n",
    "        k_local = self.k_proj(x_norm).view(B, T, H, K).transpose(1, 2)\n",
    "        v_local = self.v_proj(x_norm).view(B, T, H, K).transpose(1, 2)\n",
    "        \n",
    "        mask = torch.ones(T, T, device=x.device, dtype=torch.bool)\n",
    "        mask = mask.triu(1) | mask.tril(-W - 1)\n",
    "        \n",
    "        attn_local = (q @ k_local.transpose(-2, -1)) * self.scale\n",
    "        attn_local = attn_local.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        attn_weights_local = F.softmax(attn_local, dim=-1)\n",
    "        local_out = attn_weights_local @ v_local\n",
    "        \n",
    "        local_out = local_out.transpose(1, 2).reshape(B, T, H * K)\n",
    "        local_out = self.o_proj(local_out)\n",
    "        \n",
    "        # === DEDICATED SPARSE RETRIEVAL ===\n",
    "        retrieval_out = torch.zeros(B, T, D, device=x.device, dtype=x.dtype)\n",
    "        attn_weights_global = None\n",
    "        gate_values = torch.full((B, H, T, 1), 0.5, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        if gdn_state is not None:\n",
    "            state = gdn_state.to(x.dtype)  # [B, H, K, V]\n",
    "            \n",
    "            # === OWN QUERY with ReLU SPARSITY ===\n",
    "            q_global = self.global_q_proj(x_norm).view(B, T, H, K).transpose(1, 2)\n",
    "            q_global = F.relu(q_global)  # Sparsity: zero out negative values\n",
    "            q_global = F.normalize(q_global.float(), p=2, dim=-1).to(x.dtype)\n",
    "            \n",
    "            # === LINEAR RETRIEVAL: State @ q ===\n",
    "            retrieved = torch.einsum('bhkv,bhtk->bhtv', state, q_global)\n",
    "            \n",
    "            # For diagnostics\n",
    "            state_k_norms = state.norm(dim=-1)\n",
    "            attn_weights_global = torch.einsum('bhtk,bhk->bhtk', q_global.abs(), state_k_norms)\n",
    "            attn_weights_global = F.softmax(attn_weights_global / 0.1, dim=-1)\n",
    "            \n",
    "            # Project retrieved values\n",
    "            retrieved = retrieved.transpose(1, 2).reshape(B, T, H * V)\n",
    "            retrieval_out = self.retrieval_o_proj(retrieved)\n",
    "            \n",
    "            # Learned gate\n",
    "            gate_logits = self.gate_proj(x_norm)\n",
    "            gate = torch.sigmoid(gate_logits)\n",
    "            gate_values = gate.transpose(1, 2).unsqueeze(-1)\n",
    "            \n",
    "            gate_scale = gate.mean(dim=-1, keepdim=True)\n",
    "            retrieval_out = gate_scale * retrieval_out\n",
    "        \n",
    "        # === COMBINE ===\n",
    "        out = x + local_out + retrieval_out\n",
    "        \n",
    "        # Diagnostics\n",
    "        diagnostics = {\n",
    "            'local_attn_entropy': -(attn_weights_local * attn_weights_local.clamp(min=1e-8).log()).sum(-1).mean().item(),\n",
    "            'gate_mean': gate_values.mean().item(),\n",
    "            'gate_std': gate_values.std().item(),\n",
    "            'retrieval_norm': retrieval_out.norm().item(),\n",
    "            'local_norm': local_out.norm().item(),\n",
    "            'global_attn_entropy': (\n",
    "                -(attn_weights_global * attn_weights_global.clamp(min=1e-8).log()).sum(-1).mean().item() \n",
    "                if attn_weights_global is not None else 0\n",
    "            ),\n",
    "        }\n",
    "        \n",
    "        if return_attn:\n",
    "            return out, diagnostics, attn_weights_local, attn_weights_global, gate_values\n",
    "        return out, diagnostics\n",
    "\n",
    "\n",
    "print(\"SlidingWindowAttention loaded (SPARSE RETRIEVAL).\")\n",
    "print(\"  - Dedicated global_q_proj for state retrieval\")\n",
    "print(\"  - ReLU sparsity on retrieval query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-4-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransparentHybrid loaded (FIXED).\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: TransparentHybrid Model (FIXED ACCUMULATION)\n",
    "# =============================================================================\n",
    "\n",
    "class TransparentHybrid(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid GDN + SWA model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: HybridConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        nn.init.normal_(self.embed.weight, std=cfg.init_std)\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        \n",
    "        for i, layer_type in enumerate(cfg.layer_pattern):\n",
    "            if layer_type == 'G':\n",
    "                self.layers.append(GatedDeltaNetLayer(cfg, i))\n",
    "            elif layer_type == 'S':\n",
    "                self.layers.append(SlidingWindowAttention(cfg, i))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
    "            self.ffns.append(FFN(cfg))\n",
    "        \n",
    "        self.norm_f = RMSNorm(cfg.d_model)\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embed.weight\n",
    "        \n",
    "    def _accumulate_state(\n",
    "        self, \n",
    "        accumulated: Optional[torch.Tensor], \n",
    "        new_state: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        if accumulated is None:\n",
    "            return new_state\n",
    "        \n",
    "        strategy = self.cfg.state_accumulation\n",
    "        \n",
    "        # --- FIX: Residual Sum is safer for depth ---\n",
    "        if strategy == 'weighted':\n",
    "             # Treat 'weighted' as residual sum with a scale factor if desired,\n",
    "             # but simple addition is robust.\n",
    "             return accumulated + (self.cfg.state_weight_new * new_state)\n",
    "             \n",
    "        elif strategy == 'replace':\n",
    "            return new_state\n",
    "        elif strategy == 'sum': # Added explicit sum\n",
    "             return accumulated + new_state\n",
    "        else:\n",
    "            # Fallback to sum if unsure, averaging destroys info in deep nets\n",
    "            return accumulated + new_state\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.Tensor, \n",
    "        targets: Optional[torch.Tensor] = None,\n",
    "        return_diagnostics: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[List[Dict]], Optional[torch.Tensor]]:\n",
    "        x = self.embed(input_ids)\n",
    "        accumulated_state = None\n",
    "        all_diagnostics = []\n",
    "        \n",
    "        for i, (layer, ffn) in enumerate(zip(self.layers, self.ffns)):\n",
    "            layer_type = self.cfg.layer_pattern[i]\n",
    "            \n",
    "            if layer_type == 'G':\n",
    "                x, state, diag = layer(x, initial_state=accumulated_state, output_state=True)\n",
    "                accumulated_state = self._accumulate_state(accumulated_state, state)\n",
    "                diag['layer_type'] = 'GDN'\n",
    "            elif layer_type == 'S':\n",
    "                x, diag = layer(x, gdn_state=accumulated_state)\n",
    "                diag['layer_type'] = 'SWA'\n",
    "            \n",
    "            x = ffn(x)\n",
    "            diag['layer_idx'] = i\n",
    "            all_diagnostics.append(diag)\n",
    "        \n",
    "        x = self.norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        if return_diagnostics:\n",
    "            return logits, loss, all_diagnostics, accumulated_state\n",
    "        return logits, loss, None, accumulated_state\n",
    "\n",
    "\n",
    "print(\"TransparentHybrid loaded (FIXED).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-5-diagnostics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic toolkit loaded (FIXED).\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Diagnostic Toolkit (FIXED)\n",
    "# =============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def probe_gdn_state_content(\n",
    "    model: TransparentHybrid, \n",
    "    input_ids: torch.Tensor, \n",
    "    target_token_pos: int = 32,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Check if needle is stored in GDN state.\"\"\"\n",
    "    model.eval()\n",
    "    results = {'layers': [], 'summary': {}}\n",
    "    \n",
    "    x = model.embed(input_ids)\n",
    "    accumulated_state = None\n",
    "    needle_id = input_ids[0, target_token_pos].item()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Needle token ID: {needle_id} at position {target_token_pos}\")\n",
    "    \n",
    "    for i, (layer, ffn) in enumerate(zip(model.layers, model.ffns)):\n",
    "        layer_type = model.cfg.layer_pattern[i]\n",
    "        \n",
    "        if layer_type == 'G':\n",
    "            x, layer_state, diag = layer(x, initial_state=accumulated_state, output_state=True)\n",
    "            accumulated_state = model._accumulate_state(accumulated_state, layer_state)\n",
    "            \n",
    "            # Probe: Can we retrieve the needle?\n",
    "            needle_embed = model.embed.weight[needle_id]\n",
    "            q_needle = layer.q_proj(needle_embed.unsqueeze(0))\n",
    "            q_needle = q_needle.view(1, model.cfg.n_heads, model.cfg.head_dim)\n",
    "            \n",
    "            # --- FIX: Cast query to match state dtype (likely float32) ---\n",
    "            q_needle = q_needle.to(layer_state.dtype) \n",
    "            \n",
    "            retrieved = torch.einsum('bhkv,bhk->bhv', layer_state, q_needle)\n",
    "            needle_retrieval = retrieved.norm().item()\n",
    "            \n",
    "            # Random baseline\n",
    "            random_id = torch.randint(0, model.cfg.vocab_size, (1,)).item()\n",
    "            random_embed = model.embed.weight[random_id]\n",
    "            q_random = layer.q_proj(random_embed.unsqueeze(0))\n",
    "            q_random = q_random.view(1, model.cfg.n_heads, model.cfg.head_dim)\n",
    "            \n",
    "            # --- FIX: Cast random query to match state dtype ---\n",
    "            q_random = q_random.to(layer_state.dtype)\n",
    "            \n",
    "            retrieved_random = torch.einsum('bhkv,bhk->bhv', layer_state, q_random)\n",
    "            random_retrieval = retrieved_random.norm().item()\n",
    "            \n",
    "            snr = needle_retrieval / (random_retrieval + 1e-8)\n",
    "            \n",
    "            layer_result = {\n",
    "                'layer_idx': i,\n",
    "                'layer_type': 'GDN',\n",
    "                'state_norm': layer_state.norm().item(),\n",
    "                'needle_retrieval': needle_retrieval,\n",
    "                'random_retrieval': random_retrieval,\n",
    "                'signal_to_noise': snr,\n",
    "                'beta_mean': diag['beta_mean'],\n",
    "                'beta_std': diag['beta_std'],\n",
    "                'g_mean': diag['g_mean'],\n",
    "            }\n",
    "            results['layers'].append(layer_result)\n",
    "            \n",
    "            if verbose:\n",
    "                status = \"✓\" if snr > 1.0 else \"✗\"\n",
    "                snr_quality = \"GOOD\" if snr > 1.0 else \"WEAK\"\n",
    "                print(f\"\\n  [GDN Layer {i}] {status}\")\n",
    "                print(f\"      State norm: {layer_state.norm().item():.4f}\")\n",
    "                print(f\"      Needle Retrieval: {needle_retrieval:.6f}\")\n",
    "                print(f\"      Random Retrieval: {random_retrieval:.6f}\")\n",
    "                print(f\"      Signal-to-Noise:  {snr:.4f} ({snr_quality})\")\n",
    "                print(f\"      β={diag['beta_mean']:.3f}±{diag['beta_std']:.3f}, g={diag['g_mean']:.3f}\")\n",
    "        else:\n",
    "            x, _ = layer(x, gdn_state=accumulated_state)\n",
    "        \n",
    "        x = ffn(x)\n",
    "    \n",
    "    gdn_results = [r for r in results['layers'] if r['layer_type'] == 'GDN']\n",
    "    if gdn_results:\n",
    "        avg_snr = sum(r['signal_to_noise'] for r in gdn_results) / len(gdn_results)\n",
    "        max_snr = max(r['signal_to_noise'] for r in gdn_results)\n",
    "        results['summary'] = {'avg_snr': avg_snr, 'max_snr': max_snr, 'needle_stored': max_snr > 1.0}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n  Summary: avg_SNR={avg_snr:.4f}, max_SNR={max_snr:.4f}\")\n",
    "            print(f\"  → Needle {'IS' if max_snr > 1.0 else 'NOT'} stored in GDN state\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_full_diagnostic(model, seq_len=128, needle_pos=32):\n",
    "    \"\"\"Run complete diagnostic suite.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FULL DIAGNOSTIC SUITE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "    print(f\"Needle position: {needle_pos}\")\n",
    "    \n",
    "    # Create test sequence\n",
    "    seq = torch.randint(0, model.cfg.vocab_size - 100, (1, seq_len), device=device)\n",
    "    needle_id = 50000\n",
    "    seq[0, needle_pos] = needle_id\n",
    "    \n",
    "    print(f\"\\n--- GDN State Analysis ---\")\n",
    "    gdn_result = probe_gdn_state_content(model, seq, needle_pos)\n",
    "    \n",
    "    return gdn_result\n",
    "\n",
    "\n",
    "print(\"Diagnostic toolkit loaded (FIXED).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-6-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading dataset...\n",
      "Tokenizing...\n",
      "Total tokens: 2,000,000\n",
      "Batches per epoch: 976\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Data Loading\n",
    "# =============================================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len=128):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.tokens) - 1) // self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_len\n",
    "        chunk = self.tokens[start:start + self.seq_len + 1]\n",
    "        return torch.tensor(chunk, dtype=torch.long)\n",
    "\n",
    "\n",
    "def load_data(n_tokens=2_000_000, seq_len=128, batch_size=16):\n",
    "    \"\"\"Load and tokenize training data.\"\"\"\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train')\n",
    "    \n",
    "    print(\"Tokenizing...\")\n",
    "    all_tokens = []\n",
    "    for item in dataset:\n",
    "        if item['text'].strip():\n",
    "            tokens = tokenizer.encode(item['text'])\n",
    "            all_tokens.extend(tokens)\n",
    "            if len(all_tokens) >= n_tokens:\n",
    "                break\n",
    "    \n",
    "    all_tokens = all_tokens[:n_tokens]\n",
    "    print(f\"Total tokens: {len(all_tokens):,}\")\n",
    "    \n",
    "    ds = TextDataset(all_tokens, seq_len=seq_len)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    return loader, tokenizer\n",
    "\n",
    "\n",
    "# Load data\n",
    "data_loader, tokenizer = load_data(n_tokens=2_000_000, seq_len=128, batch_size=16)\n",
    "print(f\"Batches per epoch: {len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-7-retrieval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval infrastructure loaded.\n",
      "  - RetrievalDataGenerator: Creates synthetic retrieval tasks\n",
      "  - compute_auxiliary_retrieval_loss(): Direct retrieval gradient\n",
      "  - proper_niah_test(): NIAH with MARKER + CUE\n",
      "  - test_niah_by_distance(): Find capacity limits\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Retrieval Training & Testing Infrastructure\n",
    "# =============================================================================\n",
    "#\n",
    "# This cell provides:\n",
    "# 1. Proper NIAH test with MARKER + CUE tokens\n",
    "# 2. Synthetic retrieval data generator\n",
    "# 3. Auxiliary retrieval loss computation\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "class RetrievalDataGenerator:\n",
    "    \"\"\"\n",
    "    Generates synthetic retrieval training examples.\n",
    "    \n",
    "    Format: [context] MARKER value [distractor] CUE -> value\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: HybridConfig):\n",
    "        self.vocab_size = cfg.vocab_size\n",
    "        self.marker_token = cfg.marker_token\n",
    "        self.cue_token = cfg.cue_token\n",
    "        \n",
    "    def generate_batch(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        seq_len: int,\n",
    "        device: str = 'cuda',\n",
    "        min_distance: int = 10,\n",
    "        max_distance: int = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Generate retrieval training batch.\n",
    "        \n",
    "        Returns:\n",
    "            input_ids: [B, seq_len]\n",
    "            targets: [B, seq_len] with -100 for non-retrieval positions\n",
    "            needle_ids: [B] the tokens to retrieve\n",
    "        \"\"\"\n",
    "        if max_distance is None:\n",
    "            max_distance = seq_len - 10\n",
    "        \n",
    "        # Random base sequence\n",
    "        input_ids = torch.randint(\n",
    "            0, self.vocab_size - 100,\n",
    "            (batch_size, seq_len),\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Random needles\n",
    "        needle_ids = torch.randint(\n",
    "            1000, self.vocab_size - 100,\n",
    "            (batch_size,),\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Targets: only care about retrieval position\n",
    "        targets = torch.full(\n",
    "            (batch_size, seq_len),\n",
    "            -100,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            # Varied distance for curriculum\n",
    "            distance = torch.randint(min_distance, max_distance, (1,)).item()\n",
    "            marker_pos = max(2, seq_len - distance - 3)\n",
    "            \n",
    "            # Place MARKER, needle, and CUE\n",
    "            input_ids[b, marker_pos] = self.marker_token\n",
    "            input_ids[b, marker_pos + 1] = needle_ids[b]\n",
    "            input_ids[b, -2] = self.cue_token\n",
    "            \n",
    "            # Target: position after CUE should predict needle\n",
    "            targets[b, -1] = needle_ids[b]\n",
    "        \n",
    "        return input_ids, targets, needle_ids\n",
    "\n",
    "\n",
    "def compute_auxiliary_retrieval_loss(\n",
    "    model,\n",
    "    batch_size: int = 8,\n",
    "    seq_len: int = 128,\n",
    "    device: str = 'cuda',\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Auxiliary loss: Direct gradient for state → retrieval pathway.\n",
    "    \n",
    "    This explicitly trains:\n",
    "    1. GDN to store needle when MARKER is seen\n",
    "    2. State to retain needle across distractors\n",
    "    3. SWA to retrieve when CUE is seen\n",
    "    \"\"\"\n",
    "    retrieval_gen = RetrievalDataGenerator(model.cfg)\n",
    "    input_ids, targets, needles = retrieval_gen.generate_batch(\n",
    "        batch_size, seq_len, device\n",
    "    )\n",
    "    \n",
    "    logits, _, _, _ = model(input_ids, return_diagnostics=False)\n",
    "    \n",
    "    # Loss only on retrieval positions (where targets != -100)\n",
    "    loss = F.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        targets.view(-1),\n",
    "        ignore_index=-100\n",
    "    )\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def proper_niah_test(\n",
    "    model,\n",
    "    seq_len: int = 128,\n",
    "    needle_pos: int = 32,\n",
    "    n_trials: int = 30,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    NIAH test with proper retrieval cue.\n",
    "    \n",
    "    Format: [hay] MARKER needle [hay] CUE -> should predict needle\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        needle_id = torch.randint(1000, cfg.vocab_size - 100, (1,)).item()\n",
    "        \n",
    "        # Build sequence\n",
    "        seq = torch.randint(0, cfg.vocab_size - 100, (1, seq_len), device=device)\n",
    "        seq[0, needle_pos - 1] = cfg.marker_token\n",
    "        seq[0, needle_pos] = needle_id\n",
    "        seq[0, -2] = cfg.cue_token\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _, _, _ = model(seq)\n",
    "        \n",
    "        pred_probs = F.softmax(logits[0, -1, :], dim=-1)\n",
    "        needle_prob = pred_probs[needle_id].item()\n",
    "        needle_rank = (pred_probs > needle_prob).sum().item() + 1\n",
    "        \n",
    "        random_id = torch.randint(1000, cfg.vocab_size - 100, (1,)).item()\n",
    "        random_prob = pred_probs[random_id].item()\n",
    "        \n",
    "        ratio = needle_prob / (random_prob + 1e-10)\n",
    "        \n",
    "        results.append({\n",
    "            'needle_id': needle_id,\n",
    "            'needle_prob': needle_prob,\n",
    "            'needle_rank': needle_rank,\n",
    "            'ratio': ratio,\n",
    "            'success': ratio > 1.0,\n",
    "        })\n",
    "    \n",
    "    avg_ratio = sum(r['ratio'] for r in results) / len(results)\n",
    "    avg_rank = sum(r['needle_rank'] for r in results) / len(results)\n",
    "    success_rate = sum(r['success'] for r in results) / len(results)\n",
    "    \n",
    "    status = \"PASS\" if avg_ratio > 1.0 else \"FAIL\"\n",
    "    print(f\"NIAH: {avg_ratio:.4f}x random ({status})\")\n",
    "    print(f\"  Avg rank: {avg_rank:.0f}/{cfg.vocab_size}, Success: {success_rate*100:.1f}%\")\n",
    "    \n",
    "    return {'avg_ratio': avg_ratio, 'avg_rank': avg_rank, 'success_rate': success_rate}\n",
    "\n",
    "\n",
    "def test_niah_by_distance(\n",
    "    model,\n",
    "    distances: List[int] = [5, 10, 20, 40, 60, 95],\n",
    "    n_trials: int = 20,\n",
    "):\n",
    "    \"\"\"Test NIAH at increasing distances to find capacity limit.\"\"\"\n",
    "    model.eval()\n",
    "    print(\"\\nNIAH by distance:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results = []\n",
    "    seq_len = 128\n",
    "    \n",
    "    for dist in distances:\n",
    "        needle_pos = max(2, seq_len - dist - 2)\n",
    "        result = proper_niah_test(model, seq_len=seq_len, needle_pos=needle_pos, n_trials=n_trials)\n",
    "        results.append({'distance': dist, **result})\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Retrieval infrastructure loaded.\")\n",
    "print(\"  - RetrievalDataGenerator: Creates synthetic retrieval tasks\")\n",
    "print(\"  - compute_auxiliary_retrieval_loss(): Direct retrieval gradient\")\n",
    "print(\"  - proper_niah_test(): NIAH with MARKER + CUE\")\n",
    "print(\"  - test_niah_by_distance(): Find capacity limits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-8-training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed training loop loaded.\n",
      "  - LM loss + Auxiliary retrieval loss on LM batches\n",
      "  - Pure retrieval batches mixed in\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Mixed Training Loop (LM + Retrieval + Auxiliary)\n",
    "# =============================================================================\n",
    "\n",
    "def train_mixed(\n",
    "    model,\n",
    "    lm_data_loader,\n",
    "    steps: int = 20000,\n",
    "    lr: float = 3e-4,\n",
    "    warmup_steps: int = 2000,\n",
    "    retrieval_ratio: float = 0.1,\n",
    "    auxiliary_weight: float = 0.1,\n",
    "    log_every: int = 100,\n",
    "    niah_every: int = 1000,\n",
    "    device: str = 'cuda',\n",
    "):\n",
    "    \"\"\"\n",
    "    Training with mixed objectives:\n",
    "    1. LM loss (standard next-token prediction)\n",
    "    2. Retrieval loss (synthetic MARKER/CUE tasks)\n",
    "    3. Auxiliary retrieval loss (direct state→retrieval gradient)\n",
    "    \n",
    "    Args:\n",
    "        retrieval_ratio: Fraction of batches that are pure retrieval\n",
    "        auxiliary_weight: Weight for auxiliary loss added to LM batches\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=steps, eta_min=lr * 0.01)\n",
    "    \n",
    "    retrieval_gen = RetrievalDataGenerator(model.cfg)\n",
    "    lm_iter = iter(lm_data_loader)\n",
    "    \n",
    "    history = {'lm_loss': [], 'ret_loss': [], 'aux_loss': [], 'niah': []}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # Warmup\n",
    "        if step < warmup_steps:\n",
    "            lr_scale = (step + 1) / warmup_steps\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr * lr_scale\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Decide batch type\n",
    "        if torch.rand(1).item() < retrieval_ratio:\n",
    "            # Pure retrieval batch\n",
    "            input_ids, targets, _ = retrieval_gen.generate_batch(16, 128, device)\n",
    "            logits, _, diags, _ = model(input_ids, return_diagnostics=True)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "            history['ret_loss'].append(loss.item())\n",
    "            batch_type = 'RET'\n",
    "        else:\n",
    "            # LM batch + auxiliary loss\n",
    "            try:\n",
    "                batch = next(lm_iter)\n",
    "            except StopIteration:\n",
    "                lm_iter = iter(lm_data_loader)\n",
    "                batch = next(lm_iter)\n",
    "            \n",
    "            input_ids = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "            \n",
    "            logits, lm_loss, diags, _ = model(input_ids, targets, return_diagnostics=True)\n",
    "            \n",
    "            # Add auxiliary retrieval loss\n",
    "            aux_loss = compute_auxiliary_retrieval_loss(model, batch_size=8, seq_len=128, device=device)\n",
    "            \n",
    "            loss = lm_loss + auxiliary_weight * aux_loss\n",
    "            \n",
    "            history['lm_loss'].append(lm_loss.item())\n",
    "            history['aux_loss'].append(aux_loss.item())\n",
    "            batch_type = 'LM+AUX'\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step >= warmup_steps:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Logging\n",
    "        if step % log_every == 0:\n",
    "            lm_avg = sum(history['lm_loss'][-100:]) / max(1, len(history['lm_loss'][-100:]))\n",
    "            ret_avg = sum(history['ret_loss'][-100:]) / max(1, len(history['ret_loss'][-100:]))\n",
    "            aux_avg = sum(history['aux_loss'][-100:]) / max(1, len(history['aux_loss'][-100:]))\n",
    "            \n",
    "            beta = diags[0].get('beta_mean', 0) if diags else 0\n",
    "            g = diags[0].get('g_mean', 0) if diags else 0\n",
    "            gate = diags[1].get('gate_mean', 0) if diags and len(diags) > 1 else 0\n",
    "            \n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            print(f\"[{step:5d}] LM={lm_avg:.3f} RET={ret_avg:.3f} AUX={aux_avg:.3f} | \"\n",
    "                  f\"β={beta:.3f} g={g:.3f} gate={gate:.2f} | lr={current_lr:.2e}\")\n",
    "        \n",
    "        # NIAH check\n",
    "        if step % niah_every == 0 and step > 0:\n",
    "            model.eval()\n",
    "            niah_result = proper_niah_test(model, seq_len=128, needle_pos=32, n_trials=20)\n",
    "            history['niah'].append(niah_result['avg_ratio'])\n",
    "            model.train()\n",
    "    \n",
    "    print(f\"\\nTraining complete in {(time.time() - start_time)/60:.1f} min\")\n",
    "    print(f\"Final LM loss: {sum(history['lm_loss'][-100:])/100:.3f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"Mixed training loop loaded.\")\n",
    "print(\"  - LM loss + Auxiliary retrieval loss on LM batches\")\n",
    "print(\"  - Pure retrieval batches mixed in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-9-model-init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Architecture: GS\n",
      "  GDN layers: [0]\n",
      "  SWA layers: [1]\n",
      "  Parameters: 15,298,064\n",
      "  State shape: [B, 8, 32, 64]\n",
      "\n",
      "Initial diagnostics:\n",
      "  GDN β=0.128 (should be ~0.12)\n",
      "  GDN g=0.500\n",
      "  SWA gate=0.500\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Model Instantiation\n",
    "# =============================================================================\n",
    "\n",
    "cfg = HybridConfig(\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    "    layer_pattern=\"GS\",\n",
    "    window_size=64,\n",
    "    state_accumulation='weighted',\n",
    "    state_weight_new=0.5,\n",
    ")\n",
    "\n",
    "model = TransparentHybrid(cfg).cuda().bfloat16()\n",
    "params = count_params(model)\n",
    "\n",
    "print(f\"\\nArchitecture: {cfg.layer_pattern}\")\n",
    "print(f\"  GDN layers: {cfg.gdn_indices}\")\n",
    "print(f\"  SWA layers: {cfg.swa_indices}\")\n",
    "print(f\"  Parameters: {params:,}\")\n",
    "print(f\"  State shape: [B, {cfg.n_heads}, {cfg.head_dim}, {cfg.value_dim}]\")\n",
    "\n",
    "# Quick forward test\n",
    "x = torch.randint(0, 1000, (1, 128), device='cuda')\n",
    "with torch.no_grad():\n",
    "    logits, _, diags, state = model(x, return_diagnostics=True)\n",
    "\n",
    "print(f\"\\nInitial diagnostics:\")\n",
    "print(f\"  GDN β={diags[0]['beta_mean']:.3f} (should be ~0.12)\")\n",
    "print(f\"  GDN g={diags[0]['g_mean']:.3f}\")\n",
    "print(f\"  SWA gate={diags[1]['gate_mean']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-10-baseline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "# PRE-TRAINING BASELINE\n",
      "############################################################\n",
      "\n",
      "1. NIAH with proper retrieval cue:\n",
      "NIAH: 1.1766x random (PASS)\n",
      "  Avg rank: 28167/50257, Success: 56.7%\n",
      "\n",
      "2. NIAH by distance:\n",
      "\n",
      "NIAH by distance:\n",
      "----------------------------------------\n",
      "NIAH: 1.0494x random (PASS)\n",
      "  Avg rank: 29253/50257, Success: 40.0%\n",
      "NIAH: 1.1925x random (PASS)\n",
      "  Avg rank: 25554/50257, Success: 45.0%\n",
      "NIAH: 0.9634x random (FAIL)\n",
      "  Avg rank: 28194/50257, Success: 35.0%\n",
      "NIAH: 1.1655x random (PASS)\n",
      "  Avg rank: 23925/50257, Success: 50.0%\n",
      "NIAH: 1.1483x random (PASS)\n",
      "  Avg rank: 24883/50257, Success: 50.0%\n",
      "NIAH: 0.9975x random (FAIL)\n",
      "  Avg rank: 22382/50257, Success: 40.0%\n",
      "\n",
      "3. GDN state analysis:\n",
      "\n",
      "============================================================\n",
      "FULL DIAGNOSTIC SUITE\n",
      "============================================================\n",
      "Sequence length: 128\n",
      "Needle position: 32\n",
      "\n",
      "--- GDN State Analysis ---\n",
      "Needle token ID: 50000 at position 32\n",
      "\n",
      "  [GDN Layer 0] ✗\n",
      "      State norm: 1.6587\n",
      "      Needle Retrieval: 0.012147\n",
      "      Random Retrieval: 0.028279\n",
      "      Signal-to-Noise:  0.4295 (WEAK)\n",
      "      β=0.134±0.066, g=0.504\n",
      "\n",
      "  Summary: avg_SNR=0.4295, max_SNR=0.4295\n",
      "  → Needle NOT stored in GDN state\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'layers': [{'layer_idx': 0,\n",
       "   'layer_type': 'GDN',\n",
       "   'state_norm': 1.658738613128662,\n",
       "   'needle_retrieval': 0.012146753259003162,\n",
       "   'random_retrieval': 0.028279220685362816,\n",
       "   'signal_to_noise': 0.429529126663628,\n",
       "   'beta_mean': 0.1337890625,\n",
       "   'beta_std': 0.06640625,\n",
       "   'g_mean': 0.50390625}],\n",
       " 'summary': {'avg_snr': 0.429529126663628,\n",
       "  'max_snr': 0.429529126663628,\n",
       "  'needle_stored': False}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: Pre-Training Baseline\n",
    "# =============================================================================\n",
    "\n",
    "print(\"#\" * 60)\n",
    "print(\"# PRE-TRAINING BASELINE\")\n",
    "print(\"#\" * 60)\n",
    "\n",
    "print(\"\\n1. NIAH with proper retrieval cue:\")\n",
    "baseline_niah = proper_niah_test(model, seq_len=128, needle_pos=32, n_trials=30)\n",
    "\n",
    "print(\"\\n2. NIAH by distance:\")\n",
    "baseline_distances = test_niah_by_distance(model, distances=[5, 10, 20, 40, 60, 95], n_trials=20)\n",
    "\n",
    "print(\"\\n3. GDN state analysis:\")\n",
    "run_full_diagnostic(model, seq_len=128, needle_pos=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-11-training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "# MIXED TRAINING: LM + Retrieval + Auxiliary\n",
      "############################################################\n",
      "[    0] LM=10.875 RET=0.000 AUX=10.875 | β=0.134 g=0.500 gate=0.50 | lr=1.50e-07\n",
      "[  100] LM=10.871 RET=10.906 AUX=10.894 | β=0.133 g=0.498 gate=0.50 | lr=1.51e-05\n",
      "[  200] LM=10.784 RET=10.892 AUX=10.873 | β=0.133 g=0.500 gate=0.50 | lr=3.01e-05\n",
      "[  300] LM=10.471 RET=10.895 AUX=10.890 | β=0.136 g=0.500 gate=0.50 | lr=4.51e-05\n",
      "[  400] LM=9.954 RET=10.889 AUX=10.884 | β=0.139 g=0.498 gate=0.49 | lr=6.01e-05\n",
      "[  500] LM=9.436 RET=10.891 AUX=10.885 | β=0.143 g=0.496 gate=0.49 | lr=7.51e-05\n",
      "[  600] LM=8.836 RET=10.902 AUX=10.891 | β=0.130 g=0.504 gate=0.49 | lr=9.01e-05\n",
      "[  700] LM=8.229 RET=10.906 AUX=10.920 | β=0.155 g=0.496 gate=0.47 | lr=1.05e-04\n",
      "[  800] LM=7.858 RET=10.908 AUX=10.889 | β=0.166 g=0.494 gate=0.48 | lr=1.20e-04\n",
      "[  900] LM=7.626 RET=10.916 AUX=10.880 | β=0.177 g=0.494 gate=0.52 | lr=1.35e-04\n",
      "[ 1000] LM=7.453 RET=10.910 AUX=10.892 | β=0.189 g=0.494 gate=0.59 | lr=1.50e-04\n",
      "NIAH: 1.0076x random (PASS)\n",
      "  Avg rank: 21853/50257, Success: 40.0%\n",
      "[ 1100] LM=7.295 RET=10.909 AUX=10.869 | β=0.199 g=0.492 gate=0.67 | lr=1.65e-04\n",
      "[ 1200] LM=7.193 RET=10.909 AUX=10.891 | β=0.216 g=0.494 gate=0.73 | lr=1.80e-04\n",
      "[ 1300] LM=7.064 RET=10.904 AUX=10.873 | β=0.230 g=0.498 gate=0.73 | lr=1.95e-04\n",
      "[ 1400] LM=6.978 RET=10.904 AUX=10.866 | β=0.242 g=0.508 gate=0.72 | lr=2.10e-04\n",
      "[ 1500] LM=6.870 RET=10.902 AUX=10.892 | β=0.246 g=0.508 gate=0.63 | lr=2.25e-04\n",
      "[ 1600] LM=6.790 RET=10.902 AUX=10.905 | β=0.254 g=0.508 gate=0.59 | lr=2.40e-04\n",
      "[ 1700] LM=6.695 RET=10.897 AUX=10.873 | β=0.250 g=0.508 gate=0.56 | lr=2.55e-04\n",
      "[ 1800] LM=6.607 RET=10.896 AUX=10.889 | β=0.247 g=0.520 gate=0.54 | lr=2.70e-04\n",
      "[ 1900] LM=6.526 RET=10.896 AUX=10.907 | β=0.249 g=0.535 gate=0.52 | lr=2.85e-04\n",
      "[ 2000] LM=6.437 RET=10.892 AUX=10.886 | β=0.132 g=0.498 gate=0.62 | lr=3.00e-04\n",
      "NIAH: 1.2650x random (PASS)\n",
      "  Avg rank: 20376/50257, Success: 55.0%\n",
      "[ 2100] LM=6.375 RET=10.894 AUX=10.890 | β=0.244 g=0.559 gate=0.50 | lr=3.00e-04\n",
      "[ 2200] LM=6.264 RET=10.895 AUX=10.928 | β=0.134 g=0.498 gate=0.61 | lr=3.00e-04\n",
      "[ 2300] LM=6.175 RET=10.893 AUX=10.926 | β=0.249 g=0.582 gate=0.53 | lr=3.00e-04\n",
      "[ 2400] LM=6.141 RET=10.894 AUX=10.919 | β=0.248 g=0.582 gate=0.50 | lr=3.00e-04\n",
      "[ 2500] LM=6.132 RET=10.896 AUX=10.922 | β=0.252 g=0.590 gate=0.53 | lr=3.00e-04\n",
      "[ 2600] LM=6.079 RET=10.897 AUX=10.897 | β=0.252 g=0.586 gate=0.54 | lr=2.99e-04\n",
      "[ 2700] LM=6.049 RET=10.901 AUX=10.900 | β=0.252 g=0.586 gate=0.55 | lr=2.99e-04\n",
      "[ 2800] LM=6.001 RET=10.896 AUX=10.901 | β=0.247 g=0.586 gate=0.54 | lr=2.99e-04\n",
      "[ 2900] LM=5.998 RET=10.899 AUX=10.914 | β=0.139 g=0.500 gate=0.64 | lr=2.99e-04\n",
      "[ 3000] LM=5.965 RET=10.898 AUX=10.974 | β=0.260 g=0.602 gate=0.55 | lr=2.98e-04\n",
      "NIAH: 0.9434x random (FAIL)\n",
      "  Avg rank: 28304/50257, Success: 45.0%\n",
      "[ 3100] LM=5.910 RET=10.898 AUX=10.893 | β=0.254 g=0.598 gate=0.56 | lr=2.98e-04\n",
      "[ 3200] LM=5.898 RET=10.895 AUX=10.913 | β=0.138 g=0.504 gate=0.64 | lr=2.97e-04\n",
      "[ 3300] LM=5.794 RET=10.901 AUX=10.914 | β=0.264 g=0.617 gate=0.56 | lr=2.97e-04\n",
      "[ 3400] LM=5.760 RET=10.901 AUX=10.941 | β=0.268 g=0.625 gate=0.56 | lr=2.96e-04\n",
      "[ 3500] LM=5.765 RET=10.902 AUX=10.930 | β=0.143 g=0.508 gate=0.61 | lr=2.96e-04\n",
      "[ 3600] LM=5.739 RET=10.908 AUX=10.914 | β=0.277 g=0.625 gate=0.56 | lr=2.95e-04\n",
      "[ 3700] LM=5.747 RET=10.910 AUX=10.930 | β=0.275 g=0.621 gate=0.58 | lr=2.95e-04\n",
      "[ 3800] LM=5.724 RET=10.915 AUX=10.943 | β=0.275 g=0.621 gate=0.57 | lr=2.94e-04\n",
      "[ 3900] LM=5.725 RET=10.920 AUX=10.921 | β=0.271 g=0.625 gate=0.58 | lr=2.93e-04\n",
      "[ 4000] LM=5.705 RET=10.913 AUX=10.928 | β=0.277 g=0.629 gate=0.56 | lr=2.93e-04\n",
      "NIAH: 1.1419x random (PASS)\n",
      "  Avg rank: 24246/50257, Success: 50.0%\n",
      "[ 4100] LM=5.691 RET=10.910 AUX=10.916 | β=0.277 g=0.617 gate=0.57 | lr=2.92e-04\n",
      "[ 4200] LM=5.690 RET=10.918 AUX=10.912 | β=0.273 g=0.621 gate=0.56 | lr=2.91e-04\n",
      "[ 4300] LM=5.690 RET=10.921 AUX=10.915 | β=0.279 g=0.621 gate=0.55 | lr=2.90e-04\n",
      "[ 4400] LM=5.607 RET=10.919 AUX=10.931 | β=0.285 g=0.637 gate=0.56 | lr=2.90e-04\n",
      "[ 4500] LM=5.607 RET=10.921 AUX=10.914 | β=0.287 g=0.637 gate=0.59 | lr=2.89e-04\n",
      "[ 4600] LM=5.602 RET=10.930 AUX=10.918 | β=0.291 g=0.641 gate=0.59 | lr=2.88e-04\n",
      "[ 4700] LM=5.574 RET=10.932 AUX=10.943 | β=0.291 g=0.637 gate=0.59 | lr=2.87e-04\n",
      "[ 4800] LM=5.570 RET=10.932 AUX=10.932 | β=0.293 g=0.641 gate=0.57 | lr=2.86e-04\n",
      "[ 4900] LM=5.563 RET=10.932 AUX=10.908 | β=0.299 g=0.641 gate=0.60 | lr=2.85e-04\n",
      "[ 5000] LM=5.562 RET=10.927 AUX=10.932 | β=0.295 g=0.641 gate=0.59 | lr=2.84e-04\n",
      "NIAH: 1.1875x random (PASS)\n",
      "  Avg rank: 21012/50257, Success: 50.0%\n",
      "[ 5100] LM=5.550 RET=10.940 AUX=10.925 | β=0.301 g=0.641 gate=0.61 | lr=2.83e-04\n",
      "[ 5200] LM=5.558 RET=10.939 AUX=10.923 | β=0.293 g=0.637 gate=0.60 | lr=2.82e-04\n",
      "[ 5300] LM=5.549 RET=10.934 AUX=10.947 | β=0.297 g=0.641 gate=0.61 | lr=2.80e-04\n",
      "[ 5400] LM=5.510 RET=10.932 AUX=10.891 | β=0.301 g=0.648 gate=0.61 | lr=2.79e-04\n",
      "[ 5500] LM=5.444 RET=10.932 AUX=10.890 | β=0.311 g=0.656 gate=0.60 | lr=2.78e-04\n",
      "[ 5600] LM=5.475 RET=10.931 AUX=10.906 | β=0.311 g=0.645 gate=0.63 | lr=2.77e-04\n",
      "[ 5700] LM=5.448 RET=10.928 AUX=10.904 | β=0.309 g=0.648 gate=0.62 | lr=2.76e-04\n",
      "[ 5800] LM=5.458 RET=10.927 AUX=10.954 | β=0.312 g=0.652 gate=0.62 | lr=2.74e-04\n",
      "[ 5900] LM=5.472 RET=10.930 AUX=10.943 | β=0.311 g=0.648 gate=0.62 | lr=2.73e-04\n",
      "[ 6000] LM=5.455 RET=10.926 AUX=10.932 | β=0.156 g=0.508 gate=0.61 | lr=2.72e-04\n",
      "NIAH: 1.1548x random (PASS)\n",
      "  Avg rank: 24053/50257, Success: 50.0%\n",
      "[ 6100] LM=5.473 RET=10.930 AUX=10.936 | β=0.309 g=0.656 gate=0.59 | lr=2.70e-04\n",
      "[ 6200] LM=5.475 RET=10.924 AUX=10.929 | β=0.314 g=0.648 gate=0.62 | lr=2.69e-04\n",
      "[ 6300] LM=5.450 RET=10.932 AUX=10.933 | β=0.314 g=0.648 gate=0.61 | lr=2.67e-04\n",
      "[ 6400] LM=5.444 RET=10.936 AUX=10.904 | β=0.311 g=0.652 gate=0.62 | lr=2.66e-04\n",
      "[ 6500] LM=5.408 RET=10.941 AUX=10.928 | β=0.157 g=0.512 gate=0.61 | lr=2.64e-04\n",
      "[ 6600] LM=5.381 RET=10.942 AUX=10.908 | β=0.322 g=0.664 gate=0.61 | lr=2.63e-04\n",
      "[ 6700] LM=5.367 RET=10.944 AUX=10.929 | β=0.328 g=0.664 gate=0.61 | lr=2.61e-04\n",
      "[ 6800] LM=5.377 RET=10.942 AUX=10.928 | β=0.322 g=0.656 gate=0.61 | lr=2.60e-04\n",
      "[ 6900] LM=5.372 RET=10.943 AUX=10.923 | β=0.322 g=0.656 gate=0.62 | lr=2.58e-04\n",
      "[ 7000] LM=5.371 RET=10.944 AUX=10.913 | β=0.155 g=0.508 gate=0.61 | lr=2.56e-04\n",
      "NIAH: 1.1040x random (PASS)\n",
      "  Avg rank: 26338/50257, Success: 35.0%\n",
      "[ 7100] LM=5.406 RET=10.946 AUX=10.939 | β=0.322 g=0.664 gate=0.61 | lr=2.55e-04\n",
      "[ 7200] LM=5.398 RET=10.947 AUX=10.941 | β=0.322 g=0.664 gate=0.60 | lr=2.53e-04\n",
      "[ 7300] LM=5.397 RET=10.946 AUX=10.918 | β=0.320 g=0.660 gate=0.61 | lr=2.51e-04\n",
      "[ 7400] LM=5.378 RET=10.947 AUX=10.925 | β=0.324 g=0.660 gate=0.62 | lr=2.50e-04\n",
      "[ 7500] LM=5.383 RET=10.944 AUX=10.936 | β=0.324 g=0.660 gate=0.63 | lr=2.48e-04\n",
      "[ 7600] LM=5.344 RET=10.941 AUX=10.921 | β=0.328 g=0.664 gate=0.63 | lr=2.46e-04\n",
      "[ 7700] LM=5.331 RET=10.941 AUX=10.914 | β=0.336 g=0.664 gate=0.65 | lr=2.44e-04\n",
      "[ 7800] LM=5.311 RET=10.933 AUX=10.913 | β=0.332 g=0.668 gate=0.63 | lr=2.42e-04\n",
      "[ 7900] LM=5.315 RET=10.931 AUX=10.916 | β=0.334 g=0.660 gate=0.63 | lr=2.41e-04\n",
      "[ 8000] LM=5.307 RET=10.929 AUX=10.908 | β=0.334 g=0.660 gate=0.62 | lr=2.39e-04\n",
      "NIAH: 0.9967x random (FAIL)\n",
      "  Avg rank: 25442/50257, Success: 40.0%\n",
      "[ 8100] LM=5.321 RET=10.922 AUX=10.887 | β=0.159 g=0.512 gate=0.61 | lr=2.37e-04\n",
      "[ 8200] LM=5.327 RET=10.925 AUX=10.943 | β=0.336 g=0.664 gate=0.64 | lr=2.35e-04\n",
      "[ 8300] LM=5.316 RET=10.923 AUX=10.916 | β=0.336 g=0.664 gate=0.62 | lr=2.33e-04\n",
      "[ 8400] LM=5.325 RET=10.925 AUX=10.939 | β=0.344 g=0.668 gate=0.63 | lr=2.31e-04\n",
      "[ 8500] LM=5.339 RET=10.919 AUX=10.936 | β=0.330 g=0.664 gate=0.66 | lr=2.29e-04\n",
      "[ 8600] LM=5.327 RET=10.921 AUX=10.934 | β=0.340 g=0.664 gate=0.64 | lr=2.27e-04\n",
      "[ 8700] LM=5.278 RET=10.919 AUX=10.936 | β=0.340 g=0.668 gate=0.64 | lr=2.25e-04\n",
      "[ 8800] LM=5.254 RET=10.916 AUX=10.938 | β=0.342 g=0.664 gate=0.64 | lr=2.23e-04\n",
      "[ 8900] LM=5.259 RET=10.918 AUX=10.943 | β=0.344 g=0.668 gate=0.65 | lr=2.21e-04\n",
      "[ 9000] LM=5.247 RET=10.919 AUX=10.896 | β=0.344 g=0.672 gate=0.63 | lr=2.19e-04\n",
      "NIAH: 1.1717x random (PASS)\n",
      "  Avg rank: 24605/50257, Success: 60.0%\n",
      "[ 9100] LM=5.286 RET=10.919 AUX=10.916 | β=0.344 g=0.664 gate=0.62 | lr=2.17e-04\n",
      "[ 9200] LM=5.285 RET=10.916 AUX=10.897 | β=0.344 g=0.664 gate=0.64 | lr=2.15e-04\n",
      "[ 9300] LM=5.290 RET=10.912 AUX=10.926 | β=0.342 g=0.664 gate=0.64 | lr=2.13e-04\n",
      "[ 9400] LM=5.272 RET=10.911 AUX=10.894 | β=0.344 g=0.664 gate=0.64 | lr=2.10e-04\n",
      "[ 9500] LM=5.277 RET=10.906 AUX=10.916 | β=0.350 g=0.672 gate=0.63 | lr=2.08e-04\n",
      "[ 9600] LM=5.268 RET=10.916 AUX=10.919 | β=0.336 g=0.668 gate=0.61 | lr=2.06e-04\n",
      "[ 9700] LM=5.284 RET=10.914 AUX=10.932 | β=0.346 g=0.664 gate=0.63 | lr=2.04e-04\n",
      "[ 9800] LM=5.224 RET=10.919 AUX=10.937 | β=0.348 g=0.668 gate=0.64 | lr=2.02e-04\n",
      "[ 9900] LM=5.232 RET=10.920 AUX=10.946 | β=0.344 g=0.668 gate=0.64 | lr=2.00e-04\n",
      "[10000] LM=5.226 RET=10.929 AUX=10.936 | β=0.350 g=0.672 gate=0.63 | lr=1.97e-04\n",
      "NIAH: 1.3810x random (PASS)\n",
      "  Avg rank: 24832/50257, Success: 60.0%\n",
      "[10100] LM=5.222 RET=10.932 AUX=10.909 | β=0.350 g=0.668 gate=0.62 | lr=1.95e-04\n",
      "[10200] LM=5.232 RET=10.937 AUX=10.927 | β=0.350 g=0.668 gate=0.65 | lr=1.93e-04\n",
      "[10300] LM=5.250 RET=10.939 AUX=10.900 | β=0.348 g=0.664 gate=0.63 | lr=1.91e-04\n",
      "[10400] LM=5.222 RET=10.938 AUX=10.917 | β=0.352 g=0.668 gate=0.64 | lr=1.88e-04\n",
      "[10500] LM=5.226 RET=10.937 AUX=10.891 | β=0.352 g=0.676 gate=0.62 | lr=1.86e-04\n",
      "[10600] LM=5.254 RET=10.936 AUX=10.939 | β=0.354 g=0.672 gate=0.63 | lr=1.84e-04\n",
      "[10700] LM=5.255 RET=10.929 AUX=10.936 | β=0.350 g=0.668 gate=0.64 | lr=1.82e-04\n",
      "[10800] LM=5.232 RET=10.924 AUX=10.928 | β=0.348 g=0.668 gate=0.63 | lr=1.79e-04\n",
      "[10900] LM=5.176 RET=10.926 AUX=10.922 | β=0.352 g=0.672 gate=0.64 | lr=1.77e-04\n",
      "[11000] LM=5.210 RET=10.931 AUX=10.910 | β=0.170 g=0.516 gate=0.64 | lr=1.75e-04\n",
      "NIAH: 0.8883x random (FAIL)\n",
      "  Avg rank: 26533/50257, Success: 40.0%\n",
      "[11100] LM=5.199 RET=10.935 AUX=10.935 | β=0.355 g=0.664 gate=0.65 | lr=1.72e-04\n",
      "[11200] LM=5.180 RET=10.939 AUX=10.943 | β=0.355 g=0.672 gate=0.65 | lr=1.70e-04\n",
      "[11300] LM=5.189 RET=10.941 AUX=10.959 | β=0.359 g=0.668 gate=0.65 | lr=1.68e-04\n",
      "[11400] LM=5.223 RET=10.933 AUX=10.940 | β=0.357 g=0.668 gate=0.65 | lr=1.65e-04\n",
      "[11500] LM=5.223 RET=10.926 AUX=10.922 | β=0.352 g=0.668 gate=0.62 | lr=1.63e-04\n",
      "[11600] LM=5.205 RET=10.925 AUX=10.947 | β=0.355 g=0.676 gate=0.64 | lr=1.61e-04\n",
      "[11700] LM=5.207 RET=10.929 AUX=10.898 | β=0.173 g=0.520 gate=0.63 | lr=1.58e-04\n",
      "[11800] LM=5.219 RET=10.926 AUX=10.926 | β=0.357 g=0.672 gate=0.64 | lr=1.56e-04\n",
      "[11900] LM=5.214 RET=10.921 AUX=10.908 | β=0.357 g=0.668 gate=0.63 | lr=1.54e-04\n",
      "[12000] LM=5.162 RET=10.911 AUX=10.924 | β=0.352 g=0.672 gate=0.64 | lr=1.51e-04\n",
      "NIAH: 1.2243x random (PASS)\n",
      "  Avg rank: 26504/50257, Success: 45.0%\n",
      "[12100] LM=5.177 RET=10.911 AUX=10.909 | β=0.352 g=0.668 gate=0.62 | lr=1.49e-04\n",
      "[12200] LM=5.182 RET=10.906 AUX=10.923 | β=0.361 g=0.668 gate=0.65 | lr=1.47e-04\n",
      "[12300] LM=5.171 RET=10.903 AUX=10.928 | β=0.352 g=0.668 gate=0.64 | lr=1.44e-04\n",
      "[12400] LM=5.174 RET=10.911 AUX=10.911 | β=0.359 g=0.668 gate=0.65 | lr=1.42e-04\n",
      "[12500] LM=5.201 RET=10.914 AUX=10.881 | β=0.361 g=0.668 gate=0.65 | lr=1.40e-04\n",
      "[12600] LM=5.192 RET=10.922 AUX=10.915 | β=0.363 g=0.672 gate=0.66 | lr=1.38e-04\n",
      "[12700] LM=5.171 RET=10.919 AUX=10.903 | β=0.359 g=0.668 gate=0.66 | lr=1.35e-04\n",
      "[12800] LM=5.216 RET=10.919 AUX=10.906 | β=0.354 g=0.676 gate=0.62 | lr=1.33e-04\n",
      "[12900] LM=5.191 RET=10.917 AUX=10.877 | β=0.357 g=0.668 gate=0.64 | lr=1.31e-04\n",
      "[13000] LM=5.195 RET=10.916 AUX=10.896 | β=0.361 g=0.664 gate=0.67 | lr=1.28e-04\n",
      "NIAH: 1.2529x random (PASS)\n",
      "  Avg rank: 22815/50257, Success: 50.0%\n",
      "[13100] LM=5.157 RET=10.919 AUX=10.911 | β=0.357 g=0.664 gate=0.67 | lr=1.26e-04\n",
      "[13200] LM=5.163 RET=10.916 AUX=10.906 | β=0.174 g=0.520 gate=0.64 | lr=1.24e-04\n",
      "[13300] LM=5.144 RET=10.915 AUX=10.877 | β=0.354 g=0.672 gate=0.64 | lr=1.21e-04\n",
      "[13400] LM=5.168 RET=10.914 AUX=10.882 | β=0.350 g=0.672 gate=0.64 | lr=1.19e-04\n",
      "[13500] LM=5.184 RET=10.912 AUX=10.905 | β=0.361 g=0.672 gate=0.64 | lr=1.17e-04\n",
      "[13600] LM=5.178 RET=10.912 AUX=10.909 | β=0.361 g=0.668 gate=0.64 | lr=1.15e-04\n",
      "[13700] LM=5.180 RET=10.903 AUX=10.906 | β=0.363 g=0.676 gate=0.66 | lr=1.12e-04\n",
      "[13800] LM=5.159 RET=10.902 AUX=10.905 | β=0.365 g=0.668 gate=0.65 | lr=1.10e-04\n",
      "[13900] LM=5.177 RET=10.904 AUX=10.908 | β=0.175 g=0.527 gate=0.64 | lr=1.08e-04\n",
      "[14000] LM=5.173 RET=10.904 AUX=10.931 | β=0.355 g=0.668 gate=0.64 | lr=1.06e-04\n",
      "NIAH: 1.1488x random (PASS)\n",
      "  Avg rank: 21987/50257, Success: 55.0%\n",
      "[14100] LM=5.166 RET=10.909 AUX=10.905 | β=0.363 g=0.672 gate=0.65 | lr=1.03e-04\n",
      "[14200] LM=5.158 RET=10.904 AUX=10.900 | β=0.365 g=0.668 gate=0.66 | lr=1.01e-04\n",
      "[14300] LM=5.174 RET=10.902 AUX=10.912 | β=0.354 g=0.672 gate=0.62 | lr=9.90e-05\n",
      "[14400] LM=5.161 RET=10.901 AUX=10.911 | β=0.174 g=0.520 gate=0.64 | lr=9.68e-05\n",
      "[14500] LM=5.152 RET=10.899 AUX=10.914 | β=0.363 g=0.672 gate=0.65 | lr=9.46e-05\n",
      "[14600] LM=5.158 RET=10.902 AUX=10.926 | β=0.361 g=0.672 gate=0.65 | lr=9.25e-05\n",
      "[14700] LM=5.165 RET=10.902 AUX=10.912 | β=0.359 g=0.672 gate=0.64 | lr=9.04e-05\n",
      "[14800] LM=5.156 RET=10.896 AUX=10.900 | β=0.361 g=0.668 gate=0.66 | lr=8.83e-05\n",
      "[14900] LM=5.152 RET=10.902 AUX=10.917 | β=0.176 g=0.523 gate=0.64 | lr=8.61e-05\n",
      "[15000] LM=5.149 RET=10.903 AUX=10.911 | β=0.363 g=0.672 gate=0.64 | lr=8.41e-05\n",
      "NIAH: 1.2847x random (PASS)\n",
      "  Avg rank: 23136/50257, Success: 45.0%\n",
      "[15100] LM=5.166 RET=10.904 AUX=10.907 | β=0.363 g=0.672 gate=0.64 | lr=8.20e-05\n",
      "[15200] LM=5.151 RET=10.898 AUX=10.914 | β=0.361 g=0.676 gate=0.63 | lr=7.99e-05\n",
      "[15300] LM=5.151 RET=10.895 AUX=10.892 | β=0.365 g=0.668 gate=0.66 | lr=7.79e-05\n",
      "[15400] LM=5.158 RET=10.902 AUX=10.909 | β=0.357 g=0.676 gate=0.62 | lr=7.59e-05\n",
      "[15500] LM=5.162 RET=10.899 AUX=10.899 | β=0.363 g=0.672 gate=0.64 | lr=7.39e-05\n",
      "[15600] LM=5.153 RET=10.904 AUX=10.873 | β=0.177 g=0.527 gate=0.64 | lr=7.19e-05\n",
      "[15700] LM=5.147 RET=10.902 AUX=10.869 | β=0.363 g=0.672 gate=0.64 | lr=7.00e-05\n",
      "[15800] LM=5.147 RET=10.895 AUX=10.902 | β=0.359 g=0.668 gate=0.65 | lr=6.80e-05\n",
      "[15900] LM=5.133 RET=10.899 AUX=10.914 | β=0.175 g=0.523 gate=0.65 | lr=6.61e-05\n",
      "[16000] LM=5.162 RET=10.890 AUX=10.894 | β=0.361 g=0.672 gate=0.65 | lr=6.42e-05\n",
      "NIAH: 1.3582x random (PASS)\n",
      "  Avg rank: 22366/50257, Success: 70.0%\n",
      "[16100] LM=5.170 RET=10.891 AUX=10.901 | β=0.365 g=0.676 gate=0.65 | lr=6.23e-05\n",
      "[16200] LM=5.143 RET=10.902 AUX=10.893 | β=0.363 g=0.672 gate=0.65 | lr=6.05e-05\n",
      "[16300] LM=5.130 RET=10.905 AUX=10.909 | β=0.363 g=0.676 gate=0.66 | lr=5.86e-05\n",
      "[16400] LM=5.152 RET=10.898 AUX=10.905 | β=0.373 g=0.676 gate=0.66 | lr=5.68e-05\n",
      "[16500] LM=5.140 RET=10.902 AUX=10.891 | β=0.355 g=0.672 gate=0.64 | lr=5.50e-05\n",
      "[16600] LM=5.159 RET=10.909 AUX=10.925 | β=0.178 g=0.527 gate=0.65 | lr=5.33e-05\n",
      "[16700] LM=5.144 RET=10.906 AUX=10.906 | β=0.357 g=0.672 gate=0.64 | lr=5.15e-05\n",
      "[16800] LM=5.148 RET=10.904 AUX=10.886 | β=0.365 g=0.676 gate=0.65 | lr=4.98e-05\n",
      "[16900] LM=5.174 RET=10.912 AUX=10.881 | β=0.365 g=0.668 gate=0.66 | lr=4.81e-05\n",
      "[17000] LM=5.147 RET=10.909 AUX=10.881 | β=0.359 g=0.672 gate=0.64 | lr=4.65e-05\n",
      "NIAH: 1.2359x random (PASS)\n",
      "  Avg rank: 22443/50257, Success: 50.0%\n",
      "[17100] LM=5.157 RET=10.908 AUX=10.905 | β=0.367 g=0.672 gate=0.65 | lr=4.48e-05\n",
      "[17200] LM=5.146 RET=10.911 AUX=10.893 | β=0.363 g=0.672 gate=0.64 | lr=4.32e-05\n",
      "[17300] LM=5.151 RET=10.908 AUX=10.896 | β=0.359 g=0.672 gate=0.64 | lr=4.16e-05\n",
      "[17400] LM=5.115 RET=10.904 AUX=10.883 | β=0.357 g=0.672 gate=0.62 | lr=4.01e-05\n",
      "[17500] LM=5.152 RET=10.913 AUX=10.888 | β=0.363 g=0.668 gate=0.67 | lr=3.86e-05\n",
      "[17600] LM=5.143 RET=10.915 AUX=10.905 | β=0.355 g=0.668 gate=0.63 | lr=3.71e-05\n",
      "[17700] LM=5.142 RET=10.913 AUX=10.893 | β=0.361 g=0.672 gate=0.64 | lr=3.56e-05\n",
      "[17800] LM=5.124 RET=10.913 AUX=10.887 | β=0.363 g=0.672 gate=0.65 | lr=3.41e-05\n",
      "[17900] LM=5.157 RET=10.914 AUX=10.890 | β=0.363 g=0.664 gate=0.65 | lr=3.27e-05\n",
      "[18000] LM=5.152 RET=10.904 AUX=10.874 | β=0.363 g=0.668 gate=0.66 | lr=3.13e-05\n",
      "NIAH: 0.9451x random (FAIL)\n",
      "  Avg rank: 26240/50257, Success: 35.0%\n",
      "[18100] LM=5.160 RET=10.906 AUX=10.890 | β=0.365 g=0.672 gate=0.64 | lr=3.00e-05\n",
      "[18200] LM=5.159 RET=10.899 AUX=10.918 | β=0.177 g=0.523 gate=0.63 | lr=2.87e-05\n",
      "[18300] LM=5.156 RET=10.905 AUX=10.886 | β=0.357 g=0.668 gate=0.64 | lr=2.74e-05\n",
      "[18400] LM=5.141 RET=10.891 AUX=10.902 | β=0.172 g=0.520 gate=0.64 | lr=2.61e-05\n",
      "[18500] LM=5.157 RET=10.882 AUX=10.893 | β=0.365 g=0.672 gate=0.64 | lr=2.49e-05\n",
      "[18600] LM=5.153 RET=10.883 AUX=10.901 | β=0.359 g=0.672 gate=0.63 | lr=2.37e-05\n",
      "[18700] LM=5.140 RET=10.883 AUX=10.889 | β=0.363 g=0.668 gate=0.64 | lr=2.25e-05\n",
      "[18800] LM=5.133 RET=10.887 AUX=10.870 | β=0.357 g=0.672 gate=0.63 | lr=2.14e-05\n",
      "[18900] LM=5.148 RET=10.898 AUX=10.903 | β=0.363 g=0.676 gate=0.63 | lr=2.02e-05\n",
      "[19000] LM=5.154 RET=10.902 AUX=10.885 | β=0.361 g=0.672 gate=0.65 | lr=1.92e-05\n",
      "NIAH: 1.2010x random (PASS)\n",
      "  Avg rank: 27865/50257, Success: 45.0%\n",
      "[19100] LM=5.140 RET=10.903 AUX=10.902 | β=0.363 g=0.668 gate=0.66 | lr=1.81e-05\n",
      "[19200] LM=5.136 RET=10.900 AUX=10.894 | β=0.363 g=0.664 gate=0.65 | lr=1.71e-05\n",
      "[19300] LM=5.140 RET=10.898 AUX=10.909 | β=0.365 g=0.668 gate=0.66 | lr=1.61e-05\n",
      "[19400] LM=5.152 RET=10.895 AUX=10.879 | β=0.363 g=0.668 gate=0.65 | lr=1.52e-05\n",
      "[19500] LM=5.135 RET=10.896 AUX=10.906 | β=0.367 g=0.672 gate=0.65 | lr=1.43e-05\n",
      "[19600] LM=5.141 RET=10.902 AUX=10.922 | β=0.363 g=0.672 gate=0.65 | lr=1.34e-05\n",
      "[19700] LM=5.142 RET=10.902 AUX=10.887 | β=0.367 g=0.668 gate=0.67 | lr=1.26e-05\n",
      "[19800] LM=5.144 RET=10.901 AUX=10.886 | β=0.355 g=0.672 gate=0.64 | lr=1.18e-05\n",
      "[19900] LM=5.132 RET=10.897 AUX=10.901 | β=0.365 g=0.672 gate=0.65 | lr=1.10e-05\n",
      "\n",
      "Training complete in 20.1 min\n",
      "Final LM loss: 5.143\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: Training\n",
    "# =============================================================================\n",
    "\n",
    "print(\"#\" * 60)\n",
    "print(\"# MIXED TRAINING: LM + Retrieval + Auxiliary\")\n",
    "print(\"#\" * 60)\n",
    "\n",
    "history = train_mixed(\n",
    "    model,\n",
    "    data_loader,\n",
    "    steps=20000,\n",
    "    lr=3e-4,\n",
    "    warmup_steps=2000,\n",
    "    retrieval_ratio=0.1,      # 10% pure retrieval batches\n",
    "    auxiliary_weight=0.1,     # Auxiliary loss weight on LM batches\n",
    "    log_every=100,\n",
    "    niah_every=1000,\n",
    "    device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-12-evaluation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "# POST-TRAINING EVALUATION\n",
      "############################################################\n",
      "\n",
      "1. NIAH with proper cue:\n",
      "NIAH: 1.0847x random (PASS)\n",
      "  Avg rank: 27809/50257, Success: 40.0%\n",
      "\n",
      "2. NIAH by distance:\n",
      "\n",
      "NIAH by distance:\n",
      "----------------------------------------\n",
      "NIAH: 1.2785x random (PASS)\n",
      "  Avg rank: 23196/50257, Success: 40.0%\n",
      "NIAH: 1.3466x random (PASS)\n",
      "  Avg rank: 21868/50257, Success: 60.0%\n",
      "NIAH: 1.1938x random (PASS)\n",
      "  Avg rank: 21348/50257, Success: 50.0%\n",
      "NIAH: 1.1246x random (PASS)\n",
      "  Avg rank: 25958/50257, Success: 35.0%\n",
      "NIAH: 1.3827x random (PASS)\n",
      "  Avg rank: 23393/50257, Success: 65.0%\n",
      "NIAH: 1.0713x random (PASS)\n",
      "  Avg rank: 25967/50257, Success: 50.0%\n",
      "\n",
      "3. Comparison:\n",
      "  Baseline NIAH: 1.1766x\n",
      "  Final NIAH:    1.0847x\n",
      "  Improvement:   0.92x\n",
      "\n",
      "4. GDN state analysis:\n",
      "\n",
      "============================================================\n",
      "FULL DIAGNOSTIC SUITE\n",
      "============================================================\n",
      "Sequence length: 128\n",
      "Needle position: 32\n",
      "\n",
      "--- GDN State Analysis ---\n",
      "Needle token ID: 50000 at position 32\n",
      "\n",
      "  [GDN Layer 0] ✓\n",
      "      State norm: 3.0477\n",
      "      Needle Retrieval: 0.034810\n",
      "      Random Retrieval: 0.033040\n",
      "      Signal-to-Noise:  1.0536 (GOOD)\n",
      "      β=0.150±0.113, g=0.498\n",
      "\n",
      "  Summary: avg_SNR=1.0536, max_SNR=1.0536\n",
      "  → Needle IS stored in GDN state\n",
      "\n",
      "5. Training history:\n",
      "  Final LM loss: 5.143\n",
      "  Final RET loss: 10.896\n",
      "  Final AUX loss: 10.901\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: Post-Training Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"#\" * 60)\n",
    "print(\"# POST-TRAINING EVALUATION\")\n",
    "print(\"#\" * 60)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n1. NIAH with proper cue:\")\n",
    "final_niah = proper_niah_test(model, seq_len=128, needle_pos=32, n_trials=30)\n",
    "\n",
    "print(\"\\n2. NIAH by distance:\")\n",
    "final_distances = test_niah_by_distance(model, distances=[5, 10, 20, 40, 60, 95], n_trials=20)\n",
    "\n",
    "print(\"\\n3. Comparison:\")\n",
    "print(f\"  Baseline NIAH: {baseline_niah['avg_ratio']:.4f}x\")\n",
    "print(f\"  Final NIAH:    {final_niah['avg_ratio']:.4f}x\")\n",
    "if baseline_niah['avg_ratio'] > 0:\n",
    "    print(f\"  Improvement:   {final_niah['avg_ratio'] / baseline_niah['avg_ratio']:.2f}x\")\n",
    "\n",
    "print(\"\\n4. GDN state analysis:\")\n",
    "run_full_diagnostic(model, seq_len=128, needle_pos=32)\n",
    "\n",
    "print(\"\\n5. Training history:\")\n",
    "print(f\"  Final LM loss: {sum(history['lm_loss'][-100:])/100:.3f}\")\n",
    "print(f\"  Final RET loss: {sum(history['ret_loss'][-100:])/max(1,len(history['ret_loss'][-100:])):.3f}\")\n",
    "print(f\"  Final AUX loss: {sum(history['aux_loss'][-100:])/100:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e709575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Curriculum Training...\n",
      "\n",
      "============================================================\n",
      "PHASE 1: RETRIEVAL BOOTCAMP (Force the Circuit)\n",
      "============================================================\n",
      "[Bootcamp 0] Loss: 1.0875 | β: 0.133\n",
      "[Bootcamp 100] Loss: 10.8812 | β: 0.132\n",
      "[Bootcamp 200] Loss: 10.8625 | β: 0.135\n",
      "[Bootcamp 300] Loss: 10.8562 | β: 0.137\n",
      "[Bootcamp 400] Loss: 10.8875 | β: 0.140\n",
      "[Bootcamp 500] Loss: 10.8812 | β: 0.142\n",
      "[Bootcamp 600] Loss: 10.8562 | β: 0.141\n",
      "[Bootcamp 700] Loss: 10.8438 | β: 0.141\n",
      "[Bootcamp 800] Loss: 10.6375 | β: 0.138\n",
      "[Bootcamp 900] Loss: 10.3688 | β: 0.133\n",
      "[Bootcamp 1000] Loss: 9.9688 | β: 0.122\n",
      "\n",
      "============================================================\n",
      "PHASE 2: LANGUAGE INTEGRATION (Preserve the Circuit)\n",
      "============================================================\n",
      "[Integration 0] Task: RET | Loss: 10.1250\n",
      "[Integration 200] Task: RET | Loss: 10.3125\n",
      "[Integration 400] Task: RET | Loss: 10.3750\n",
      "[Integration 600] Task: RET | Loss: 9.7500\n",
      "[Integration 800] Task: RET | Loss: 9.7500\n",
      "[Integration 1000] Task: RET | Loss: 9.8750\n",
      "[Integration 1200] Task: RET | Loss: 10.4375\n",
      "[Integration 1400] Task: RET | Loss: 9.5625\n",
      "[Integration 1600] Task: RET | Loss: 10.1875\n",
      "[Integration 1800] Task: RET | Loss: 9.9375\n",
      "\n",
      "Running Final Diagnostics...\n",
      "\n",
      "============================================================\n",
      "FULL DIAGNOSTIC SUITE\n",
      "============================================================\n",
      "Sequence length: 128\n",
      "Needle position: 32\n",
      "\n",
      "--- GDN State Analysis ---\n",
      "Needle token ID: 50000 at position 32\n",
      "\n",
      "  [GDN Layer 0] ✓\n",
      "      State norm: 1.5464\n",
      "      Needle Retrieval: 0.034080\n",
      "      Random Retrieval: 0.028425\n",
      "      Signal-to-Noise:  1.1990 (GOOD)\n",
      "      β=0.101±0.054, g=0.447\n",
      "\n",
      "  Summary: avg_SNR=1.1990, max_SNR=1.1990\n",
      "  → Needle IS stored in GDN state\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'layers': [{'layer_idx': 0,\n",
       "   'layer_type': 'GDN',\n",
       "   'state_norm': 1.5464245080947876,\n",
       "   'needle_retrieval': 0.03408018499612808,\n",
       "   'random_retrieval': 0.02842489629983902,\n",
       "   'signal_to_noise': 1.1989550514831808,\n",
       "   'beta_mean': 0.1005859375,\n",
       "   'beta_std': 0.053955078125,\n",
       "   'g_mean': 0.447265625}],\n",
       " 'summary': {'avg_snr': 1.1989550514831808,\n",
       "  'max_snr': 1.1989550514831808,\n",
       "  'needle_stored': True}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: Curriculum Training (The Fix)\n",
    "# =============================================================================\n",
    "\n",
    "def train_curriculum(model, lm_loader, device='cuda'):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PHASE 1: RETRIEVAL BOOTCAMP (Force the Circuit)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Setup for pure retrieval\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    retrieval_gen = RetrievalDataGenerator(model.cfg)\n",
    "    \n",
    "    # Trackers\n",
    "    losses = []\n",
    "    \n",
    "    # --- PHASE 1: PURE RETRIEVAL (1000 steps) ---\n",
    "    model.train()\n",
    "    for step in range(1001):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 100% Synthetic Data\n",
    "        input_ids, targets, _ = retrieval_gen.generate_batch(\n",
    "            batch_size=32,       # Larger batch for stable gradients\n",
    "            seq_len=128, \n",
    "            device=device,\n",
    "            min_distance=5,      # Start easy\n",
    "            max_distance=90      # Go hard\n",
    "        )\n",
    "        \n",
    "        logits, _, diags, _ = model(input_ids, return_diagnostics=True)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)), \n",
    "            targets.view(-1), \n",
    "            ignore_index=-100\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            avg_loss = sum(losses[-10:]) / 10\n",
    "            beta = diags[0]['beta_mean']\n",
    "            print(f\"[Bootcamp {step}] Loss: {avg_loss:.4f} | β: {beta:.3f}\")\n",
    "            \n",
    "            # Early exit if solved\n",
    "            if avg_loss < 0.5:\n",
    "                print(\">> Circuit formed! Moving to Phase 2.\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PHASE 2: LANGUAGE INTEGRATION (Preserve the Circuit)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Re-initialize optimizer for mixed phase\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) # Lower LR to protect circuit\n",
    "    lm_iter = iter(lm_loader)\n",
    "    \n",
    "    for step in range(2000): # Short fine-tuning\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 50/50 Mix\n",
    "        if step % 2 == 0:\n",
    "            # Retrieval Task (Maintain Memory)\n",
    "            input_ids, targets, _ = retrieval_gen.generate_batch(16, 128, device)\n",
    "            logits, _, _, _ = model(input_ids, return_diagnostics=True)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-100)\n",
    "            task = \"RET\"\n",
    "        else:\n",
    "            # Language Task (Learn Syntax)\n",
    "            try:\n",
    "                batch = next(lm_iter)\n",
    "            except StopIteration:\n",
    "                lm_iter = iter(lm_loader)\n",
    "                batch = next(lm_iter)\n",
    "            \n",
    "            input_ids = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "            logits, loss, _, _ = model(input_ids, targets, return_diagnostics=True)\n",
    "            task = \"LM \"\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(f\"[Integration {step}] Task: {task} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- EXECUTE ---\n",
    "print(\"Starting Curriculum Training...\")\n",
    "model = TransparentHybrid(cfg).cuda().bfloat16() # Reset model\n",
    "train_curriculum(model, data_loader)\n",
    "\n",
    "print(\"\\nRunning Final Diagnostics...\")\n",
    "run_full_diagnostic(model, seq_len=128, needle_pos=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
