{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809bc4cb",
   "metadata": {},
   "source": [
    "# Force Memory Usage\n",
    "\n",
    "Testing fixes from compass doc:\n",
    "1. **Distractor > window_size**: 80 tokens distractor, 64 window → fact outside local attention\n",
    "2. **local_dropout=0.3**: Bottleneck the SWA shortcut, force state retrieval\n",
    "\n",
    "**Changes made:**\n",
    "- `config.py`: Added `local_dropout: float = 0.3`\n",
    "- `model.py`: `local_out = self.local_dropout(local_out)` after o_proj\n",
    "- `make_example()`: distractor_tokens=80 (was 400, but 400//5=80 anyway, now explicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891e0308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x77439c594190>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer\n",
    "import random\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de828433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model - reload to pick up changes\n",
    "import importlib\n",
    "import config\n",
    "import model as model_module\n",
    "importlib.reload(config)\n",
    "importlib.reload(model_module)\n",
    "\n",
    "from config import HybridConfig\n",
    "from model import TransparentHybrid\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf0339",
   "metadata": {},
   "source": [
    "## Test Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ebe559",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = ['Alice', 'Bob', 'Carol', 'Dave', 'Eve', 'Frank', 'Grace', 'Henry']\n",
    "OBJECTS = ['ball', 'car', 'hat', 'book', 'pen', 'cup', 'ring', 'lamp']\n",
    "COLORS = ['red', 'blue', 'green', 'yellow', 'purple', 'orange', 'pink', 'white']\n",
    "\n",
    "def make_example(distractor_tokens=80):\n",
    "    \"\"\"Fact at start, distractor BEYOND window_size (64), query at end.\"\"\"\n",
    "    name = random.choice(NAMES)\n",
    "    obj = random.choice(OBJECTS)\n",
    "    color = random.choice(COLORS)\n",
    "    \n",
    "    fact = f'{name} has a {color} {obj}.'\n",
    "    distractor = ' The sky is blue.' * (distractor_tokens // 5)\n",
    "    query = f' What does {name} have?'\n",
    "    answer = f' {color}'\n",
    "    \n",
    "    return fact + distractor + query, answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309331b",
   "metadata": {},
   "source": [
    "## State Ablation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90620c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_state_ablation(model, cfg, n_samples=100):\n",
    "    \"\"\"Test if zeroing state hurts accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    correct_normal = 0\n",
    "    correct_zeroed = 0\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        text, answer = make_example()\n",
    "        tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=600)['input_ids'].to(DEVICE)\n",
    "        answer_id = tokenizer.encode(answer)[0]\n",
    "        \n",
    "        # Normal\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            logits, _, _, _ = model(tokens)\n",
    "        if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "            correct_normal += 1\n",
    "        \n",
    "        # Zeroed state\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            x = model.embed(tokens)\n",
    "            x = model.embed_norm(x)\n",
    "            state = None\n",
    "            for i, (layer, ffn) in enumerate(zip(model.layers, model.ffns)):\n",
    "                if cfg.layer_pattern[i] == 'G':\n",
    "                    x, state, _ = layer(x, initial_state=state, input_ids=tokens)\n",
    "                    key_bank = layer.key_bank\n",
    "                else:\n",
    "                    x, _ = layer(x, gdn_state=torch.zeros_like(state), input_ids=tokens, key_bank=key_bank)\n",
    "                x = ffn(x)\n",
    "            logits_z = model.lm_head(model.norm_f(x))\n",
    "        if answer_id in logits_z[0, -1].topk(5).indices.tolist():\n",
    "            correct_zeroed += 1\n",
    "    \n",
    "    print(f'Normal: {correct_normal}/{n_samples} ({correct_normal/n_samples:.0%})')\n",
    "    print(f'Zeroed: {correct_zeroed}/{n_samples} ({correct_zeroed/n_samples:.0%})')\n",
    "    print(f'Delta: {correct_normal - correct_zeroed}')\n",
    "    return correct_normal, correct_zeroed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1dc324",
   "metadata": {},
   "source": [
    "## Baseline (current broken model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eedb0ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline...\n",
      "Step 0: loss=10.855\n",
      "Step 500: loss=0.039\n",
      "Step 1000: loss=0.072\n",
      "Step 1500: loss=0.031\n",
      "Step 2000: loss=0.017\n",
      "Step 2500: loss=0.019\n",
      "\n",
      "Baseline ablation:\n",
      "Normal: 63/100 (63%)\n",
      "Zeroed: 65/100 (65%)\n",
      "Delta: -2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(63, 65)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = HybridConfig(\n",
    "    d_model=256, n_heads=4, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GS', vocab_size=50257,\n",
    "    window_size=64, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=0.1\n",
    ")\n",
    "model = TransparentHybrid(cfg).to(DEVICE)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "print('Training baseline...')\n",
    "for step in range(3000):\n",
    "    text, answer = make_example()\n",
    "    tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=600)['input_ids'].to(DEVICE)\n",
    "    targets = tokens[:, 1:].contiguous()\n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model(tokens)\n",
    "        loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if step % 500 == 0:\n",
    "        print(f'Step {step}: loss={loss.item():.3f}')\n",
    "\n",
    "print('\\nBaseline ablation:')\n",
    "test_state_ablation(model, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa8bb0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with local_scale=0.0 (no local attention at all)...\n",
      "Step 0: loss=10.908\n",
      "Step 500: loss=0.064\n",
      "Step 1000: loss=0.028\n",
      "Step 1500: loss=0.028\n",
      "Step 2000: loss=0.032\n",
      "Step 2500: loss=0.116\n",
      "\n",
      "With local_scale=0.0:\n",
      "Normal: 59/100 (59%)\n",
      "Zeroed: 65/100 (65%)\n",
      "Delta: -6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(59, 65)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG: Where is information flowing?\n",
    "# Test: What if we remove the state ENTIRELY and use local_scale=0?\n",
    "# If model still works, info flows through residual x\n",
    "\n",
    "# Let's trace a single forward pass\n",
    "cfg_debug = HybridConfig(\n",
    "    d_model=256, n_heads=4, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GS', vocab_size=50257,\n",
    "    window_size=64, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=0.1,\n",
    "    local_scale=0.0  # ZERO out local attention entirely\n",
    ")\n",
    "model_debug = TransparentHybrid(cfg_debug).to(DEVICE)\n",
    "opt_debug = torch.optim.AdamW(model_debug.parameters(), lr=1e-3)\n",
    "\n",
    "print('Training with local_scale=0.0 (no local attention at all)...')\n",
    "for step in range(3000):\n",
    "    text, answer = make_example()\n",
    "    tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=600)['input_ids'].to(DEVICE)\n",
    "    targets = tokens[:, 1:].contiguous()\n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_debug(tokens)\n",
    "        loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "    opt_debug.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_debug.step()\n",
    "    if step % 500 == 0:\n",
    "        print(f'Step {step}: loss={loss.item():.3f}')\n",
    "\n",
    "print('\\nWith local_scale=0.0:')\n",
    "test_state_ablation(model_debug, cfg_debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89eec1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on HARD task (400+ token distractor, paraphrased query)...\n",
      "Step 0: loss=10.909\n",
      "Step 500: loss=0.028\n",
      "Step 1000: loss=0.043\n",
      "Step 1500: loss=0.017\n",
      "Step 2000: loss=0.018\n",
      "Step 2500: loss=0.018\n",
      "\n",
      "Hard task ablation:\n",
      "Normal: 64/100 (64%)\n",
      "Zeroed: 66/100 (66%)\n",
      "Delta: -2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 66)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REAL TEST: distractor = 400 tokens (way beyond window=64)\n",
    "# Different query phrasing to prevent pattern matching\n",
    "\n",
    "def make_hard_example():\n",
    "    \"\"\"Fact at start, LONG distractor, paraphrased query at end.\"\"\"\n",
    "    name = random.choice(NAMES)\n",
    "    obj = random.choice(OBJECTS)\n",
    "    color = random.choice(COLORS)\n",
    "    \n",
    "    fact = f'{name} has a {color} {obj}.'\n",
    "    # 400 tokens of distractor (well beyond 64 window)\n",
    "    distractor = ' '.join(['The weather is nice today.' for _ in range(60)])\n",
    "    # Different query phrasing\n",
    "    query = f\" Tell me the color of {name}'s item.\"\n",
    "    answer = f' {color}'\n",
    "    \n",
    "    return fact + distractor + query, answer\n",
    "\n",
    "# Test with BRUTAL settings\n",
    "cfg_hard = HybridConfig(\n",
    "    d_model=256, n_heads=4, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GS', vocab_size=50257,\n",
    "    window_size=64, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=0.1,\n",
    "    local_scale=0.3\n",
    ")\n",
    "model_hard = TransparentHybrid(cfg_hard).to(DEVICE)\n",
    "opt_hard = torch.optim.AdamW(model_hard.parameters(), lr=1e-3)\n",
    "\n",
    "print('Training on HARD task (400+ token distractor, paraphrased query)...')\n",
    "for step in range(3000):\n",
    "    text, answer = make_hard_example()\n",
    "    tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=700)['input_ids'].to(DEVICE)\n",
    "    targets = tokens[:, 1:].contiguous()\n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_hard(tokens)\n",
    "        loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "    opt_hard.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_hard.step()\n",
    "    if step % 500 == 0:\n",
    "        print(f'Step {step}: loss={loss.item():.3f}')\n",
    "\n",
    "# Test with make_hard_example\n",
    "def test_state_ablation_hard(model, cfg, n_samples=100):\n",
    "    \"\"\"Test if zeroing state hurts accuracy on hard examples.\"\"\"\n",
    "    model.eval()\n",
    "    correct_normal = 0\n",
    "    correct_zeroed = 0\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        text, answer = make_hard_example()\n",
    "        tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=700)['input_ids'].to(DEVICE)\n",
    "        answer_id = tokenizer.encode(answer)[0]\n",
    "        \n",
    "        # Normal\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            logits, _, _, _ = model(tokens)\n",
    "        if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "            correct_normal += 1\n",
    "        \n",
    "        # Zeroed state\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            x = model.embed(tokens)\n",
    "            x = model.embed_norm(x)\n",
    "            state = None\n",
    "            for i, (layer, ffn) in enumerate(zip(model.layers, model.ffns)):\n",
    "                if cfg.layer_pattern[i] == 'G':\n",
    "                    x, state, _ = layer(x, initial_state=state, input_ids=tokens)\n",
    "                    key_bank = layer.key_bank\n",
    "                else:\n",
    "                    x, _ = layer(x, gdn_state=torch.zeros_like(state), input_ids=tokens, key_bank=key_bank)\n",
    "                x = ffn(x)\n",
    "            logits_z = model.lm_head(model.norm_f(x))\n",
    "        if answer_id in logits_z[0, -1].topk(5).indices.tolist():\n",
    "            correct_zeroed += 1\n",
    "    \n",
    "    print(f'Normal: {correct_normal}/{n_samples} ({correct_normal/n_samples:.0%})')\n",
    "    print(f'Zeroed: {correct_zeroed}/{n_samples} ({correct_zeroed/n_samples:.0%})')\n",
    "    print(f'Delta: {correct_normal - correct_zeroed}')\n",
    "    return correct_normal, correct_zeroed\n",
    "\n",
    "print('\\nHard task ablation:')\n",
    "test_state_ablation_hard(model_hard, cfg_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc75e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SANITY CHECKS ===\n",
      "✗ Answer: pink     | Pred:  green   | Top5: [' green', ' white', ' red', ' orange', ' purple']\n",
      "✓ Answer: white    | Pred:  green   | Top5: [' green', ' white', ' red', ' orange', ' purple']\n",
      "✗ Answer: blue     | Pred:  pink    | Top5: [' pink', ' green', ' yellow', ' purple', ' white']\n",
      "✗ Answer: orange   | Pred:  green   | Top5: [' green', ' white', 'The', ' red', ' purple']\n",
      "✓ Answer: red      | Pred:  white   | Top5: [' white', ' red', ' orange', ' green', ' yellow']\n",
      "\n",
      "Accuracy (top-5): 12/20 = 60%\n",
      "\n",
      "Random baseline: 1/8 = 12.5%\n",
      "\n",
      "Most common predictions: [('green', 57), ('white', 30), ('pink', 13)]\n"
     ]
    }
   ],
   "source": [
    "# SANITY CHECK: Is the model learning ANYTHING meaningful?\n",
    "# Test: shuffle answers and see if accuracy drops to ~12.5% (1/8 colors)\n",
    "\n",
    "print(\"=== SANITY CHECKS ===\")\n",
    "model_hard.eval()\n",
    "\n",
    "# 1. Normal accuracy with verbose output\n",
    "correct = 0\n",
    "for i in range(20):\n",
    "    text, answer = make_hard_example()\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=700)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_hard(tokens)\n",
    "    \n",
    "    pred_token = logits[0, -1].argmax().item()\n",
    "    pred_text = tokenizer.decode([pred_token])\n",
    "    top5 = [tokenizer.decode([t]) for t in logits[0, -1].topk(5).indices.tolist()]\n",
    "    \n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct += 1\n",
    "        status = \"✓\"\n",
    "    else:\n",
    "        status = \"✗\"\n",
    "    \n",
    "    if i < 5:  # Show first 5\n",
    "        print(f\"{status} Answer: {answer.strip():8s} | Pred: {pred_text:8s} | Top5: {top5}\")\n",
    "\n",
    "print(f\"\\nAccuracy (top-5): {correct}/20 = {correct/20:.0%}\")\n",
    "\n",
    "# 2. Random baseline: what's the chance of guessing?\n",
    "print(f\"\\nRandom baseline: 1/{len(COLORS)} = {1/len(COLORS):.1%}\")\n",
    "\n",
    "# 3. Most common prediction\n",
    "from collections import Counter\n",
    "preds = []\n",
    "for _ in range(100):\n",
    "    text, answer = make_hard_example()\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=700)['input_ids'].to(DEVICE)\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_hard(tokens)\n",
    "    preds.append(tokenizer.decode([logits[0, -1].argmax().item()]).strip())\n",
    "\n",
    "print(f\"\\nMost common predictions: {Counter(preds).most_common(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97395fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with ANSWER-ONLY loss (ignoring distractors)...\n",
      "Step 0: loss=11.250\n",
      "Step 500: loss=0.004\n",
      "Step 1000: loss=0.002\n",
      "Step 1500: loss=0.001\n",
      "Step 2000: loss=0.000\n",
      "Step 2500: loss=0.000\n",
      "\n",
      "Answer-only trained model:\n",
      "Normal: 59/100 (59%)\n",
      "Zeroed: 60/100 (60%)\n",
      "Delta: -1\n",
      "\n",
      "Top-5 accuracy: 68%\n",
      "Prediction distribution: [('white', 100)]\n"
     ]
    }
   ],
   "source": [
    "# FIX: Train with ANSWER-ONLY loss\n",
    "# Only compute loss on the answer token, ignore distractors\n",
    "\n",
    "cfg_answer_only = HybridConfig(\n",
    "    d_model=256, n_heads=4, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GS', vocab_size=50257,\n",
    "    window_size=64, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=0.1,\n",
    "    local_scale=0.3\n",
    ")\n",
    "model_answer_only = TransparentHybrid(cfg_answer_only).to(DEVICE)\n",
    "opt_answer_only = torch.optim.AdamW(model_answer_only.parameters(), lr=1e-3)\n",
    "\n",
    "print('Training with ANSWER-ONLY loss (ignoring distractors)...')\n",
    "for step in range(3000):\n",
    "    text, answer = make_hard_example()\n",
    "    \n",
    "    # Tokenize separately to know where answer starts\n",
    "    text_tokens = tokenizer(text, return_tensors='pt')['input_ids']\n",
    "    answer_tokens = tokenizer(answer, return_tensors='pt')['input_ids']\n",
    "    full_tokens = torch.cat([text_tokens, answer_tokens], dim=1)[:, :700].to(DEVICE)\n",
    "    \n",
    "    answer_start = text_tokens.size(1)\n",
    "    \n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_answer_only(full_tokens)\n",
    "        \n",
    "        # Only compute loss on answer position\n",
    "        # logits[0, answer_start-1] predicts answer_tokens[0, 0]\n",
    "        loss = F.cross_entropy(\n",
    "            logits[0, answer_start-1:answer_start].reshape(-1, 50257),\n",
    "            answer_tokens[0, :1].reshape(-1).to(DEVICE)\n",
    "        )\n",
    "    \n",
    "    opt_answer_only.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_answer_only.step()\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        print(f'Step {step}: loss={loss.item():.3f}')\n",
    "\n",
    "print('\\nAnswer-only trained model:')\n",
    "test_state_ablation_hard(model_answer_only, cfg_answer_only)\n",
    "\n",
    "# Check predictions\n",
    "model_answer_only.eval()\n",
    "correct = 0\n",
    "preds = []\n",
    "for _ in range(100):\n",
    "    text, answer = make_hard_example()\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=700)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_answer_only(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct += 1\n",
    "    preds.append(tokenizer.decode([logits[0, -1].argmax().item()]).strip())\n",
    "\n",
    "print(f\"\\nTop-5 accuracy: {correct}%\")\n",
    "print(f\"Prediction distribution: {Counter(preds).most_common(8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf979e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with COMBINED loss (LM + answer + state recon)...\n",
      "Step 0: lm=10.958, ans=11.062, recon=2.094\n",
      "Step 500: lm=0.028, ans=0.007, recon=0.000\n",
      "Step 1000: lm=0.028, ans=0.002, recon=0.000\n",
      "Step 1500: lm=0.024, ans=0.001, recon=0.000\n",
      "Step 2000: lm=0.023, ans=0.001, recon=0.000\n",
      "Step 2500: lm=0.023, ans=0.000, recon=0.000\n",
      "Step 3000: lm=0.024, ans=0.000, recon=0.000\n",
      "Step 3500: lm=0.021, ans=0.000, recon=0.000\n",
      "Step 4000: lm=0.023, ans=0.000, recon=0.000\n",
      "Step 4500: lm=0.026, ans=0.000, recon=0.000\n",
      "\n",
      "Combined training model:\n",
      "Normal: 65/100 (65%)\n",
      "Zeroed: 68/100 (68%)\n",
      "Delta: -3\n",
      "\n",
      "State reconstruction accuracy: 10%\n",
      "(This shows if state actually encodes the fact)\n"
     ]
    }
   ],
   "source": [
    "# PROPER FIX: Combined LM loss + Retrieval loss + State reconstruction loss\n",
    "# The model must learn language modeling AND retrieval AND prove state stores info\n",
    "\n",
    "cfg_combined = HybridConfig(\n",
    "    d_model=256, n_heads=4, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GS', vocab_size=50257,\n",
    "    window_size=64, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=0.1,\n",
    "    local_scale=0.3\n",
    ")\n",
    "model_combined = TransparentHybrid(cfg_combined).to(DEVICE)\n",
    "\n",
    "# State reconstruction head: given final state, predict the color token\n",
    "# This forces the state to actually encode the fact\n",
    "state_dim = cfg_combined.n_heads * cfg_combined.head_dim * cfg_combined.value_dim\n",
    "recon_head = nn.Sequential(\n",
    "    nn.Linear(state_dim, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, len(COLORS))  # 8 classes\n",
    ").to(DEVICE)\n",
    "\n",
    "# Color to index mapping\n",
    "color_to_idx = {c: i for i, c in enumerate(COLORS)}\n",
    "\n",
    "opt_combined = torch.optim.AdamW(\n",
    "    list(model_combined.parameters()) + list(recon_head.parameters()), \n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "print('Training with COMBINED loss (LM + answer + state recon)...')\n",
    "for step in range(5000):\n",
    "    text, answer = make_hard_example()\n",
    "    color = answer.strip()\n",
    "    color_idx = color_to_idx[color]\n",
    "    \n",
    "    text_tokens = tokenizer(text, return_tensors='pt')['input_ids']\n",
    "    answer_tokens = tokenizer(answer, return_tensors='pt')['input_ids']\n",
    "    full_tokens = torch.cat([text_tokens, answer_tokens], dim=1)[:, :700].to(DEVICE)\n",
    "    answer_start = text_tokens.size(1)\n",
    "    \n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, state = model_combined(full_tokens)\n",
    "        \n",
    "        # 1. LM loss on all tokens (keeps model grounded)\n",
    "        targets = full_tokens[:, 1:].contiguous()\n",
    "        lm_loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "        \n",
    "        # 2. Answer loss (weighted higher)\n",
    "        answer_loss = F.cross_entropy(\n",
    "            logits[0, answer_start-1:answer_start].reshape(-1, 50257),\n",
    "            answer_tokens[0, :1].reshape(-1).to(DEVICE)\n",
    "        )\n",
    "        \n",
    "        # 3. State reconstruction loss: state must encode the color\n",
    "        state_flat = state.reshape(1, -1).float()\n",
    "        recon_logits = recon_head(state_flat)\n",
    "        recon_loss = F.cross_entropy(\n",
    "            recon_logits, \n",
    "            torch.tensor([color_idx], device=DEVICE)\n",
    "        )\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = lm_loss + 2.0 * answer_loss + 1.0 * recon_loss\n",
    "    \n",
    "    opt_combined.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_combined.step()\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        print(f'Step {step}: lm={lm_loss.item():.3f}, ans={answer_loss.item():.3f}, recon={recon_loss.item():.3f}')\n",
    "\n",
    "print('\\nCombined training model:')\n",
    "test_state_ablation_hard(model_combined, cfg_combined)\n",
    "\n",
    "# Check if state actually encodes color\n",
    "model_combined.eval()\n",
    "state_correct = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_hard_example()\n",
    "    color = answer.strip()\n",
    "    color_idx = color_to_idx[color]\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=700)['input_ids'].to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        _, _, _, state = model_combined(tokens)\n",
    "    state_flat = state.reshape(1, -1).float()\n",
    "    recon_pred = recon_head(state_flat).argmax().item()\n",
    "    if recon_pred == color_idx:\n",
    "        state_correct += 1\n",
    "\n",
    "print(f\"\\nState reconstruction accuracy: {state_correct}%\")\n",
    "print(\"(This shows if state actually encodes the fact)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b216dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with reconstruction loss on PROMPT state (not prompt+answer)...\n",
      "Step 0: lm=10.923, ans=10.312, recon=2.047\n",
      "Step 500: lm=0.030, ans=0.006, recon=2.062\n",
      "Step 1000: lm=0.028, ans=0.002, recon=2.156\n",
      "Step 1500: lm=0.028, ans=0.001, recon=2.094\n",
      "Step 2000: lm=0.342, ans=0.001, recon=2.094\n",
      "Step 2500: lm=0.026, ans=0.000, recon=2.078\n",
      "Step 3000: lm=0.026, ans=0.000, recon=2.125\n",
      "Step 3500: lm=0.024, ans=0.000, recon=2.062\n",
      "Step 4000: lm=0.026, ans=0.000, recon=2.094\n",
      "Step 4500: lm=0.025, ans=0.000, recon=2.078\n",
      "\n",
      "V2 model (prompt-state reconstruction):\n",
      "Normal: 60/100 (60%)\n",
      "Zeroed: 60/100 (60%)\n",
      "Delta: 0\n",
      "\n",
      "State reconstruction accuracy: 9%\n"
     ]
    }
   ],
   "source": [
    "# ACTUALLY CORRECT: Reconstruct fact from state AT QUERY TIME\n",
    "# The reconstruction loss should use state before seeing the answer\n",
    "\n",
    "cfg_v2 = HybridConfig(\n",
    "    d_model=256, n_heads=4, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GS', vocab_size=50257,\n",
    "    window_size=64, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=0.1,\n",
    "    local_scale=0.3\n",
    ")\n",
    "model_v2 = TransparentHybrid(cfg_v2).to(DEVICE)\n",
    "\n",
    "# State reconstruction head\n",
    "state_dim = cfg_v2.n_heads * cfg_v2.head_dim * cfg_v2.value_dim\n",
    "recon_head_v2 = nn.Sequential(\n",
    "    nn.Linear(state_dim, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, len(COLORS))\n",
    ").to(DEVICE)\n",
    "\n",
    "opt_v2 = torch.optim.AdamW(\n",
    "    list(model_v2.parameters()) + list(recon_head_v2.parameters()), \n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "print('Training with reconstruction loss on PROMPT state (not prompt+answer)...')\n",
    "for step in range(5000):\n",
    "    text, answer = make_hard_example()\n",
    "    color = answer.strip()\n",
    "    color_idx = color_to_idx[color]\n",
    "    \n",
    "    # Process PROMPT ONLY for reconstruction loss\n",
    "    prompt_tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=700)['input_ids'].to(DEVICE)\n",
    "    \n",
    "    # Process FULL sequence for LM loss\n",
    "    full_tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=700)['input_ids'].to(DEVICE)\n",
    "    answer_start = prompt_tokens.size(1)\n",
    "    \n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        # Get state from prompt only\n",
    "        _, _, _, prompt_state = model_v2(prompt_tokens)\n",
    "        \n",
    "        # Get logits from full sequence for LM loss\n",
    "        logits, _, _, _ = model_v2(full_tokens)\n",
    "        \n",
    "        # 1. LM loss\n",
    "        targets = full_tokens[:, 1:].contiguous()\n",
    "        lm_loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "        \n",
    "        # 2. Answer loss\n",
    "        answer_loss = F.cross_entropy(\n",
    "            logits[0, answer_start-1:answer_start].reshape(-1, 50257),\n",
    "            tokenizer(answer, return_tensors='pt')['input_ids'][0, :1].to(DEVICE)\n",
    "        )\n",
    "        \n",
    "        # 3. State reconstruction from PROMPT state\n",
    "        state_flat = prompt_state.reshape(1, -1).float()\n",
    "        recon_logits = recon_head_v2(state_flat)\n",
    "        recon_loss = F.cross_entropy(\n",
    "            recon_logits, \n",
    "            torch.tensor([color_idx], device=DEVICE)\n",
    "        )\n",
    "        \n",
    "        loss = lm_loss + 2.0 * answer_loss + 2.0 * recon_loss\n",
    "    \n",
    "    opt_v2.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_v2.step()\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        print(f'Step {step}: lm={lm_loss.item():.3f}, ans={answer_loss.item():.3f}, recon={recon_loss.item():.3f}')\n",
    "\n",
    "print('\\nV2 model (prompt-state reconstruction):')\n",
    "test_state_ablation_hard(model_v2, cfg_v2)\n",
    "\n",
    "# Check state reconstruction\n",
    "model_v2.eval()\n",
    "state_correct = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_hard_example()\n",
    "    color = answer.strip()\n",
    "    color_idx = color_to_idx[color]\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=700)['input_ids'].to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        _, _, _, state = model_v2(tokens)\n",
    "    state_flat = state.reshape(1, -1).float()\n",
    "    recon_pred = recon_head_v2(state_flat).argmax().item()\n",
    "    if recon_pred == color_idx:\n",
    "        state_correct += 1\n",
    "\n",
    "print(f\"\\nState reconstruction accuracy: {state_correct}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7053490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TINY model on minimal retrieval task...\n",
      "Step 0: loss=10.887\n",
      "Step 400: loss=0.040\n",
      "Step 800: loss=0.007\n",
      "Step 1200: loss=0.002\n",
      "Step 1600: loss=0.001\n",
      "\n",
      "Accuracy: 100%\n",
      "Sample predictions by expected:\n",
      "  Expected \"red\": got [('red', 15)]\n",
      "  Expected \"blue\": got [('blue', 17)]\n",
      "  Expected \"yellow\": got [('yellow', 9)]\n",
      "  Expected \"white\": got [('white', 15)]\n"
     ]
    }
   ],
   "source": [
    "# SIMPLEST TEST: Can we even train a tiny model to store and retrieve 1 fact?\n",
    "# Forget distractors - just: store fact, query, retrieve\n",
    "\n",
    "# Minimal test: \"red\" -> state -> query -> \"red\"\n",
    "cfg_tiny = HybridConfig(\n",
    "    d_model=128, n_heads=2, head_dim=64, value_dim=64,\n",
    "    layer_pattern='G',  # Just GDN, no SWA\n",
    "    vocab_size=50257,\n",
    "    window_size=32, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=1.0  # Always write\n",
    ")\n",
    "model_tiny = TransparentHybrid(cfg_tiny).to(DEVICE)\n",
    "\n",
    "# Simple task: \"Alice has red. What color?\" -> \"red\"\n",
    "def make_minimal_example():\n",
    "    color = random.choice(COLORS)\n",
    "    text = f\"{color}. What color?\"\n",
    "    answer = f\" {color}\"\n",
    "    return text, answer\n",
    "\n",
    "opt_tiny = torch.optim.AdamW(model_tiny.parameters(), lr=1e-3)\n",
    "\n",
    "print('Training TINY model on minimal retrieval task...')\n",
    "for step in range(2000):\n",
    "    text, answer = make_minimal_example()\n",
    "    tokens = tokenizer(text + answer, return_tensors='pt')['input_ids'].to(DEVICE)\n",
    "    targets = tokens[:, 1:].contiguous()\n",
    "    \n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_tiny(tokens)\n",
    "        loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "    \n",
    "    opt_tiny.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_tiny.step()\n",
    "    \n",
    "    if step % 400 == 0:\n",
    "        print(f'Step {step}: loss={loss.item():.3f}')\n",
    "\n",
    "# Test\n",
    "model_tiny.eval()\n",
    "correct = 0\n",
    "preds_dict = {}\n",
    "for _ in range(100):\n",
    "    text, answer = make_minimal_example()\n",
    "    tokens = tokenizer(text, return_tensors='pt')['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_tiny(tokens)\n",
    "    \n",
    "    pred = tokenizer.decode([logits[0, -1].argmax().item()]).strip()\n",
    "    expected = answer.strip()\n",
    "    \n",
    "    if expected not in preds_dict:\n",
    "        preds_dict[expected] = []\n",
    "    preds_dict[expected].append(pred)\n",
    "    \n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct += 1\n",
    "\n",
    "print(f'\\nAccuracy: {correct}%')\n",
    "print('Sample predictions by expected:')\n",
    "for exp, preds in list(preds_dict.items())[:4]:\n",
    "    print(f'  Expected \"{exp}\": got {Counter(preds).most_common(3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "916f2795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curriculum training...\n",
      "Step 500: dist=0, loss=0.015\n",
      "Step 1000: dist=10, loss=0.003\n",
      "Step 1500: dist=30, loss=0.041\n",
      "Step 2000: dist=50, loss=0.083\n",
      "Step 2500: dist=100, loss=0.085\n",
      "\n",
      "Testing at each distractor level:\n",
      "    0 distractors: 50/50 = 100%\n",
      "   10 distractors: 50/50 = 100%\n",
      "   30 distractors: 50/50 = 100%\n",
      "   50 distractors: 50/50 = 100%\n",
      "  100 distractors: 50/50 = 100%\n"
     ]
    }
   ],
   "source": [
    "# CURRICULUM: Gradually increase distractor length\n",
    "# Start with 0 distractors, increase to 50, 100, 200\n",
    "\n",
    "def make_curriculum_example(n_distractor_tokens):\n",
    "    \"\"\"Fact, distractor, query.\"\"\"\n",
    "    color = random.choice(COLORS)\n",
    "    fact = f\"{color}.\"\n",
    "    distractor = \" word\" * n_distractor_tokens\n",
    "    query = \" What color?\"\n",
    "    answer = f\" {color}\"\n",
    "    return fact + distractor + query, answer\n",
    "\n",
    "# Fresh model\n",
    "cfg_curr = HybridConfig(\n",
    "    d_model=128, n_heads=2, head_dim=64, value_dim=64,\n",
    "    layer_pattern='G',\n",
    "    vocab_size=50257,\n",
    "    window_size=32, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=1.0\n",
    ")\n",
    "model_curr = TransparentHybrid(cfg_curr).to(DEVICE)\n",
    "opt_curr = torch.optim.AdamW(model_curr.parameters(), lr=1e-3)\n",
    "\n",
    "curriculum = [\n",
    "    (0, 500),    # 0 distractors for 500 steps\n",
    "    (10, 500),   # 10 distractors\n",
    "    (30, 500),   # 30 distractors\n",
    "    (50, 500),   # 50 distractors\n",
    "    (100, 500),  # 100 distractors\n",
    "]\n",
    "\n",
    "print('Curriculum training...')\n",
    "step = 0\n",
    "for n_dist, n_steps in curriculum:\n",
    "    for _ in range(n_steps):\n",
    "        text, answer = make_curriculum_example(n_dist)\n",
    "        tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "        targets = tokens[:, 1:].contiguous()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            logits, _, _, _ = model_curr(tokens)\n",
    "            loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "        \n",
    "        opt_curr.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_curr.step()\n",
    "        step += 1\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(f'Step {step}: dist={n_dist}, loss={loss.item():.3f}')\n",
    "\n",
    "# Test at each distractor level\n",
    "print('\\nTesting at each distractor level:')\n",
    "model_curr.eval()\n",
    "for test_dist in [0, 10, 30, 50, 100]:\n",
    "    correct = 0\n",
    "    for _ in range(50):\n",
    "        text, answer = make_curriculum_example(test_dist)\n",
    "        tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "        answer_id = tokenizer.encode(answer)[0]\n",
    "        \n",
    "        with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            logits, _, _, _ = model_curr(tokens)\n",
    "        \n",
    "        if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "            correct += 1\n",
    "    \n",
    "    print(f'  {test_dist:3d} distractors: {correct}/50 = {correct*2}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e60dbd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ablation on curriculum-trained model:\n",
      "Testing at 100 distractors (well beyond any local window):\n",
      "\n",
      "Normal:  100/100 = 100%\n",
      "Zeroed:  57/100 = 57%\n",
      "Delta:   43\n",
      "\n",
      "STATE MATTERS!\n"
     ]
    }
   ],
   "source": [
    "# STATE ABLATION on curriculum-trained GDN model\n",
    "print('State ablation on curriculum-trained model:')\n",
    "print('Testing at 100 distractors (well beyond any local window):\\n')\n",
    "\n",
    "correct_normal = 0\n",
    "correct_zeroed = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    \n",
    "    # Normal\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_curr(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct_normal += 1\n",
    "    \n",
    "    # Zeroed state - manually run forward with state zeroed\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        x = model_curr.embed(tokens)\n",
    "        x = model_curr.embed_norm(x)\n",
    "        state = torch.zeros(1, cfg_curr.n_heads, cfg_curr.head_dim, cfg_curr.value_dim, \n",
    "                           device=DEVICE, dtype=x.dtype)\n",
    "        \n",
    "        for i, (layer, ffn) in enumerate(zip(model_curr.layers, model_curr.ffns)):\n",
    "            lt = cfg_curr.layer_pattern[i]\n",
    "            if lt == 'G':\n",
    "                # Run GDN but KEEP STATE ZEROED (don't update)\n",
    "                x_norm = layer.norm(x)\n",
    "                k = layer.k_proj(x_norm).view(1, -1, cfg_curr.n_heads, cfg_curr.head_dim)\n",
    "                v = layer.v_proj(x_norm).view(1, -1, cfg_curr.n_heads, cfg_curr.value_dim)\n",
    "                \n",
    "                # Output as if state is zero: S @ k = 0\n",
    "                out = torch.zeros_like(v)\n",
    "                \n",
    "                output = out.reshape(1, -1, cfg_curr.n_heads * cfg_curr.value_dim)\n",
    "                x = x + layer.o_proj(output)\n",
    "            x = ffn(x)\n",
    "        \n",
    "        logits_z = model_curr.lm_head(model_curr.norm_f(x))\n",
    "    \n",
    "    if answer_id in logits_z[0, -1].topk(5).indices.tolist():\n",
    "        correct_zeroed += 1\n",
    "\n",
    "print(f'Normal:  {correct_normal}/100 = {correct_normal}%')\n",
    "print(f'Zeroed:  {correct_zeroed}/100 = {correct_zeroed}%')\n",
    "print(f'Delta:   {correct_normal - correct_zeroed}')\n",
    "print(f'\\n{\"STATE MATTERS!\" if correct_normal - correct_zeroed > 20 else \"STATE IS STILL DECORATIVE\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2938fa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid curriculum training (starting HARD)...\n",
      "Step 500: dist=100, loss=0.056\n",
      "Step 1000: dist=100, loss=0.046\n",
      "Step 1500: dist=50, loss=0.027\n",
      "Step 2000: dist=30, loss=0.014\n",
      "Step 2500: dist=10, loss=0.001\n",
      "\n",
      "Hybrid model accuracy at 100 distractors:\n",
      "Accuracy: 29%\n",
      "\n",
      "State ablation:\n",
      "Normal: 35%\n",
      "Zeroed: 65%\n",
      "Delta: -30\n"
     ]
    }
   ],
   "source": [
    "# HYBRID with curriculum + aggressive local_scale (bottleneck)\n",
    "cfg_hybrid = HybridConfig(\n",
    "    d_model=128, n_heads=2, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GS',  # GDN + SWA\n",
    "    vocab_size=50257,\n",
    "    window_size=32, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=1.0,\n",
    "    local_scale=0.1  # Aggressive bottleneck on SWA local path\n",
    ")\n",
    "model_hybrid = TransparentHybrid(cfg_hybrid).to(DEVICE)\n",
    "opt_hybrid = torch.optim.AdamW(model_hybrid.parameters(), lr=1e-3)\n",
    "\n",
    "curriculum = [\n",
    "    (100, 1000),  # Start with 100 distractors (forces memory use)\n",
    "    (50, 500),\n",
    "    (30, 500),\n",
    "    (10, 500),\n",
    "]\n",
    "\n",
    "print('Hybrid curriculum training (starting HARD)...')\n",
    "step = 0\n",
    "for n_dist, n_steps in curriculum:\n",
    "    for _ in range(n_steps):\n",
    "        text, answer = make_curriculum_example(n_dist)\n",
    "        tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "        targets = tokens[:, 1:].contiguous()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            logits, _, _, _ = model_hybrid(tokens)\n",
    "            loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "        \n",
    "        opt_hybrid.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_hybrid.step()\n",
    "        step += 1\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(f'Step {step}: dist={n_dist}, loss={loss.item():.3f}')\n",
    "\n",
    "# Test hybrid\n",
    "print('\\nHybrid model accuracy at 100 distractors:')\n",
    "model_hybrid.eval()\n",
    "correct = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_hybrid(tokens)\n",
    "    \n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct += 1\n",
    "\n",
    "print(f'Accuracy: {correct}%')\n",
    "\n",
    "# State ablation\n",
    "print('\\nState ablation:')\n",
    "correct_normal = 0\n",
    "correct_zeroed = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    \n",
    "    # Normal\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_hybrid(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct_normal += 1\n",
    "    \n",
    "    # Zeroed - run manually\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        x = model_hybrid.embed(tokens)\n",
    "        x = model_hybrid.embed_norm(x)\n",
    "        state = None\n",
    "        for i, (layer, ffn) in enumerate(zip(model_hybrid.layers, model_hybrid.ffns)):\n",
    "            if cfg_hybrid.layer_pattern[i] == 'G':\n",
    "                x, state, _ = layer(x, initial_state=state, input_ids=tokens)\n",
    "                key_bank = layer.key_bank\n",
    "            else:\n",
    "                # Zero the state for SWA\n",
    "                x, _ = layer(x, gdn_state=torch.zeros_like(state), input_ids=tokens, key_bank=key_bank)\n",
    "            x = ffn(x)\n",
    "        logits_z = model_hybrid.lm_head(model_hybrid.norm_f(x))\n",
    "    \n",
    "    if answer_id in logits_z[0, -1].topk(5).indices.tolist():\n",
    "        correct_zeroed += 1\n",
    "\n",
    "print(f'Normal: {correct_normal}%')\n",
    "print(f'Zeroed: {correct_zeroed}%')\n",
    "print(f'Delta: {correct_normal - correct_zeroed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8eda42a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with PROPER settings (LR 3e-4, warmup, 5:1 ratio, grad clip)...\n",
      "Step 0: loss=8.866, lr=6.00e-07\n",
      "Step 500: loss=0.080, lr=3.00e-04\n",
      "Step 1000: loss=0.018, lr=2.91e-04\n",
      "Step 1500: loss=0.015, lr=2.65e-04\n",
      "Step 2000: loss=0.027, lr=2.25e-04\n",
      "Step 2500: loss=0.014, lr=1.76e-04\n",
      "Step 3000: loss=0.041, lr=1.24e-04\n",
      "Step 3500: loss=0.055, lr=7.49e-05\n",
      "Step 4000: loss=0.001, lr=3.50e-05\n",
      "Step 4500: loss=0.000, lr=9.01e-06\n",
      "\n",
      "Properly trained model (5:1 GDN:SWA ratio):\n",
      "Accuracy: 44%\n",
      "\n",
      "State Ablation:\n",
      "Normal: 50%\n",
      "Zeroed: 52%\n",
      "Delta: -2\n"
     ]
    }
   ],
   "source": [
    "# PROPER TRAINING per ssm_training_text.md\n",
    "# Fixes:\n",
    "# 1. LR 3e-4 (not 1e-3)\n",
    "# 2. 500 step warmup\n",
    "# 3. Layer ratio: GGGGGS (5:1 GDN:SWA per doc's 1:7 attention:SSM)\n",
    "# 4. Gradient clipping\n",
    "# 5. Use BF16 but keep state operations careful\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "def get_warmup_scheduler(optimizer, warmup_steps, total_steps):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Proper hybrid config with 5:1 GDN:SWA ratio\n",
    "cfg_proper = HybridConfig(\n",
    "    d_model=128, n_heads=2, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GGGGGS',  # 5:1 ratio\n",
    "    vocab_size=50257,\n",
    "    window_size=32, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=1.0,\n",
    "    local_scale=1.0\n",
    ")\n",
    "\n",
    "model_proper = TransparentHybrid(cfg_proper).to(DEVICE)\n",
    "\n",
    "# Proper optimizer settings per doc\n",
    "opt_proper = torch.optim.AdamW(\n",
    "    model_proper.parameters(), \n",
    "    lr=3e-4,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=0.1\n",
    ")\n",
    "\n",
    "total_steps = 5000\n",
    "warmup_steps = 500\n",
    "scheduler = get_warmup_scheduler(opt_proper, warmup_steps, total_steps)\n",
    "\n",
    "print('Training with PROPER settings (LR 3e-4, warmup, 5:1 ratio, grad clip)...')\n",
    "for step in range(total_steps):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    targets = tokens[:, 1:].contiguous()\n",
    "    \n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_proper(tokens)\n",
    "        loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "    \n",
    "    opt_proper.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model_proper.parameters(), 1.0)\n",
    "    opt_proper.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        print(f'Step {step}: loss={loss.item():.3f}, lr={scheduler.get_last_lr()[0]:.2e}')\n",
    "\n",
    "# Test\n",
    "print('\\nProperly trained model (5:1 GDN:SWA ratio):')\n",
    "model_proper.eval()\n",
    "correct = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_proper(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct += 1\n",
    "print(f'Accuracy: {correct}%')\n",
    "\n",
    "# State ablation\n",
    "correct_normal = 0\n",
    "correct_zeroed = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_proper(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct_normal += 1\n",
    "    \n",
    "    # Zeroed\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        x = model_proper.embed(tokens)\n",
    "        x = model_proper.embed_norm(x)\n",
    "        state = None\n",
    "        for i, (layer, ffn) in enumerate(zip(model_proper.layers, model_proper.ffns)):\n",
    "            if cfg_proper.layer_pattern[i] == 'G':\n",
    "                x, state, _ = layer(x, initial_state=state, input_ids=tokens)\n",
    "                key_bank = layer.key_bank\n",
    "            else:\n",
    "                x, _ = layer(x, gdn_state=torch.zeros_like(state), input_ids=tokens, key_bank=key_bank)\n",
    "            x = ffn(x)\n",
    "        logits_z = model_proper.lm_head(model_proper.norm_f(x))\n",
    "    if answer_id in logits_z[0, -1].topk(5).indices.tolist():\n",
    "        correct_zeroed += 1\n",
    "\n",
    "print(f'\\nState Ablation:')\n",
    "print(f'Normal: {correct_normal}%')\n",
    "print(f'Zeroed: {correct_zeroed}%')\n",
    "print(f'Delta: {correct_normal - correct_zeroed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea71fcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GDN-ONLY with proper settings...\n",
      "Step 0: loss=8.986\n",
      "Step 1000: loss=0.080\n",
      "Step 2000: loss=0.061\n",
      "Step 3000: loss=0.043\n",
      "Step 4000: loss=0.085\n",
      "\n",
      "GDN-ONLY model:\n",
      "Accuracy: 68%\n",
      "\n",
      "GDN-ONLY State Ablation:\n",
      "Normal: 71%\n",
      "Zeroed: 71%\n",
      "Delta: 0\n"
     ]
    }
   ],
   "source": [
    "# Compare: GDN-ONLY with same proper training settings\n",
    "# This tests if the issue is with GDN or with the hybrid\n",
    "\n",
    "cfg_gdn_only = HybridConfig(\n",
    "    d_model=128, n_heads=2, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GGGGGG',  # 6 GDN layers, NO SWA\n",
    "    vocab_size=50257,\n",
    "    window_size=32, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=1.0\n",
    ")\n",
    "\n",
    "model_gdn_only = TransparentHybrid(cfg_gdn_only).to(DEVICE)\n",
    "opt_gdn_only = torch.optim.AdamW(\n",
    "    model_gdn_only.parameters(), \n",
    "    lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1\n",
    ")\n",
    "scheduler_gdn = get_warmup_scheduler(opt_gdn_only, 500, 5000)\n",
    "\n",
    "print('Training GDN-ONLY with proper settings...')\n",
    "for step in range(5000):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    targets = tokens[:, 1:].contiguous()\n",
    "    \n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_gdn_only(tokens)\n",
    "        loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "    \n",
    "    opt_gdn_only.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model_gdn_only.parameters(), 1.0)\n",
    "    opt_gdn_only.step()\n",
    "    scheduler_gdn.step()\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print(f'Step {step}: loss={loss.item():.3f}')\n",
    "\n",
    "# Test\n",
    "print('\\nGDN-ONLY model:')\n",
    "model_gdn_only.eval()\n",
    "correct = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_gdn_only(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct += 1\n",
    "print(f'Accuracy: {correct}%')\n",
    "\n",
    "# State ablation for GDN-only\n",
    "correct_normal = 0\n",
    "correct_zeroed = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_gdn_only(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct_normal += 1\n",
    "    \n",
    "    # Zeroed - for GDN-only, we zero all states\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        x = model_gdn_only.embed(tokens)\n",
    "        x = model_gdn_only.embed_norm(x)\n",
    "        for i, (layer, ffn) in enumerate(zip(model_gdn_only.layers, model_gdn_only.ffns)):\n",
    "            # Run with zero initial state and don't accumulate\n",
    "            x_norm = layer.norm(x)\n",
    "            # Just output zeros from delta rule (state is zeroed)\n",
    "            out = torch.zeros(1, tokens.size(1), cfg_gdn_only.n_heads * cfg_gdn_only.value_dim, \n",
    "                            device=DEVICE, dtype=x.dtype)\n",
    "            x = x + layer.o_proj(out)\n",
    "            x = ffn(x)\n",
    "        logits_z = model_gdn_only.lm_head(model_gdn_only.norm_f(x))\n",
    "    if answer_id in logits_z[0, -1].topk(5).indices.tolist():\n",
    "        correct_zeroed += 1\n",
    "\n",
    "print(f'\\nGDN-ONLY State Ablation:')\n",
    "print(f'Normal: {correct_normal}%')\n",
    "print(f'Zeroed: {correct_zeroed}%')\n",
    "print(f'Delta: {correct_normal - correct_zeroed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e087183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training single GDN + curriculum (the successful recipe)...\n",
      "Step 500: dist=0, loss=0.012\n",
      "Step 1000: dist=10, loss=0.172\n",
      "Step 1500: dist=30, loss=0.089\n",
      "Step 2000: dist=50, loss=0.140\n",
      "Step 2500: dist=100, loss=0.174\n",
      "\n",
      "Single GDN + curriculum accuracy at 100 distractors:\n",
      "Accuracy: 100%\n",
      "\n",
      "State Ablation (single GDN + curriculum):\n",
      "Normal: 100%\n",
      "Zeroed: 64%\n",
      "Delta: 36\n",
      "\n",
      "✅ STATE MATTERS!\n"
     ]
    }
   ],
   "source": [
    "# REPLICATE SUCCESSFUL CONFIG: single GDN layer + curriculum + high LR\n",
    "# This got Delta=43 earlier\n",
    "\n",
    "cfg_success = HybridConfig(\n",
    "    d_model=128, n_heads=2, head_dim=64, value_dim=64,\n",
    "    layer_pattern='G',  # SINGLE GDN layer\n",
    "    vocab_size=50257,\n",
    "    window_size=32, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=1.0  # Always write\n",
    ")\n",
    "model_success = TransparentHybrid(cfg_success).to(DEVICE)\n",
    "opt_success = torch.optim.AdamW(model_success.parameters(), lr=1e-3)  # High LR\n",
    "\n",
    "# Curriculum: 0 -> 100 distractors\n",
    "curriculum_success = [\n",
    "    (0, 500),\n",
    "    (10, 500),\n",
    "    (30, 500),\n",
    "    (50, 500),\n",
    "    (100, 500),\n",
    "]\n",
    "\n",
    "print('Training single GDN + curriculum (the successful recipe)...')\n",
    "step = 0\n",
    "for n_dist, n_steps in curriculum_success:\n",
    "    for _ in range(n_steps):\n",
    "        text, answer = make_curriculum_example(n_dist)\n",
    "        tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "        targets = tokens[:, 1:].contiguous()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            logits, _, _, _ = model_success(tokens)\n",
    "            loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "        \n",
    "        opt_success.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_success.step()\n",
    "        step += 1\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(f'Step {step}: dist={n_dist}, loss={loss.item():.3f}')\n",
    "\n",
    "# Verify it works\n",
    "print('\\nSingle GDN + curriculum accuracy at 100 distractors:')\n",
    "model_success.eval()\n",
    "correct = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_success(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct += 1\n",
    "print(f'Accuracy: {correct}%')\n",
    "\n",
    "# State ablation - proper method for single GDN\n",
    "correct_normal = 0\n",
    "correct_zeroed = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_success(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct_normal += 1\n",
    "    \n",
    "    # Zeroed\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        x = model_success.embed(tokens)\n",
    "        x = model_success.embed_norm(x)\n",
    "        state = torch.zeros(1, cfg_success.n_heads, cfg_success.head_dim, cfg_success.value_dim, \n",
    "                           device=DEVICE, dtype=x.dtype)\n",
    "        for i, (layer, ffn) in enumerate(zip(model_success.layers, model_success.ffns)):\n",
    "            # Run GDN but with state contribution zeroed in output\n",
    "            x_norm = layer.norm(x)\n",
    "            out = torch.zeros(1, tokens.size(1), cfg_success.n_heads * cfg_success.value_dim, \n",
    "                            device=DEVICE, dtype=x.dtype)\n",
    "            x = x + layer.o_proj(out)\n",
    "            x = ffn(x)\n",
    "        logits_z = model_success.lm_head(model_success.norm_f(x))\n",
    "    if answer_id in logits_z[0, -1].topk(5).indices.tolist():\n",
    "        correct_zeroed += 1\n",
    "\n",
    "print(f'\\nState Ablation (single GDN + curriculum):')\n",
    "print(f'Normal: {correct_normal}%')\n",
    "print(f'Zeroed: {correct_zeroed}%')\n",
    "print(f'Delta: {correct_normal - correct_zeroed}')\n",
    "print(f'\\n{\"✅ STATE MATTERS!\" if correct_normal - correct_zeroed > 20 else \"❌ STATE IS DECORATIVE\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e07eae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GS (hybrid) with curriculum...\n",
      "Step 500: dist=0, loss=0.008\n",
      "Step 1000: dist=10, loss=0.002\n",
      "Step 1500: dist=30, loss=0.019\n",
      "Step 2000: dist=50, loss=0.071\n",
      "Step 2500: dist=100, loss=0.049\n",
      "\n",
      "GS hybrid accuracy:\n",
      "Accuracy: 18%\n",
      "\n",
      "GS Hybrid State Ablation:\n",
      "Normal: 14%\n",
      "Zeroed: 14%\n",
      "Delta: 0\n",
      "\n",
      "❌ SWA bypasses state\n"
     ]
    }
   ],
   "source": [
    "# NOW: Add SWA to successful recipe\n",
    "# Key insight from docs: SWA should NOT be able to solve the task alone\n",
    "\n",
    "# Test 1: GS with curriculum\n",
    "cfg_gs_curr = HybridConfig(\n",
    "    d_model=128, n_heads=2, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GS',\n",
    "    vocab_size=50257,\n",
    "    window_size=32, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=1.0,\n",
    "    local_scale=1.0  # Full local attention\n",
    ")\n",
    "model_gs = TransparentHybrid(cfg_gs_curr).to(DEVICE)\n",
    "opt_gs = torch.optim.AdamW(model_gs.parameters(), lr=1e-3)\n",
    "\n",
    "curriculum_gs = [\n",
    "    (0, 500),\n",
    "    (10, 500),\n",
    "    (30, 500),\n",
    "    (50, 500),\n",
    "    (100, 500),\n",
    "]\n",
    "\n",
    "print('Training GS (hybrid) with curriculum...')\n",
    "step = 0\n",
    "for n_dist, n_steps in curriculum_gs:\n",
    "    for _ in range(n_steps):\n",
    "        text, answer = make_curriculum_example(n_dist)\n",
    "        tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "        targets = tokens[:, 1:].contiguous()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            logits, _, _, _ = model_gs(tokens)\n",
    "            loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "        \n",
    "        opt_gs.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_gs.step()\n",
    "        step += 1\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(f'Step {step}: dist={n_dist}, loss={loss.item():.3f}')\n",
    "\n",
    "# Test\n",
    "print('\\nGS hybrid accuracy:')\n",
    "model_gs.eval()\n",
    "correct = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_gs(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct += 1\n",
    "print(f'Accuracy: {correct}%')\n",
    "\n",
    "# State ablation\n",
    "correct_normal = 0\n",
    "correct_zeroed = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_gs(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct_normal += 1\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        x = model_gs.embed(tokens)\n",
    "        x = model_gs.embed_norm(x)\n",
    "        state = None\n",
    "        for i, (layer, ffn) in enumerate(zip(model_gs.layers, model_gs.ffns)):\n",
    "            if cfg_gs_curr.layer_pattern[i] == 'G':\n",
    "                x, state, _ = layer(x, initial_state=state, input_ids=tokens)\n",
    "                key_bank = layer.key_bank\n",
    "            else:\n",
    "                x, _ = layer(x, gdn_state=torch.zeros_like(state), input_ids=tokens, key_bank=key_bank)\n",
    "            x = ffn(x)\n",
    "        logits_z = model_gs.lm_head(model_gs.norm_f(x))\n",
    "    if answer_id in logits_z[0, -1].topk(5).indices.tolist():\n",
    "        correct_zeroed += 1\n",
    "\n",
    "print(f'\\nGS Hybrid State Ablation:')\n",
    "print(f'Normal: {correct_normal}%')\n",
    "print(f'Zeroed: {correct_zeroed}%')\n",
    "print(f'Delta: {correct_normal - correct_zeroed}')\n",
    "print(f'\\n{\"✅ STATE MATTERS!\" if correct_normal - correct_zeroed > 10 else \"❌ SWA bypasses state\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a22e92fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HYBRID with SHARED key projection (the fix)...\n",
      "Step 500: dist=0, loss=0.008\n",
      "Step 1000: dist=10, loss=0.002\n",
      "Step 1500: dist=30, loss=0.028\n",
      "Step 2000: dist=50, loss=0.039\n",
      "Step 2500: dist=100, loss=0.048\n",
      "\n",
      "Hybrid with SHARED key projection:\n",
      "Accuracy: 37%\n",
      "\n",
      "State Ablation (SHARED key projection):\n",
      "Normal: 39%\n",
      "Zeroed: 54%\n",
      "Delta: -15\n",
      "\n",
      "❌ Still bypassing state\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FIX: SHARED KEY/QUERY SPACE (per practical_hybrid_solutions.md)\n",
    "# SWA now uses GDN's k_proj for state queries - aligned retrieval\n",
    "# ============================================================\n",
    "\n",
    "# Reload to pick up model changes\n",
    "import importlib\n",
    "import model as model_module\n",
    "importlib.reload(model_module)\n",
    "from model import TransparentHybrid\n",
    "\n",
    "# Same config as before, but now with shared key projection\n",
    "cfg_shared = HybridConfig(\n",
    "    d_model=128, n_heads=2, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GS',  # Hybrid\n",
    "    vocab_size=50257,\n",
    "    window_size=32, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=1.0,\n",
    "    local_scale=1.0  # Full local attention\n",
    ")\n",
    "model_shared = TransparentHybrid(cfg_shared).to(DEVICE)\n",
    "opt_shared = torch.optim.AdamW(model_shared.parameters(), lr=1e-3)\n",
    "\n",
    "# Same curriculum that worked for GDN-only\n",
    "curriculum_shared = [\n",
    "    (0, 500),\n",
    "    (10, 500),\n",
    "    (30, 500),\n",
    "    (50, 500),\n",
    "    (100, 500),\n",
    "]\n",
    "\n",
    "print('Training HYBRID with SHARED key projection (the fix)...')\n",
    "step = 0\n",
    "for n_dist, n_steps in curriculum_shared:\n",
    "    for _ in range(n_steps):\n",
    "        text, answer = make_curriculum_example(n_dist)\n",
    "        tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "        targets = tokens[:, 1:].contiguous()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            logits, _, _, _ = model_shared(tokens)\n",
    "            loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "        \n",
    "        opt_shared.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_shared.step()\n",
    "        step += 1\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(f'Step {step}: dist={n_dist}, loss={loss.item():.3f}')\n",
    "\n",
    "# Test accuracy\n",
    "print('\\nHybrid with SHARED key projection:')\n",
    "model_shared.eval()\n",
    "correct = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_shared(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct += 1\n",
    "print(f'Accuracy: {correct}%')\n",
    "\n",
    "# State ablation - the real test\n",
    "correct_normal = 0\n",
    "correct_zeroed = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_shared(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct_normal += 1\n",
    "    \n",
    "    # Zeroed state\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        x = model_shared.embed(tokens)\n",
    "        x = model_shared.embed_norm(x)\n",
    "        state = None\n",
    "        gdn_k_proj = None\n",
    "        for i, (layer, ffn) in enumerate(zip(model_shared.layers, model_shared.ffns)):\n",
    "            if cfg_shared.layer_pattern[i] == 'G':\n",
    "                x, state, _ = layer(x, initial_state=state, input_ids=tokens)\n",
    "                key_bank = layer.key_bank\n",
    "                gdn_k_proj = layer.k_proj\n",
    "            else:\n",
    "                # Pass ZEROED state but still use shared k_proj\n",
    "                x, _ = layer(x, gdn_state=torch.zeros_like(state), input_ids=tokens, \n",
    "                            key_bank=key_bank, gdn_k_proj=gdn_k_proj)\n",
    "            x = ffn(x)\n",
    "        logits_z = model_shared.lm_head(model_shared.norm_f(x))\n",
    "    if answer_id in logits_z[0, -1].topk(5).indices.tolist():\n",
    "        correct_zeroed += 1\n",
    "\n",
    "print(f'\\nState Ablation (SHARED key projection):')\n",
    "print(f'Normal: {correct_normal}%')\n",
    "print(f'Zeroed: {correct_zeroed}%')\n",
    "print(f'Delta: {correct_normal - correct_zeroed}')\n",
    "print(f'\\n{\"✅ STATE MATTERS - FIX WORKS!\" if correct_normal - correct_zeroed > 10 else \"❌ Still bypassing state\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac7c442c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing language modeling (perplexity):\n",
      "==================================================\n",
      "GDN-only perplexity: 607583395.3\n",
      "Hybrid perplexity: 153621140.1\n",
      "\n",
      "============================================================\n",
      "SUMMARY: GDN-only vs Hybrid\n",
      "============================================================\n",
      "Metric                    GDN-only        Hybrid         \n",
      "------------------------------------------------------------\n",
      "Retrieval accuracy        100%            37%            \n",
      "State ablation delta      +36             -15            \n",
      "Perplexity                607583395.3     153621140.1    \n",
      "------------------------------------------------------------\n",
      "\n",
      "CONCLUSION:\n",
      "  GDN-only: STATE WORKS, retrieval works, may need SWA for fluency\n",
      "  Hybrid: STATE BYPASSED, retrieval broken, SWA dominates\n",
      "\n",
      "Per practical_hybrid_solutions.md: Use GDN-only for retrieval.\n",
      "Add SWA only if perplexity is unacceptably high on real LM tasks.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONCLUSION: Per practical_hybrid_solutions.md\n",
    "# \"For retrieval tasks, GDN-only is sufficient. Add SWA only if LM quality suffers.\"\n",
    "# \n",
    "# Let's test both on actual language modeling (perplexity on real text)\n",
    "# ============================================================\n",
    "\n",
    "# Use the curriculum-trained GDN-only model (model_success) and hybrid (model_shared)\n",
    "# Test perplexity on real text\n",
    "\n",
    "test_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog. This is a test of language modeling capabilities.\",\n",
    "    \"In machine learning, neural networks are computational systems inspired by biological neural networks.\",\n",
    "    \"The weather today is sunny with a chance of rain in the afternoon. Pack an umbrella just in case.\",\n",
    "    \"Python is a high-level programming language known for its readability and versatility.\",\n",
    "    \"Coffee is one of the most popular beverages in the world, consumed by millions daily.\",\n",
    "]\n",
    "\n",
    "print('Testing language modeling (perplexity):')\n",
    "print('=' * 50)\n",
    "\n",
    "# Test GDN-only (model_success - the one that works for retrieval)\n",
    "model_success.eval()\n",
    "gdn_losses = []\n",
    "for text in test_texts:\n",
    "    tokens = tokenizer(text, return_tensors='pt')['input_ids'].to(DEVICE)\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_success(tokens)\n",
    "    targets = tokens[:, 1:].contiguous()\n",
    "    loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "    gdn_losses.append(loss.item())\n",
    "\n",
    "# Test Hybrid (model_shared - broken for retrieval but maybe better for LM)\n",
    "model_shared.eval()\n",
    "hybrid_losses = []\n",
    "for text in test_texts:\n",
    "    tokens = tokenizer(text, return_tensors='pt')['input_ids'].to(DEVICE)\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_shared(tokens)\n",
    "    targets = tokens[:, 1:].contiguous()\n",
    "    loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "    hybrid_losses.append(loss.item())\n",
    "\n",
    "import math\n",
    "gdn_ppl = math.exp(sum(gdn_losses) / len(gdn_losses))\n",
    "hybrid_ppl = math.exp(sum(hybrid_losses) / len(hybrid_losses))\n",
    "\n",
    "print(f'GDN-only perplexity: {gdn_ppl:.1f}')\n",
    "print(f'Hybrid perplexity: {hybrid_ppl:.1f}')\n",
    "print()\n",
    "\n",
    "# Summary table\n",
    "print('=' * 60)\n",
    "print('SUMMARY: GDN-only vs Hybrid')\n",
    "print('=' * 60)\n",
    "print(f'{\"Metric\":<25} {\"GDN-only\":<15} {\"Hybrid\":<15}')\n",
    "print('-' * 60)\n",
    "print(f'{\"Retrieval accuracy\":<25} {\"100%\":<15} {\"37%\":<15}')\n",
    "print(f'{\"State ablation delta\":<25} {\"+36\":<15} {\"-15\":<15}')\n",
    "print(f'{\"Perplexity\":<25} {f\"{gdn_ppl:.1f}\":<15} {f\"{hybrid_ppl:.1f}\":<15}')\n",
    "print('-' * 60)\n",
    "print()\n",
    "print('CONCLUSION:')\n",
    "print('  GDN-only: STATE WORKS, retrieval works, may need SWA for fluency')\n",
    "print('  Hybrid: STATE BYPASSED, retrieval broken, SWA dominates')\n",
    "print()\n",
    "print('Per practical_hybrid_solutions.md: Use GDN-only for retrieval.')\n",
    "print('Add SWA only if perplexity is unacceptably high on real LM tasks.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c87473",
   "metadata": {},
   "source": [
    "# Final Results Summary\n",
    "\n",
    "## Key Finding: The Hybrid Architecture Problem\n",
    "\n",
    "| Config | Retrieval Accuracy | State Delta | State Works? |\n",
    "|--------|-------------------|-------------|--------------|\n",
    "| GDN-only + curriculum | **100%** | **+36** | ✅ YES |\n",
    "| GDN+SWA (separate projections) | 18% | 0 | ❌ NO |\n",
    "| GDN+SWA (shared key projection) | 37% | -15 | ❌ NO |\n",
    "\n",
    "## Root Cause (per practical_hybrid_solutions.md)\n",
    "\n",
    "> \"The GDN-only model works because it's autoassociative—the same key used to write is used to read.\"\n",
    "\n",
    "When SWA is added:\n",
    "1. SWA's local attention has **stronger gradient signal** than state retrieval\n",
    "2. Even with shared key projection, SWA's residual path carries information\n",
    "3. Model learns to ignore state because **\"when attention is available, attention wins\"**\n",
    "\n",
    "## What Works\n",
    "\n",
    "1. **GDN-only with curriculum**: 100% accuracy, Delta=+36\n",
    "2. **beta_floor=1.0**: Always write (not gated)\n",
    "3. **Curriculum learning**: 0→10→30→50→100 distractors\n",
    "4. **High LR (1e-3)**: Simple model, simple task\n",
    "\n",
    "## Next Steps (per the docs)\n",
    "\n",
    "For a **long-context conversational agent** that maintains character/topic consistency:\n",
    "\n",
    "1. **For retrieval**: Use GDN-only layers\n",
    "2. **For language fluency**: Add minimal SWA (1:7 ratio per Jamba)\n",
    "3. **Critical**: Place attention layers LAST, not interspersed\n",
    "4. **Train on real LM objective** with retrieval auxiliary task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "542d636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GGGGGS (5:1 GDN:SWA, SWA at end only)...\n",
      "Step 500: dist=0, loss=0.008\n",
      "Step 1000: dist=10, loss=0.001\n",
      "Step 1500: dist=30, loss=0.001\n",
      "Step 2000: dist=50, loss=0.001\n",
      "Step 2500: dist=100, loss=0.023\n",
      "Step 3000: dist=100, loss=0.002\n",
      "\n",
      "GGGGGS (5:1, SWA at end):\n",
      "Accuracy: 69%\n",
      "\n",
      "State Ablation (GGGGGS):\n",
      "Normal: 61%\n",
      "Zeroed: 61%\n",
      "Delta: 0\n",
      "\n",
      "❌ Still bypassing - SWA residual too strong\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ARCHITECTURE FIX: GDN layers FIRST, SWA at END ONLY\n",
    "# Per ssm_training_text.md: \"never place Transformer blocks at the front\"\n",
    "# Per practical_hybrid_solutions.md: Use GDN for state, SWA only for fluency\n",
    "# ============================================================\n",
    "\n",
    "# Reload model\n",
    "import importlib\n",
    "import model as model_module\n",
    "importlib.reload(model_module)\n",
    "from model import TransparentHybrid\n",
    "\n",
    "# Architecture: GGGGGS (5 GDN layers, then 1 SWA at the end)\n",
    "# This ensures GDN does all the heavy lifting, SWA is just for output fluency\n",
    "cfg_final = HybridConfig(\n",
    "    d_model=128, n_heads=2, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GGGGGS',  # 5:1 ratio, SWA at END\n",
    "    vocab_size=50257,\n",
    "    window_size=32, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=1.0,\n",
    "    local_scale=0.5  # Moderate bottleneck on local path\n",
    ")\n",
    "model_final = TransparentHybrid(cfg_final).to(DEVICE)\n",
    "opt_final = torch.optim.AdamW(model_final.parameters(), lr=1e-3)\n",
    "\n",
    "# Curriculum\n",
    "curriculum_final = [\n",
    "    (0, 500),\n",
    "    (10, 500),\n",
    "    (30, 500),\n",
    "    (50, 500),\n",
    "    (100, 1000),  # More time on hard task\n",
    "]\n",
    "\n",
    "print('Training GGGGGS (5:1 GDN:SWA, SWA at end only)...')\n",
    "step = 0\n",
    "for n_dist, n_steps in curriculum_final:\n",
    "    for _ in range(n_steps):\n",
    "        text, answer = make_curriculum_example(n_dist)\n",
    "        tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "        targets = tokens[:, 1:].contiguous()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            logits, _, _, _ = model_final(tokens)\n",
    "            loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "        \n",
    "        opt_final.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_final.step()\n",
    "        step += 1\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(f'Step {step}: dist={n_dist}, loss={loss.item():.3f}')\n",
    "\n",
    "# Test\n",
    "print('\\nGGGGGS (5:1, SWA at end):')\n",
    "model_final.eval()\n",
    "correct = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_final(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct += 1\n",
    "print(f'Accuracy: {correct}%')\n",
    "\n",
    "# State ablation\n",
    "correct_normal = 0\n",
    "correct_zeroed = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_final(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct_normal += 1\n",
    "    \n",
    "    # Zeroed - manually run\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        x = model_final.embed(tokens)\n",
    "        x = model_final.embed_norm(x)\n",
    "        state = None\n",
    "        gdn_k_proj = None\n",
    "        for i, (layer, ffn) in enumerate(zip(model_final.layers, model_final.ffns)):\n",
    "            if cfg_final.layer_pattern[i] == 'G':\n",
    "                x, state, _ = layer(x, initial_state=state, input_ids=tokens)\n",
    "                key_bank = layer.key_bank\n",
    "                gdn_k_proj = layer.k_proj\n",
    "            else:\n",
    "                x, _ = layer(x, gdn_state=torch.zeros_like(state), input_ids=tokens,\n",
    "                            key_bank=key_bank, gdn_k_proj=gdn_k_proj)\n",
    "            x = ffn(x)\n",
    "        logits_z = model_final.lm_head(model_final.norm_f(x))\n",
    "    if answer_id in logits_z[0, -1].topk(5).indices.tolist():\n",
    "        correct_zeroed += 1\n",
    "\n",
    "print(f'\\nState Ablation (GGGGGS):')\n",
    "print(f'Normal: {correct_normal}%')\n",
    "print(f'Zeroed: {correct_zeroed}%')\n",
    "print(f'Delta: {correct_normal - correct_zeroed}')\n",
    "\n",
    "if correct_normal - correct_zeroed > 10:\n",
    "    print('\\n✅ STATE MATTERS with proper layer ordering!')\n",
    "else:\n",
    "    print('\\n❌ Still bypassing - SWA residual too strong')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e4bde",
   "metadata": {},
   "source": [
    "# ✅ FINAL CONCLUSIONS\n",
    "\n",
    "## What We Tested\n",
    "\n",
    "| Architecture | Accuracy | State Delta | Works? |\n",
    "|--------------|----------|-------------|--------|\n",
    "| GDN-only (1 layer) + curriculum | **100%** | **+36** | ✅ |\n",
    "| GS (1:1 hybrid) + curriculum | 18% | 0 | ❌ |\n",
    "| GS + shared key projection | 37% | -15 | ❌ |\n",
    "| GGGGGS (5:1, SWA at end) | 69% | 0 | ❌ |\n",
    "\n",
    "## Root Cause\n",
    "\n",
    "Per **compass_artifact_text_markdown.md**:\n",
    "> \"When attention is available, attention wins\"\n",
    "\n",
    "Per **practical_hybrid_solutions.md**:\n",
    "> \"The hybrid breaks this because you've introduced a second projection that never learns to query the state.\"\n",
    "\n",
    "Even with:\n",
    "- Shared key projections\n",
    "- SWA at the end only\n",
    "- Local path bottleneck (0.5 scale)\n",
    "\n",
    "...the **residual connection** `out = x + local_out + retrieval_out` carries information from GDN output through SWA without needing state retrieval.\n",
    "\n",
    "## The Solution\n",
    "\n",
    "For **long-context retrieval tasks**:\n",
    "1. Use **GDN-only** architecture\n",
    "2. Apply **curriculum learning** (easy → hard)\n",
    "3. Use **beta_floor=1.0** (always write)\n",
    "4. Test with **state ablation** to verify state is used\n",
    "\n",
    "For **language modeling + retrieval** (the real goal):\n",
    "1. Train GDN on LM objective first\n",
    "2. Add retrieval as auxiliary task\n",
    "3. Only add SWA if perplexity is unacceptable\n",
    "4. If adding SWA: **remove residual around state retrieval** (break the shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cea93ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GS with NO RESIDUAL in SWA...\n",
      "Step 500: dist=0, loss=0.007\n",
      "Step 1000: dist=10, loss=0.609\n",
      "Step 1500: dist=30, loss=0.133\n",
      "Step 2000: dist=50, loss=0.024\n",
      "Step 2500: dist=100, loss=0.051\n",
      "\n",
      "GS with NO RESIDUAL:\n",
      "Accuracy: 0%\n",
      "\n",
      "State Ablation (NO RESIDUAL):\n",
      "Normal: 0%\n",
      "Zeroed: 0%\n",
      "Delta: 0\n",
      "\n",
      "❌ Still not working\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FINAL FIX: Remove residual around state retrieval\n",
    "# Change: out = local_out + retrieval_out (NO x residual in SWA)\n",
    "# This FORCES state usage by breaking the shortcut\n",
    "# ============================================================\n",
    "\n",
    "# Modify SWA forward to not use x residual\n",
    "# We'll do this by subclassing\n",
    "\n",
    "class SWA_NoResidual(model_module.SlidingWindowAttention):\n",
    "    \"\"\"SWA without residual - forces state retrieval usage.\"\"\"\n",
    "    \n",
    "    def forward(self, x, gdn_state=None, input_ids=None, key_bank=None, gdn_k_proj=None):\n",
    "        # Call parent forward\n",
    "        out, diag = super().forward(x, gdn_state, input_ids, key_bank, gdn_k_proj)\n",
    "        # Parent does: out = x + local_out + retrieval_out\n",
    "        # We want: out = local_out + retrieval_out (no x)\n",
    "        # So subtract x\n",
    "        return out - x, diag\n",
    "\n",
    "# Build model with modified SWA\n",
    "cfg_norez = HybridConfig(\n",
    "    d_model=128, n_heads=2, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GS',\n",
    "    vocab_size=50257,\n",
    "    window_size=32, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=1.0,\n",
    "    local_scale=1.0\n",
    ")\n",
    "model_norez = TransparentHybrid(cfg_norez).to(DEVICE)\n",
    "\n",
    "# Replace SWA layer with no-residual version\n",
    "model_norez.layers[1] = SWA_NoResidual(cfg_norez, 1).to(DEVICE)\n",
    "\n",
    "opt_norez = torch.optim.AdamW(model_norez.parameters(), lr=1e-3)\n",
    "\n",
    "curriculum_norez = [\n",
    "    (0, 500),\n",
    "    (10, 500),\n",
    "    (30, 500),\n",
    "    (50, 500),\n",
    "    (100, 500),\n",
    "]\n",
    "\n",
    "print('Training GS with NO RESIDUAL in SWA...')\n",
    "step = 0\n",
    "for n_dist, n_steps in curriculum_norez:\n",
    "    for _ in range(n_steps):\n",
    "        text, answer = make_curriculum_example(n_dist)\n",
    "        tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "        targets = tokens[:, 1:].contiguous()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            logits, _, _, _ = model_norez(tokens)\n",
    "            loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "        \n",
    "        opt_norez.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_norez.step()\n",
    "        step += 1\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(f'Step {step}: dist={n_dist}, loss={loss.item():.3f}')\n",
    "\n",
    "# Test\n",
    "print('\\nGS with NO RESIDUAL:')\n",
    "model_norez.eval()\n",
    "correct = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_norez(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct += 1\n",
    "print(f'Accuracy: {correct}%')\n",
    "\n",
    "# State ablation\n",
    "correct_normal = 0\n",
    "correct_zeroed = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_norez(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct_normal += 1\n",
    "    \n",
    "    # Zeroed\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        x = model_norez.embed(tokens)\n",
    "        x = model_norez.embed_norm(x)\n",
    "        state = None\n",
    "        gdn_k_proj = None\n",
    "        for i, (layer, ffn) in enumerate(zip(model_norez.layers, model_norez.ffns)):\n",
    "            if cfg_norez.layer_pattern[i] == 'G':\n",
    "                x, state, _ = layer(x, initial_state=state, input_ids=tokens)\n",
    "                key_bank = layer.key_bank\n",
    "                gdn_k_proj = layer.k_proj\n",
    "            else:\n",
    "                x, _ = layer(x, gdn_state=torch.zeros_like(state), input_ids=tokens,\n",
    "                            key_bank=key_bank, gdn_k_proj=gdn_k_proj)\n",
    "            x = ffn(x)\n",
    "        logits_z = model_norez.lm_head(model_norez.norm_f(x))\n",
    "    if answer_id in logits_z[0, -1].topk(5).indices.tolist():\n",
    "        correct_zeroed += 1\n",
    "\n",
    "print(f'\\nState Ablation (NO RESIDUAL):')\n",
    "print(f'Normal: {correct_normal}%')\n",
    "print(f'Zeroed: {correct_zeroed}%')\n",
    "print(f'Delta: {correct_normal - correct_zeroed}')\n",
    "\n",
    "if correct_normal - correct_zeroed > 10:\n",
    "    print('\\n✅ BREAKING RESIDUAL WORKS! State matters in hybrid!')\n",
    "else:\n",
    "    print('\\n❌ Still not working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f15b257f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with STOCHASTIC LOCAL DROP (70% drop)...\n",
      "Step 500: dist=0, loss=0.009\n",
      "Step 1000: dist=10, loss=0.002\n",
      "Step 1500: dist=30, loss=0.021\n",
      "Step 2000: dist=50, loss=0.045\n",
      "Step 2500: dist=100, loss=0.025\n",
      "Step 3000: dist=100, loss=0.028\n",
      "\n",
      "Stochastic local drop:\n",
      "Accuracy: 73%\n",
      "\n",
      "State Ablation (Stochastic local):\n",
      "Normal: 64%\n",
      "Zeroed: 49%\n",
      "Delta: 15\n",
      "\n",
      "✅ STOCHASTIC DEPTH WORKS!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ALTERNATIVE: Stochastic depth on local path only\n",
    "# Per compass doc: \"applying higher dropout to non-memory pathways forces memory utilization\"\n",
    "# ============================================================\n",
    "\n",
    "class SWA_StochasticLocal(model_module.SlidingWindowAttention):\n",
    "    \"\"\"SWA with stochastic dropout on local path only.\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg, layer_idx=0, local_drop_prob=0.7):\n",
    "        super().__init__(cfg, layer_idx)\n",
    "        self.local_drop_prob = local_drop_prob\n",
    "    \n",
    "    def forward(self, x, gdn_state=None, input_ids=None, key_bank=None, gdn_k_proj=None):\n",
    "        B, T, D = x.shape\n",
    "        H = self.cfg.n_heads\n",
    "        K, V, W = self.cfg.head_dim, self.cfg.value_dim, self.cfg.window_size\n",
    "        \n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        # Local attention\n",
    "        q = self.q_proj(x_norm).view(B, T, H, D // H)\n",
    "        k = self.k_proj(x_norm).view(B, T, H, D // H)\n",
    "        v = self.v_proj(x_norm).view(B, T, H, D // H)\n",
    "        \n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        mask = torch.ones(T, T, device=x.device, dtype=torch.bool).triu(1)\n",
    "        mask |= torch.ones(T, T, device=x.device, dtype=torch.bool).tril(-W - 1)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        local_out = (F.softmax(attn, dim=-1) @ v).transpose(1, 2).reshape(B, T, D)\n",
    "        local_out = self.o_proj(local_out)\n",
    "        \n",
    "        # STOCHASTIC DEPTH: drop local path during training\n",
    "        if self.training and torch.rand(1).item() < self.local_drop_prob:\n",
    "            local_out = torch.zeros_like(local_out)\n",
    "        \n",
    "        # State retrieval\n",
    "        retrieval_out = torch.zeros_like(x)\n",
    "        if gdn_state is not None and gdn_k_proj is not None:\n",
    "            q_g = gdn_k_proj(x_norm).view(B, T, H, K)\n",
    "            q_g = q_g.transpose(1, 2)\n",
    "            retrieved = torch.einsum('bhkv,bhtk->bhtv', gdn_state.to(x.dtype), q_g)\n",
    "            retrieved = retrieved.transpose(1, 2).reshape(B, T, H * V)\n",
    "            retrieval_out = self.retrieval_o_proj(retrieved)\n",
    "            \n",
    "            gate = torch.sigmoid(self.gate_proj(x_norm))\n",
    "            retrieval_out = gate.mean(dim=-1, keepdim=True) * retrieval_out\n",
    "        \n",
    "        out = x + local_out + retrieval_out\n",
    "        return out, {'gate_mean': 0, 'local_norm': local_out.norm().item(), 'retrieval_norm': retrieval_out.norm().item()}\n",
    "\n",
    "# Build model\n",
    "cfg_stoch = HybridConfig(\n",
    "    d_model=128, n_heads=2, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GS',\n",
    "    vocab_size=50257,\n",
    "    window_size=32, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=1.0\n",
    ")\n",
    "model_stoch = TransparentHybrid(cfg_stoch).to(DEVICE)\n",
    "model_stoch.layers[1] = SWA_StochasticLocal(cfg_stoch, 1, local_drop_prob=0.7).to(DEVICE)\n",
    "\n",
    "opt_stoch = torch.optim.AdamW(model_stoch.parameters(), lr=1e-3)\n",
    "\n",
    "curriculum_stoch = [\n",
    "    (0, 500),\n",
    "    (10, 500),\n",
    "    (30, 500),\n",
    "    (50, 500),\n",
    "    (100, 1000),\n",
    "]\n",
    "\n",
    "print('Training with STOCHASTIC LOCAL DROP (70% drop)...')\n",
    "step = 0\n",
    "for n_dist, n_steps in curriculum_stoch:\n",
    "    for _ in range(n_steps):\n",
    "        text, answer = make_curriculum_example(n_dist)\n",
    "        tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "        targets = tokens[:, 1:].contiguous()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            logits, _, _, _ = model_stoch(tokens)\n",
    "            loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "        \n",
    "        opt_stoch.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_stoch.step()\n",
    "        step += 1\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(f'Step {step}: dist={n_dist}, loss={loss.item():.3f}')\n",
    "\n",
    "# Test\n",
    "print('\\nStochastic local drop:')\n",
    "model_stoch.eval()  # Dropout off at eval\n",
    "correct = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_stoch(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct += 1\n",
    "print(f'Accuracy: {correct}%')\n",
    "\n",
    "# State ablation\n",
    "correct_normal = 0\n",
    "correct_zeroed = 0\n",
    "for _ in range(100):\n",
    "    text, answer = make_curriculum_example(100)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_stoch(tokens)\n",
    "    if answer_id in logits[0, -1].topk(5).indices.tolist():\n",
    "        correct_normal += 1\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        x = model_stoch.embed(tokens)\n",
    "        x = model_stoch.embed_norm(x)\n",
    "        state = None\n",
    "        gdn_k_proj = None\n",
    "        for i, (layer, ffn) in enumerate(zip(model_stoch.layers, model_stoch.ffns)):\n",
    "            if cfg_stoch.layer_pattern[i] == 'G':\n",
    "                x, state, _ = layer(x, initial_state=state, input_ids=tokens)\n",
    "                key_bank = layer.key_bank\n",
    "                gdn_k_proj = layer.k_proj\n",
    "            else:\n",
    "                x, _ = layer(x, gdn_state=torch.zeros_like(state), input_ids=tokens,\n",
    "                            key_bank=key_bank, gdn_k_proj=gdn_k_proj)\n",
    "            x = ffn(x)\n",
    "        logits_z = model_stoch.lm_head(model_stoch.norm_f(x))\n",
    "    if answer_id in logits_z[0, -1].topk(5).indices.tolist():\n",
    "        correct_zeroed += 1\n",
    "\n",
    "print(f'\\nState Ablation (Stochastic local):')\n",
    "print(f'Normal: {correct_normal}%')\n",
    "print(f'Zeroed: {correct_zeroed}%')\n",
    "print(f'Delta: {correct_normal - correct_zeroed}')\n",
    "\n",
    "if correct_normal - correct_zeroed > 10:\n",
    "    print('\\n✅ STOCHASTIC DEPTH WORKS!')\n",
    "else:\n",
    "    print('\\n❌ Still bypassing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b019c19",
   "metadata": {},
   "source": [
    "# 🎉 WORKING HYBRID CONFIGURATION\n",
    "\n",
    "## Final Results\n",
    "\n",
    "| Architecture | Accuracy | State Delta | Works? |\n",
    "|--------------|----------|-------------|--------|\n",
    "| GDN-only + curriculum | 100% | +36 | ✅ |\n",
    "| GS (vanilla) | 18% | 0 | ❌ |\n",
    "| GS + shared keys | 37% | -15 | ❌ |\n",
    "| GGGGGS (5:1) | 69% | 0 | ❌ |\n",
    "| **GS + stochastic local drop (70%)** | **73%** | **+15** | ✅ |\n",
    "\n",
    "## The Working Fix\n",
    "\n",
    "Per **compass_artifact_text_markdown.md**:\n",
    "> \"Stochastic depth randomly drops entire residual blocks during training; applied inversely to non-memory pathways, this forces information through memory.\"\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "if self.training and torch.rand(1).item() < 0.7:\n",
    "    local_out = torch.zeros_like(local_out)  # Drop local 70% of time\n",
    "```\n",
    "\n",
    "This forces the model to learn state retrieval because local attention is unreliable during training.\n",
    "\n",
    "## Summary of What Works\n",
    "\n",
    "1. **GDN-only**: Best for pure retrieval (Delta=+36)\n",
    "2. **GS + stochastic local**: Hybrid that uses state (Delta=+15)\n",
    "3. **Curriculum learning**: Essential for both\n",
    "4. **Shared key projection**: Helps alignment but not sufficient alone\n",
    "5. **beta_floor=1.0**: Always write, don't gate\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Increase training steps for stochastic hybrid\n",
    "2. Try 50% local drop instead of 70%\n",
    "3. Test on language modeling perplexity\n",
    "4. Scale to larger model/longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSIS: Is SWA's state retrieval pathway learning at all?\n",
    "# Check gradient norms during training\n",
    "\n",
    "cfg_diag = HybridConfig(\n",
    "    d_model=128, n_heads=2, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GS',\n",
    "    vocab_size=50257,\n",
    "    window_size=32, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=1.0,\n",
    "    local_scale=0.3  # Bottleneck local path\n",
    ")\n",
    "model_diag = TransparentHybrid(cfg_diag).to(DEVICE)\n",
    "opt_diag = torch.optim.AdamW(model_diag.parameters(), lr=1e-3)\n",
    "\n",
    "print('Training hybrid and monitoring gradient norms...')\n",
    "gdn_grads = []\n",
    "swa_local_grads = []\n",
    "swa_retrieval_grads = []\n",
    "\n",
    "for step in range(1000):\n",
    "    text, answer = make_curriculum_example(100)  # Hard task\n",
    "    tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=300)['input_ids'].to(DEVICE)\n",
    "    targets = tokens[:, 1:].contiguous()\n",
    "    \n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_diag(tokens)\n",
    "        loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "    \n",
    "    opt_diag.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Collect gradient norms\n",
    "    if step % 100 == 0:\n",
    "        gdn_layer = model_diag.layers[0]  # GDN\n",
    "        swa_layer = model_diag.layers[1]  # SWA\n",
    "        \n",
    "        gdn_grad = gdn_layer.k_proj.weight.grad.norm().item() if gdn_layer.k_proj.weight.grad is not None else 0\n",
    "        swa_local_grad = swa_layer.q_proj.weight.grad.norm().item() if swa_layer.q_proj.weight.grad is not None else 0\n",
    "        swa_retrieval_grad = swa_layer.global_q_proj.weight.grad.norm().item() if swa_layer.global_q_proj.weight.grad is not None else 0\n",
    "        \n",
    "        gdn_grads.append(gdn_grad)\n",
    "        swa_local_grads.append(swa_local_grad)\n",
    "        swa_retrieval_grads.append(swa_retrieval_grad)\n",
    "        \n",
    "        print(f'Step {step}: GDN k_proj={gdn_grad:.4f}, SWA local={swa_local_grad:.4f}, SWA retrieval={swa_retrieval_grad:.4f}')\n",
    "    \n",
    "    opt_diag.step()\n",
    "\n",
    "print(f'\\nAverage gradient norms:')\n",
    "print(f'  GDN k_proj: {sum(gdn_grads)/len(gdn_grads):.4f}')\n",
    "print(f'  SWA local: {sum(swa_local_grads)/len(swa_local_grads):.4f}')\n",
    "print(f'  SWA retrieval: {sum(swa_retrieval_grads)/len(swa_retrieval_grads):.4f}')\n",
    "\n",
    "ratio = sum(swa_local_grads)/max(sum(swa_retrieval_grads), 1e-8)\n",
    "print(f'\\nLocal/Retrieval gradient ratio: {ratio:.1f}x')\n",
    "print('(If ratio >> 1, local path dominates learning)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786d07fc",
   "metadata": {},
   "source": [
    "## Fix 1: Auxiliary Reconstruction Loss\n",
    "\n",
    "Force state to encode retrievable info by adding loss that reconstructs input from state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff11dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with reconstruction loss: state must be able to reconstruct early tokens\n",
    "cfg2 = HybridConfig(\n",
    "    d_model=256, n_heads=4, head_dim=64, value_dim=64,\n",
    "    layer_pattern='GS', vocab_size=50257,\n",
    "    window_size=64, beta_bias=0.0, g_bias=2.0,\n",
    "    shifted_value=True, beta_floor=0.1\n",
    ")\n",
    "model2 = TransparentHybrid(cfg2).to(DEVICE)\n",
    "\n",
    "# Add reconstruction head: state -> predict early tokens\n",
    "recon_head = nn.Linear(cfg2.n_heads * cfg2.head_dim * cfg2.value_dim, 50257).to(DEVICE)\n",
    "\n",
    "opt2 = torch.optim.AdamW(list(model2.parameters()) + list(recon_head.parameters()), lr=1e-3)\n",
    "\n",
    "print('Training with reconstruction loss...')\n",
    "for step in range(3000):\n",
    "    text, answer = make_example()\n",
    "    tokens = tokenizer(text + answer, return_tensors='pt', truncation=True, max_length=600)['input_ids'].to(DEVICE)\n",
    "    targets = tokens[:, 1:].contiguous()\n",
    "    \n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, state = model2(tokens)\n",
    "        lm_loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), targets.reshape(-1))\n",
    "        \n",
    "        # Reconstruction loss: predict first 10 tokens from final state\n",
    "        state_flat = state.reshape(1, -1)  # [1, H*K*V]\n",
    "        recon_logits = recon_head(state_flat.float())  # [1, vocab]\n",
    "        # Target: average of first 10 token embeddings -> predict first token\n",
    "        first_token = tokens[0, 0]\n",
    "        recon_loss = F.cross_entropy(recon_logits, first_token.unsqueeze(0))\n",
    "        \n",
    "        loss = lm_loss + 0.5 * recon_loss\n",
    "    \n",
    "    opt2.zero_grad()\n",
    "    loss.backward()\n",
    "    opt2.step()\n",
    "    if step % 500 == 0:\n",
    "        print(f'Step {step}: lm={lm_loss.item():.3f}, recon={recon_loss.item():.3f}')\n",
    "\n",
    "print('\\nWith reconstruction loss:')\n",
    "test_state_ablation(model2, cfg2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
