# Why neural networks learn to ignore their own memory

Memory-augmented neural networks consistently learn to bypass their auxiliary memory components, treating external memory as "decorative"—computed but unused. **The core problem is gradient dilution**: memory pathways involve multiple multiplicative operations (key generation → similarity computation → softmax → weighted read), each spreading gradients across all memory locations, while direct computation paths have fewer stages and stronger gradient signal. This phenomenon is documented across classic memory networks (NTMs, DNCs), modern state space models (Mamba, RWKV), and hybrid architectures, with solutions ranging from architectural bottlenecks to auxiliary losses that force memory utilization.

## Soft attention creates a gradient graveyard for memory operations

The fundamental mechanism causing memory bypass traces to how differentiable memory access works. **Softmax attention assumes normally distributed inputs for gradient stability, but memory attention scores are not normally distributed.** Values exceeding expected ranges fall into softmax saturation areas where derivatives approach zero, causing attention weights to become "stuck" at certain patterns. Wang et al. (2021) demonstrated this empirically, showing that "in attention mechanisms, there is always a part of the input stuck in the saturation area, leading to gradient vanishing and long training."

The memory pathway through NTMs and DNCs involves five multiplicative stages: input → controller → key generation → similarity computation → softmax normalization → memory read → output integration. Each stage dilutes gradients multiplicatively. Meanwhile, controllers (often LSTMs) maintain their own internal state that offers a shortcut path with stronger gradient flow. **Controllers consistently learn to use internal state instead of external memory—this is documented behavior that researchers now explicitly design architectures to prevent** (Taguchi et al., 2018). Santoro et al. (2016) even proposed "hamstringing the controller's ability" to force actual memory usage, acknowledging that without constraints, memory becomes decorative.

Collier and Beel's 2018 ICANN Best Paper found that multiple open-source NTM implementations were "unstable during training and/or fail to replicate reported performance," with gradients frequently becoming NaN. The solution required careful initialization (memory contents to small constants like **10⁻⁶**), gradient clipping to [-10, 10], and curriculum learning starting with sequences under 20 steps—without which training fails entirely.

## State space models face an information bottleneck that worsens with context

For modern efficient architectures like Mamba and RWKV, a different but related problem emerges: **fixed-size recurrent states create fundamental information bottlenecks that become increasingly problematic as context length grows**. Stanford's Hazy Research group documented that "all architectures obeyed a fundamental tradeoff: the less memory consumed during inference, the worse performance on associative recall." Testing on Multi-Query Associative Recall showed sliding window attention dropping from 100% accuracy (1MB state) to 50% accuracy (65KB state).

Park et al. (2025) revealed the "Stuffed Mamba" problem: Mamba-2 and RWKV-6 "do not know how to robustly forget earlier information to avoid memory overload." When state size is large relative to training context length, models can perform well without learning proper forgetting mechanisms. **There exists a minimum training length threshold—below this, models never learn to forget, rendering the state less useful at scale.** Multiple memory entries interfere and cause retrieval errors as the state cannot store mutually orthogonal vectors beyond a certain capacity.

The recall-memory tradeoff is severe. Arora et al. (2024) showed that "recurrent LMs cannot recall and use all the information in long contexts, leading to brittle in-context learning quality." Models like Mamba and RWKV "drastically underperform Transformers on recall-intensive tasks"—not because the state is absent, but because it's insufficiently utilized.

## Hybrid architectures: when attention is available, attention wins

Perhaps the strongest evidence for memory becoming decorative comes from hybrid architectures combining sliding window attention with linear attention or state space layers. AI21 Labs' Jamba experiments revealed a counter-intuitive finding: **Mamba-1 combined with attention outperforms Mamba-2 combined with attention**, despite Mamba-2's larger state capacity. The hypothesis: "advantages of Mamba-2 over Mamba-1, particularly the ability to use a much larger state size, are less significant when we have full attention layers interleaved, as they can pool information from the entire context."

This directly demonstrates attention components reducing reliance on recurrent state. The SWAX paper found that "larger sliding windows do not improve long-context performance"—instead, **short window attention encourages the model to better train the long-term memory** by forcing it to not rely on attention for long-context retrieval. When given the choice between utilizing state or relying on attention, models consistently choose attention.

Systematic analysis of hybrid linear attention architectures showed that RetNet exhibited near-zero recall even when full-attention layers were added, because its exponential decay failed to protect long-range cues. More troubling: models could "freely optimize the linear to full attention ratio with minimal effect on language modeling performance," suggesting the linear components contribute little. The "Based" paper found that **64-128 token sliding windows recover 90.8% of full softmax attention's recall accuracy**, suggesting most model performance derives from local context rather than the recurrent state.

## Forcing memory usage requires breaking the shortcut

Several proven techniques force networks to actually use memory rather than bypassing it. **Auxiliary reconstruction losses** are among the most effective: Trinh et al. (2018) added unsupervised auxiliary losses requiring RNNs to reconstruct previous events, forcing memory to store retrievable information. This handled sequences up to 16,000 tokens and achieved 40% accuracy on CIFAR-10 using only the auxiliary loss—no supervised signal—demonstrating that reconstruction truly forces memory encoding.

**Expert Choice Routing** (Zhou et al., 2022) reverses the routing paradigm: instead of tokens choosing which memory slots to use, memory slots choose which tokens to store. This achieved **>2× faster training convergence** and perfect load balancing by design, eliminating the need for auxiliary losses entirely. Wang et al. (2024) further developed Loss-Free Balancing that applies dynamic expert-wise bias to routing scores, achieving better performance and load balance than auxiliary-loss methods while avoiding interference gradients.

Architectural interventions show similar promise. **Memory masking** for DNCs (Csordás & Schmidhuber, 2019) dynamically learned which parts of memory cells to use for lookup, improving mean error rate by **43%** on bAbI reasoning tasks. **Stochastic depth** (Huang et al., 2016) randomly drops entire residual blocks during training; applied inversely to non-memory pathways, this forces information through memory. Information bottleneck approaches maximize I(Z,Y) - β×I(Z,X), forcing networks to compress information through bottlenecks that require selective storage.

Curriculum learning proves essential for memory-dependent tasks. Graves et al. (2017) used multi-armed bandits to select training samples maximizing learning progress, finding that memory tasks learned faster when curricula first focused on short sequences with high repetition requirements before scaling to longer sequences. Without curriculum, NTM models trained on sequences of 1-20 steps "did not converge at all."

## Detecting decorative components before they waste compute

Several diagnostic methods identify unused components. **Ablation studies** remain foundational: Voita et al. (2019) found that **pruning 38 of 48 encoder attention heads caused only 0.15 BLEU drop** on English-Russian translation—over 75% of attention heads were effectively decorative. The Lottery Ticket Hypothesis (Frankle & Carlin, 2019) showed networks can be pruned by 90%+ without accuracy loss, with winning tickets identifiable through iterative magnitude pruning.

**Gradient flow analysis** tracks ∥∂L/∂w∥ across layers, identifying components with vanishing gradients. Tessera et al. showed L2 regularization can increase gradient magnitudes in sparse networks, potentially causing instability, while near-zero gradient norms indicate decorative components that receive no learning signal. **Activation patching** (Heimersheim & Janiak, 2024) provides causal evidence: run the model on corrupted input, patch specific component activations with clean-run values, and measure whether correct behavior is restored. Components that don't restore behavior when patched are causally unimportant.

For attention specifically, entropy analysis reveals heads that attend uniformly (high entropy, likely decorative) versus selectively (low entropy, likely functional). Memory utilization metrics track frequency of memory slot access and read/write patterns—slots never accessed are obviously unused.

## Practical implications for architecture design

The evidence converges on several principles. First, **memory pathways must be architecturally unavoidable**: skip connections and powerful controllers enable bypass. Limiting controller capacity, removing direct residual connections around memory operations, or applying higher dropout to non-memory pathways forces memory utilization. Second, **training context length must exceed state capacity** to teach forgetting mechanisms—short-context training produces models that never learn proper state management. Third, **hybrid architectures require careful component placement**: attention layers at the front allow attention to dominate; Jamba found "never place Transformer blocks at the front" as a key design rule.

The DeltaNet and Gated DeltaNet architectures explicitly address these issues by combining gating for adaptive memory control with delta update rules for precise modifications. As the authors note, "vanilla linear attention has underperformed compared to softmax attention by a large margin" precisely because of memory overload—"the enemy of memory is not time; it's other memories." These architectures achieve perfect performance on associative recall benchmarks by explicitly building forgetting and updating mechanisms that vanilla architectures never learn.

## Conclusion

Memory becoming decorative is not a training bug but a natural consequence of optimization following the path of least resistance. **When memory pathways require more gradient stages than direct computation, when controllers can solve tasks using internal state, and when attention can substitute for recurrent state, models will bypass memory**. The solutions work by eliminating escape routes: reconstruction losses that demand memory retrieval, architectural bottlenecks that remove alternatives, curriculum learning that teaches memory dependency before shortcuts become available, and hybrid designs that limit attention's ability to compensate for weak state utilization. The field is moving toward architectures that make memory usage mandatory rather than optional—recognizing that differentiable memory only works when the model has no choice but to use it.