{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# GroundThink v7\n",
    "\n",
    "## GDN + SWA Hybrid with Chunk-Recurrent Delta Rule\n",
    "\n",
    "\n",
    "**v7 Changes from v6:**\n",
    "- Chunk-recurrent backward pass (numerically stable)\n",
    "- Modular code, but now all scripts are in a single folder for direct import\n",
    "- No inline Triton kernels in notebook\n",
    "\n",
    "**Current Script Structure:**\n",
    "```\n",
    "config.py      # HybridConfig\n",
    "core.py        # Triton kernels + chunk_delta_rule\n",
    "model.py       # GDN, SWA, TransparentHybrid\n",
    "analysis.py    # NIAH tests, training utils\n",
    "```\n",
    "\n",
    "**Notebook Import Mode:**\n",
    "- All imports are now direct from scripts (not as a package)\n",
    "- This enables compatibility with flat-folder workflows and dynamic imports in notebooks\n",
    "\n",
    "**Notebook/Progress Bar Fixes:**\n",
    "- tqdm notebook progress bars require `jupyter` and `ipywidgets` to be installed\n",
    "- Both are now included in requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.11.0.dev20260128+cu128\n",
      "CUDA: True\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "sys.path: ['/home/m_tes/groundthink/gt-v6/v7-design/groundthink_v7', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/home/m_tes/groundthink/gt-v6/.venv/lib/python3.12/site-packages']\n",
      "os.getcwd(): /home/m_tes/groundthink/gt-v6/v7-design/groundthink_v7\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.getcwd()))  # Ensure current folder is importable\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"sys.path:\", sys.path)\n",
    "print(\"os.getcwd():\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ v7 scripts imported (dynamic import mode)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "# Patch sys.modules to allow relative imports in scripts when running as a notebook\n",
    "import sys\n",
    "import importlib.util\n",
    "import types\n",
    "\n",
    "def import_module_from_file(module_name, file_path):\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = module\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "config = import_module_from_file('config', './config.py')\n",
    "core = import_module_from_file('core', './core.py')\n",
    "model = import_module_from_file('model', './model.py')\n",
    "analysis = import_module_from_file('analysis', './analysis.py')\n",
    "\n",
    "HybridConfig = config.HybridConfig\n",
    "TransparentHybrid = model.TransparentHybrid\n",
    "proper_niah_test = analysis.proper_niah_test\n",
    "test_niah_by_distance = analysis.test_niah_by_distance\n",
    "run_full_diagnostic = analysis.run_full_diagnostic\n",
    "validate_delta_rule = analysis.validate_delta_rule\n",
    "train_curriculum = analysis.train_curriculum\n",
    "analyze_gradients = analysis.analyze_gradients\n",
    "load_wikitext = analysis.load_wikitext\n",
    "\n",
    "print(\"✓ v7 scripts imported (dynamic import mode)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridConfig(GS, d=512, h=8, K=64, V=128)\n",
      "\n",
      "State capacity: 8 heads × 64 × 128 = 65,536 floats\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SYNC CONFIGURATION: Set Sequence Length\n",
    "# =============================================================================\n",
    "# This cell synchronizes the sequence length (NEW_T) across config, loader, and all reporting.\n",
    "\n",
    "NEW_T = 2048  # Set your new sequence length here\n",
    "\n",
    "cfg = HybridConfig(\n",
    "    d_model=512,        # UP from 256 - more capacity for multi-needle\n",
    "    n_heads=8,\n",
    "    head_dim=64,        # UP from 32 (d_model / n_heads)\n",
    "    value_dim=128,      # UP from 64 (2x head_dim is common)\n",
    "    layer_pattern=\"GS\",\n",
    "    window_size=64,\n",
    "    chunk_size=64,\n",
    "    beta_bias=-2.0,\n",
    "    g_bias=2.0,\n",
    ")\n",
    "\n",
    "print(cfg)\n",
    "print(f\"\\nState capacity: {cfg.n_heads} heads × {cfg.head_dim} × {cfg.value_dim} = {cfg.n_heads * cfg.head_dim * cfg.value_dim:,} floats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 33,217,560\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CREATE MODEL\n",
    "# =============================================================================\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = TransparentHybrid(cfg).to(DEVICE).bfloat16()\n",
    "\n",
    "print(f\"Parameters: {model.count_params():,}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "validate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FORWARD/BACKWARD TEST\n",
      "============================================================\n",
      "Forward: output=torch.Size([2, 64, 50257]), state_norm=7.5000 ✓\n",
      "Backward: loss=10.9375 ✓\n",
      "\n",
      "============================================================\n",
      "DELTA RULE VALIDATION\n",
      "============================================================\n",
      "\n",
      "1. Identical Tokens (Redundancy Suppression):\n",
      "  Error2: 0.000001 (should be ~0)\n",
      "  Growth: 1.0000x\n",
      "  → ✓ PASS\n",
      "\n",
      "2. Orthogonal Keys (Independent Storage):\n",
      "  v1 error: 0.000000\n",
      "  v2 error: 0.000000\n",
      "  → ✓ PASS\n",
      "\n",
      "3. Capacity (100 writes):\n",
      "  State norm: 44.82\n",
      "  First error: 1.4286\n",
      "  Last error: 0.0000\n",
      "  → First degrades (expected)\n",
      "\n",
      "============================================================\n",
      "OVERALL: ✓ ALL PASS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'identical_tokens': True, 'orthogonal_keys': True, 'capacity': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FORWARD/BACKWARD TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Forward\n",
    "x = torch.randint(0, 1000, (2, 64), device=DEVICE)\n",
    "with torch.no_grad():\n",
    "    logits, _, diags, state = model(x)\n",
    "print(f\"Forward: output={logits.shape}, state_norm={state.norm().item():.4f} ✓\")\n",
    "\n",
    "# Backward\n",
    "model.train()\n",
    "y = torch.randint(0, 1000, (2, 64), device=DEVICE)\n",
    "_, loss, _, _ = model(x, y)\n",
    "loss.backward()\n",
    "print(f\"Backward: loss={loss.item():.4f} ✓\")\n",
    "\n",
    "# Delta Rule validation\n",
    "validate_delta_rule(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "niah-untrained",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "NIAH TEST (Untrained)\n",
      "============================================================\n",
      "  Accuracy: 0.0% (0/20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0, 'correct': 0, 'total': 20}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NIAH (Untrained Baseline)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NIAH TEST (Untrained)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "proper_niah_test(model, seq_len=64, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2,000,000 tokens from wikitext-103...\n",
      "Loaded 2,000,000 tokens\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATA\n",
    "# =============================================================================\n",
    "# Math is: Batch Size * Sequence Length = Constant Tokens per Batch\n",
    "# Epochs = Total Tokens / (Batch Size * Sequence Length)\n",
    "# Logic is: Adjust batch size inversely with sequence length to maintain constant token count per batch.\n",
    "# Science is: Longer sequence = more context, fewer updates per epoch; \n",
    "#             Shorter sequence = less context, more updates per epoch.\n",
    "#          Keeping total tokens per batch constant balances context and update frequency.\n",
    "# Practicality is: Larger batch sizes improve training stability and throughput,\n",
    "# but require more memory. Smaller batch sizes fit in memory but may lead to noisier updates\n",
    "# and lower throughput. Find a balance based on your hardware capabilities.\n",
    "# =============================================================================\n",
    "# Note: Monitor batch size and sequence length to avoid OOM errors.  \n",
    "# These values are for a GPU with 6GB VRAM (e.g., RTX 4050).\n",
    "# 4096 sequence length with batch size 2 is stable and efficient.\n",
    "# 2048 sequence length with batch size 4-2 is safe. Batch 2 learns slower with higher tok/s.\n",
    "# 1024 sequence length with batch size 8 is a good starting point.\n",
    "# 512 or less sequence length with batch size 16 (32 untested) also works well.\n",
    "#\n",
    "# COMPETENCE MATH:\n",
    "#   - Tokens per step = batch_size * seq_len\n",
    "#   - Steps per epoch = n_tokens / tokens_per_step  \n",
    "#   - Target: ~2 epochs for learning, not memorizing\n",
    "#   - At batch=2, seq=4096: 8192 tokens/step, 2M tokens = 244 batches\n",
    "\n",
    "data_loader = load_wikitext(n_tokens=2_000_000, seq_len=NEW_T, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded87b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Profile for RTX 4050 (6GB)(DO NOT EXCEED 45%):\n",
      "==================================================\n",
      " Seq Len | Batch |   Tokens |  Peak MB | % of 6GB\n",
      "--------------------------------------------------\n",
      "     512 |     2 |     1024 |    513.3 |     8.4%\n",
      "     512 |     4 |     2048 |    800.5 |    13.0%\n",
      "     512 |     8 |     4096 |   1509.8 |    24.6%\n",
      "    1024 |     2 |     2048 |    800.2 |    13.0%\n",
      "    1024 |     4 |     4096 |   1509.2 |    24.6%\n",
      "    1024 |     8 |     8192 |   2912.6 |    47.4%\n",
      "    2048 |     2 |     4096 |   1507.6 |    24.5%\n",
      "    2048 |     4 |     8192 |   2912.1 |    47.4%\n",
      "    2048 |     8 |    16384 |   5723.5 |    93.2%\n",
      "    4096 |     2 |     8192 |   2910.3 |    47.4%\n",
      "    4096 |     4 |    16384 |   5723.0 |    93.1%\n",
      "    4096 |     8 |    32768 |  11352.4 |   184.8%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MEMORY PROFILING\n",
    "# =============================================================================\n",
    "# Track exact VRAM usage for different batch/seq combinations\n",
    "\n",
    "def profile_memory(model, seq_lens=[512, 1024, 2048, 4096], batch_sizes=[2, 4, 8]):\n",
    "    \"\"\"Profile peak memory usage for training forward+backward.\"\"\"\n",
    "    import gc\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    print(f\"{'Seq Len':>8} | {'Batch':>5} | {'Tokens':>8} | {'Peak MB':>8} | {'% of 6GB':>8}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = []\n",
    "    for T in seq_lens:\n",
    "        for B in batch_sizes:\n",
    "            # Clear cache\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            try:\n",
    "                model.train()\n",
    "                x = torch.randint(0, 1000, (B, T), device=device)\n",
    "                \n",
    "                # Forward + backward (this is what training uses)\n",
    "                _, loss, _, _ = model(x, x)\n",
    "                loss.backward()\n",
    "                \n",
    "                peak_mb = torch.cuda.max_memory_allocated() / 1024**2\n",
    "                pct_6gb = (peak_mb / 6144) * 100\n",
    "                tokens = B * T\n",
    "                \n",
    "                print(f\"{T:>8} | {B:>5} | {tokens:>8} | {peak_mb:>8.1f} | {pct_6gb:>7.1f}%\")\n",
    "                results.append({'seq_len': T, 'batch': B, 'tokens': tokens, 'peak_mb': peak_mb})\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(f\"{T:>8} | {B:>5} | {'OOM':>8} | {'---':>8} | {'---':>8}\")\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise\n",
    "            \n",
    "            model.zero_grad()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Memory Profile for RTX 4050 (6GB)(DO NOT EXCEED 45%):\")\n",
    "print(\"=\"*50)\n",
    "memory_results = profile_memory(model, seq_lens=[512, 1024, 2048, 4096], batch_sizes=[2, 4, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh model: 33,217,560 params\n",
      "Training at seq_len=2048 throughout (no mismatch)\n",
      "============================================================\n",
      "PROGRESSIVE CURRICULUM\n",
      "============================================================\n",
      "Phase 1: Retrieval until 95% (max 200 steps)\n",
      "Phase 2: Gym 2→5 needles until 70%\n",
      "Phase 3: LM (only after gym succeeds)\n",
      "============================================================\n",
      "[RETRIEVAL] Step    0: loss=11.375, β=0.131 (4.8 s/s)\n",
      "\n",
      "[TRANSITION] Retrieval → Gym at step 22 (acc=95.7%)\n",
      "Memory Gym: 2000 samples, T=2048, 2 needles, batch=4\n",
      "[GYM ] Step   50: loss=5.451, acc=43.0%, needles=2, seq=2048, β=0.131, surv=[0.00,-0.00] (8.3 s/s)\n",
      "[GYM ] Step  100: loss=4.982, acc=3.5%, needles=2, seq=2048, β=0.134, surv=[-0.00,0.00] (6.6 s/s)\n",
      "[GYM ] Step  150: loss=3.729, acc=5.5%, needles=2, seq=2048, β=0.130, surv=[0.00,0.00] (6.5 s/s)\n",
      "[GYM ] Step  200: loss=3.658, acc=3.0%, needles=2, seq=2048, β=0.129, surv=[0.00,0.00] (6.3 s/s)\n",
      "[GYM ] Step  250: loss=3.630, acc=3.0%, needles=2, seq=2048, β=0.126, surv=[-0.00,-0.00] (6.1 s/s)\n",
      "[GYM ] Step  300: loss=3.583, acc=2.5%, needles=2, seq=2048, β=0.136, surv=[0.00,0.00] (6.2 s/s)\n",
      "[GYM ] Step  350: loss=3.577, acc=3.0%, needles=2, seq=2048, β=0.130, surv=[-0.00,0.00] (6.1 s/s)\n",
      "[GYM ] Step  400: loss=3.553, acc=4.5%, needles=2, seq=2048, β=0.127, surv=[0.00,-0.00] (6.0 s/s)\n",
      "[GYM ] Step  450: loss=3.567, acc=3.5%, needles=2, seq=2048, β=0.127, surv=[-0.00,0.00] (6.0 s/s)\n",
      "[GYM ] Step  500: loss=3.547, acc=4.0%, needles=2, seq=2048, β=0.127, surv=[-0.00,-0.00] (6.0 s/s)\n",
      "[GYM ] Step  550: loss=3.519, acc=1.5%, needles=2, seq=2048, β=0.144, surv=[0.00,0.00] (6.0 s/s)\n",
      "[GYM ] Step  600: loss=3.391, acc=5.5%, needles=2, seq=2048, β=0.231, surv=[0.00,-0.00] (5.9 s/s)\n",
      "[GYM ] Step  650: loss=3.180, acc=11.0%, needles=2, seq=2048, β=0.273, surv=[-0.00,0.00] (5.9 s/s)\n",
      "[GYM ] Step  700: loss=3.018, acc=17.0%, needles=2, seq=2048, β=0.268, surv=[0.00,-0.00] (5.9 s/s)\n",
      "[GYM ] Step  750: loss=2.810, acc=24.0%, needles=2, seq=2048, β=0.277, surv=[0.00,0.00] (5.9 s/s)\n",
      "[GYM ] Step  800: loss=2.821, acc=22.5%, needles=2, seq=2048, β=0.307, surv=[0.00,0.00] (5.9 s/s)\n",
      "[GYM ] Step  850: loss=2.792, acc=23.5%, needles=2, seq=2048, β=0.332, surv=[0.00,0.00] (5.9 s/s)\n",
      "[GYM ] Step  900: loss=2.805, acc=21.0%, needles=2, seq=2048, β=0.320, surv=[-0.00,0.00] (5.9 s/s)\n",
      "[GYM ] Step  950: loss=2.530, acc=32.5%, needles=2, seq=2048, β=0.303, surv=[0.00,0.00] (5.8 s/s)\n",
      "[GYM ] Step 1000: loss=2.706, acc=27.0%, needles=2, seq=2048, β=0.320, surv=[0.00,0.00] (5.8 s/s)\n",
      "[GYM ] Step 1050: loss=1.376, acc=68.0%, needles=2, seq=2048, β=0.320, surv=[0.00,0.00] (5.8 s/s)\n",
      "Memory Gym: 2000 samples, T=2048, 3 needles, batch=4\n",
      "\n",
      "[DIFFICULTY] Increased to 3 needles at step 1060\n",
      "[GYM ] Step 1100: loss=3.111, acc=21.0%, needles=3, seq=2048, β=0.336, surv=[0.00,0.00,-0.00] (5.8 s/s)\n",
      "[GYM ] Step 1150: loss=3.609, acc=4.0%, needles=3, seq=2048, β=0.322, surv=[-0.00,0.00,-0.00] (5.8 s/s)\n",
      "[GYM ] Step 1200: loss=3.548, acc=1.0%, needles=3, seq=2048, β=0.318, surv=[-0.00,0.00,-0.00] (5.8 s/s)\n",
      "[GYM ] Step 1250: loss=3.513, acc=2.0%, needles=3, seq=2048, β=0.295, surv=[-0.00,0.00,0.00] (5.8 s/s)\n",
      "[GYM ] Step 1300: loss=3.542, acc=3.5%, needles=3, seq=2048, β=0.301, surv=[0.00,0.00,0.00] (5.8 s/s)\n",
      "[GYM ] Step 1350: loss=3.490, acc=6.5%, needles=3, seq=2048, β=0.283, surv=[-0.00,0.00,-0.00] (5.8 s/s)\n",
      "[GYM ] Step 1400: loss=3.478, acc=4.0%, needles=3, seq=2048, β=0.303, surv=[0.00,0.00,-0.00] (5.8 s/s)\n",
      "[GYM ] Step 1450: loss=3.511, acc=3.5%, needles=3, seq=2048, β=0.299, surv=[-0.00,0.00,-0.00] (5.8 s/s)\n",
      "[GYM ] Step 1500: loss=3.487, acc=4.0%, needles=3, seq=2048, β=0.303, surv=[0.00,0.00,0.00] (5.8 s/s)\n",
      "[GYM ] Step 1550: loss=3.490, acc=2.5%, needles=3, seq=2048, β=0.295, surv=[0.00,0.00,-0.00] (5.8 s/s)\n",
      "[GYM ] Step 1600: loss=1.649, acc=64.0%, needles=3, seq=2048, β=0.406, surv=[-0.00,0.00,-0.00] (5.8 s/s)\n",
      "Memory Gym: 2000 samples, T=2048, 4 needles, batch=4\n",
      "\n",
      "[DIFFICULTY] Increased to 4 needles at step 1609\n",
      "[GYM ] Step 1650: loss=3.245, acc=20.0%, needles=4, seq=2048, β=0.348, surv=[-0.00,0.00,-0.00,0.00] (5.8 s/s)\n",
      "[GYM ] Step 1700: loss=3.629, acc=2.0%, needles=4, seq=2048, β=0.326, surv=[-0.00,0.00,0.00,0.00] (5.8 s/s)\n",
      "[GYM ] Step 1750: loss=3.547, acc=2.5%, needles=4, seq=2048, β=0.307, surv=[0.00,-0.00,0.00,-0.00] (5.8 s/s)\n",
      "[GYM ] Step 1800: loss=3.499, acc=4.0%, needles=4, seq=2048, β=0.324, surv=[-0.00,0.00,-0.00,-0.00] (5.8 s/s)\n",
      "[GYM ] Step 1850: loss=3.567, acc=3.0%, needles=4, seq=2048, β=0.305, surv=[-0.00,0.00,-0.00,-0.00] (5.8 s/s)\n",
      "[GYM ] Step 1900: loss=3.471, acc=2.5%, needles=4, seq=2048, β=0.301, surv=[-0.00,0.00,-0.00,-0.00] (5.8 s/s)\n",
      "[GYM ] Step 1950: loss=3.478, acc=4.0%, needles=4, seq=2048, β=0.283, surv=[0.00,-0.00,-0.00,-0.00] (5.8 s/s)\n",
      "[GYM ] Step 2000: loss=3.506, acc=2.5%, needles=4, seq=2048, β=0.270, surv=[0.00,0.00,-0.00,-0.00] (5.7 s/s)\n",
      "\n",
      "[TRANSITION] Gym → LM at step 2022 (max steps, acc=1.5%)\n",
      "[LM  ] Step 2050: loss=7.391, β=0.209 (5.6 s/s)\n",
      "[LM  ] Step 2100: loss=4.655, β=0.629 (5.4 s/s)\n",
      "[LM  ] Step 2150: loss=0.586, β=0.703 (5.2 s/s)\n",
      "[LM  ] Step 2200: loss=0.136, β=0.707 (5.1 s/s)\n",
      "[LM  ] Step 2250: loss=0.087, β=0.719 (5.0 s/s)\n",
      "[LM  ] Step 2300: loss=0.065, β=0.711 (4.8 s/s)\n",
      "[LM  ] Step 2350: loss=0.056, β=0.707 (4.7 s/s)\n",
      "[LM  ] Step 2400: loss=0.053, β=0.719 (4.6 s/s)\n",
      "[LM  ] Step 2450: loss=0.049, β=0.703 (4.5 s/s)\n",
      "[LM  ] Step 2500: loss=0.045, β=0.707 (4.4 s/s)\n",
      "[LM  ] Step 2550: loss=0.042, β=0.727 (4.4 s/s)\n",
      "[LM  ] Step 2600: loss=0.040, β=0.707 (4.3 s/s)\n",
      "[LM  ] Step 2650: loss=0.039, β=0.715 (4.2 s/s)\n",
      "[LM  ] Step 2700: loss=0.039, β=0.703 (4.2 s/s)\n",
      "[LM  ] Step 2750: loss=0.037, β=0.703 (4.1 s/s)\n",
      "[LM  ] Step 2800: loss=0.036, β=0.707 (4.0 s/s)\n",
      "[LM  ] Step 2850: loss=0.034, β=0.719 (4.0 s/s)\n",
      "[LM  ] Step 2900: loss=0.035, β=0.719 (3.9 s/s)\n",
      "[LM  ] Step 2950: loss=0.034, β=0.715 (3.9 s/s)\n",
      "\n",
      "============================================================\n",
      "Training complete: 3000 steps in 780.7s\n",
      "Final phase: lm\n",
      "Final β mean: 0.723\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "FINAL EVALUATION\n",
      "============================================================\n",
      "\n",
      "1. Single-needle (should be 100%):\n",
      "  Accuracy: 0.0% (0/20)\n",
      "\n",
      "2. Multi-needle (should be >70%):\n",
      "\n",
      "============================================================\n",
      "MULTI-NEEDLE TEST: 3 needles in 2048 tokens\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.0% (0/30)\n",
      "\n",
      "Per-Needle Breakdown:\n",
      "  Needle 0: 0.0% (0/0)\n",
      "  Needle 1: 0.0% (0/0)\n",
      "  Needle 2: 0.0% (0/0)\n",
      "\n",
      "Confusion Matrix (rows=query, cols=retrieved):\n",
      "         N 0 N 1 N 2 \n",
      "  Query0:   0   0   0 \n",
      "  Query1:   0   0   0 \n",
      "  Query2:   0   0   0 \n",
      "\n",
      "✗ FAIL: Model has single-slot memory (can't distinguish needles)\n",
      "\n",
      "3. Multi-needle stress test (5 needles):\n",
      "\n",
      "============================================================\n",
      "MULTI-NEEDLE TEST: 5 needles in 2048 tokens\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.0% (0/30)\n",
      "\n",
      "Per-Needle Breakdown:\n",
      "  Needle 0: 0.0% (0/0)\n",
      "  Needle 1: 0.0% (0/0)\n",
      "  Needle 2: 0.0% (0/0)\n",
      "  Needle 3: 0.0% (0/0)\n",
      "  Needle 4: 0.0% (0/0)\n",
      "\n",
      "Confusion Matrix (rows=query, cols=retrieved):\n",
      "         N 0 N 1 N 2 N 3 N 4 \n",
      "  Query0:   0   0   0   0   0 \n",
      "  Query1:   0   0   0   0   0 \n",
      "  Query2:   0   0   0   0   0 \n",
      "  Query3:   0   0   0   0   0 \n",
      "  Query4:   0   0   0   0   0 \n",
      "\n",
      "✗ FAIL: Model has single-slot memory (can't distinguish needles)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'per_needle': [{'needle_idx': 0, 'accuracy': 0, 'correct': 0, 'total': 0},\n",
       "  {'needle_idx': 1, 'accuracy': 0, 'correct': 0, 'total': 0},\n",
       "  {'needle_idx': 2, 'accuracy': 0, 'correct': 0, 'total': 0},\n",
       "  {'needle_idx': 3, 'accuracy': 0, 'correct': 0, 'total': 0},\n",
       "  {'needle_idx': 4, 'accuracy': 0, 'correct': 0, 'total': 0}],\n",
       " 'total_correct': 0,\n",
       " 'total_trials': 30,\n",
       " 'confusion_matrix': tensor([[0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0]]),\n",
       " 'overall_accuracy': 0.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PROGRESSIVE CURRICULUM: Retrieval → Gym → LM\n",
    "# =============================================================================\n",
    "# Order matters! NO language modeling until multi-needle works.\n",
    "#\n",
    "#   Phase 1 (Retrieval): Single-needle until 95%+ (quick, ~100 steps)\n",
    "#   Phase 2 (Gym):       Progressive multi-needle (2→5 needles at NEW_T)\n",
    "#                        Trains β gating to be selective\n",
    "#   Phase 3 (LM):        Language modeling ONLY after gym succeeds\n",
    "#\n",
    "# Key insight: Train at target seq_len from the start - no mismatch!\n",
    "\n",
    "import importlib\n",
    "analysis = importlib.reload(analysis)\n",
    "train_progressive_curriculum = analysis.train_progressive_curriculum\n",
    "proper_niah_test = analysis.proper_niah_test\n",
    "multi_needle_test = analysis.multi_needle_test\n",
    "\n",
    "# Fresh model\n",
    "model = TransparentHybrid(cfg).to(DEVICE).bfloat16()\n",
    "print(f\"Fresh model: {model.count_params():,} params\")\n",
    "print(f\"Training at seq_len={NEW_T} throughout (no mismatch)\")\n",
    "\n",
    "# Run progressive curriculum\n",
    "history = train_progressive_curriculum(\n",
    "    model,\n",
    "    wikitext_loader=data_loader,\n",
    "    max_steps=3000,\n",
    "    lr=3e-4,\n",
    "    log_interval=50,\n",
    "    # Phase transitions\n",
    "    retrieval_threshold=0.95,    # Move to gym when single-needle > 95%\n",
    "    gym_threshold=0.70,          # Move to LM when multi-needle > 70%\n",
    "    # Gym progression - USE NEW_T THROUGHOUT\n",
    "    gym_start_needles=2,\n",
    "    gym_max_needles=5,\n",
    "    gym_start_seq=NEW_T,         # Start at target length!\n",
    "    gym_max_seq=NEW_T,           # No seq progression\n",
    "    # Safety limits\n",
    "    max_retrieval_steps=200,\n",
    "    max_gym_steps=2000,\n",
    ")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Single-needle (should be 100%):\")\n",
    "proper_niah_test(model, seq_len=NEW_T, n_trials=20)\n",
    "\n",
    "print(\"\\n2. Multi-needle (should be >70%):\")\n",
    "multi_needle_test(model, seq_len=NEW_T, n_needles=3, n_trials=30)\n",
    "\n",
    "print(\"\\n3. Multi-needle stress test (5 needles):\")\n",
    "multi_needle_test(model, seq_len=NEW_T, n_needles=5, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec1ff757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ARCHITECTURE CHECK: Testing UNTRAINED model\n",
      "============================================================\n",
      "Fresh untrained model: 33,217,560 params\n",
      "\n",
      "============================================================\n",
      "STATE vs SWA VERIFICATION (window_size=64)\n",
      "============================================================\n",
      "\n",
      "Inside SWA window (≤64 tokens):\n",
      "  Accuracy: 0.0% (0/50)\n",
      "  → Could be SWA OR state\n",
      "\n",
      "Outside SWA window (>64 tokens):\n",
      "  Accuracy: 0.0% (0/50)\n",
      "  → MUST be state (SWA can't see)\n",
      "\n",
      "✗ NO STATE USAGE: Model relies only on SWA (0.0%)\n",
      "\n",
      "============================================================\n",
      "TRAINED MODEL CHECK\n",
      "============================================================\n",
      "Testing trained model with window_size=64\n",
      "\n",
      "============================================================\n",
      "STATE vs SWA VERIFICATION (window_size=64)\n",
      "============================================================\n",
      "\n",
      "Inside SWA window (≤64 tokens):\n",
      "  Accuracy: 2.0% (1/50)\n",
      "  → Could be SWA OR state\n",
      "\n",
      "Outside SWA window (>64 tokens):\n",
      "  Accuracy: 2.0% (1/50)\n",
      "  → MUST be state (SWA can't see)\n",
      "\n",
      "✗ NO STATE USAGE: Model relies only on SWA (2.0%)\n",
      "\n",
      "============================================================\n",
      "COMPARISON\n",
      "============================================================\n",
      "Untrained outside-window: 0.0%\n",
      "Trained outside-window:   2.0%\n",
      "\n",
      "✗ STATE NEVER WORKED (architecture issue?)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DIAGNOSTIC: Is the model using STATE or just SWA?\n",
    "# =============================================================================\n",
    "# This test places needles OUTSIDE the SWA window to verify state usage.\n",
    "# If outside_window accuracy is ~0%, model bypasses GDN state entirely.\n",
    "\n",
    "import importlib\n",
    "analysis = importlib.reload(analysis)\n",
    "test_state_vs_swa = analysis.test_state_vs_swa\n",
    "\n",
    "# First: Test UNTRAINED model to check if architecture works\n",
    "print(\"=\"*60)\n",
    "print(\"ARCHITECTURE CHECK: Testing UNTRAINED model\")\n",
    "print(\"=\"*60)\n",
    "untrained_model = TransparentHybrid(cfg).to(DEVICE).bfloat16()\n",
    "print(f\"Fresh untrained model: {untrained_model.count_params():,} params\")\n",
    "untrained_result = test_state_vs_swa(untrained_model, seq_len=NEW_T, n_trials=50)\n",
    "\n",
    "# Then: Test trained model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINED MODEL CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Testing trained model with window_size={cfg.window_size}\")\n",
    "trained_result = test_state_vs_swa(model, seq_len=NEW_T, n_trials=50)\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Untrained outside-window: {untrained_result['outside_accuracy']*100:.1f}%\")\n",
    "print(f\"Trained outside-window:   {trained_result['outside_accuracy']*100:.1f}%\")\n",
    "if trained_result['outside_accuracy'] < untrained_result['outside_accuracy']:\n",
    "    print(\"\\n⚠ TRAINING BROKE STATE RETRIEVAL\")\n",
    "elif trained_result['outside_accuracy'] > 0.5:\n",
    "    print(\"\\n✓ STATE IS WORKING\")\n",
    "else:\n",
    "    print(\"\\n✗ STATE NEVER WORKED (architecture issue?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7709a1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing TRAINED model:\n",
      "\n",
      "============================================================\n",
      "STATE MECHANISM DIAGNOSTIC\n",
      "============================================================\n",
      "\n",
      "1. STATE WRITING\n",
      "   State norm: 49.7500\n",
      "   State max:  1.4297\n",
      "   ✓ State is being written\n",
      "\n",
      "2. SINGLE-TOKEN SELF-RETRIEVAL (isolated GDN layer)\n",
      "   Avg cosine similarity: 1.0000\n",
      "   Per-head sims: ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00']\n",
      "   Magnitude ratios (ret/exp): ['0.99', '1.00', '0.02', '0.75', '0.07', '1.00', '0.96', '0.94']\n",
      "   ✓ Self-retrieval works!\n",
      "\n",
      "3. SWA RETRIEVAL PATH\n",
      "   Query norm (after ReLU): 9.1875\n",
      "   Query sparsity: 57.8% zeros\n",
      "   Retrieved norm: 14.5801\n",
      "   ✓ SWA retrieval produces non-zero output\n",
      "\n",
      "4. MULTI-TOKEN INTERFERENCE (10 tokens)\n",
      "   Avg similarity across all tokens: 0.5330\n",
      "   First token (most overwritten): 0.4084\n",
      "   Last token (most recent): 0.9766\n",
      "   Per-token: ['0.41', '0.64', '0.35', '0.44', '0.43', '0.43', '0.64', '0.55', '0.47', '0.98']\n",
      "   ✗ Multi-token retrieval fails\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "✓ All mechanisms working - architecture CAN retrieve from state\n",
      "\n",
      "\n",
      "Testing UNTRAINED model:\n",
      "\n",
      "============================================================\n",
      "STATE MECHANISM DIAGNOSTIC\n",
      "============================================================\n",
      "\n",
      "1. STATE WRITING\n",
      "   State norm: 5.6875\n",
      "   State max:  0.1553\n",
      "   ✓ State is being written\n",
      "\n",
      "2. SINGLE-TOKEN SELF-RETRIEVAL (isolated GDN layer)\n",
      "   Avg cosine similarity: 1.0000\n",
      "   Per-head sims: ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00']\n",
      "   Magnitude ratios (ret/exp): ['0.15', '0.10', '0.16', '0.12', '0.11', '0.13', '0.17', '0.08']\n",
      "   ✓ Self-retrieval works!\n",
      "\n",
      "3. SWA RETRIEVAL PATH\n",
      "   Query norm (after ReLU): 7.6875\n",
      "   Query sparsity: 51.4% zeros\n",
      "   Retrieved norm: 1.8941\n",
      "   ✓ SWA retrieval produces non-zero output\n",
      "\n",
      "4. MULTI-TOKEN INTERFERENCE (10 tokens)\n",
      "   Avg similarity across all tokens: 0.8425\n",
      "   First token (most overwritten): 0.7680\n",
      "   Last token (most recent): 0.9216\n",
      "   Per-token: ['0.77', '0.61', '0.74', '0.81', '0.87', '0.89', '0.92', '0.92', '0.96', '0.92']\n",
      "   ✓ Multi-token retrieval works!\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "✓ All mechanisms working - architecture CAN retrieve from state\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEEP DIAGNOSTIC: Test the state mechanism at component level\n",
    "# =============================================================================\n",
    "# This is a unit-test level diagnostic that checks:\n",
    "# 1. Is state being written (non-zero)?\n",
    "# 2. Can we retrieve with the same key that wrote?\n",
    "# 3. Does the SWA retrieval path work?\n",
    "\n",
    "import importlib\n",
    "analysis = importlib.reload(analysis)\n",
    "diagnose_state_mechanism = analysis.diagnose_state_mechanism\n",
    "\n",
    "# Run on trained model\n",
    "print(\"Testing TRAINED model:\")\n",
    "trained_diag = diagnose_state_mechanism(model, verbose=True)\n",
    "\n",
    "print(\"\\n\\nTesting UNTRAINED model:\")\n",
    "untrained_diag = diagnose_state_mechanism(untrained_model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38923e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UNTRAINED gate analysis:\n",
      "  Layer 0 (GDN):\n",
      "    β: mean=0.1299, std=0.0645, range=[0.022, 0.447]\n",
      "    g: mean=0.8672, std=0.0669, range=[0.559, 0.977]\n",
      "    β_bias: -2.0000 (init was -2.0)\n",
      "    g_bias: 2.0000 (init was 3.0)\n",
      "\n",
      "TRAINED gate analysis:\n",
      "  Layer 0 (GDN):\n",
      "    β: mean=0.1504, std=0.1025, range=[0.015, 0.816]\n",
      "    g: mean=0.8633, std=0.0742, range=[0.590, 0.977]\n",
      "    β_bias: -2.0000 (init was -2.0)\n",
      "    g_bias: 2.0000 (init was 3.0)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARE GATE VALUES: trained vs untrained\n",
    "# =============================================================================\n",
    "# Check if training changed β and g in ways that hurt memory\n",
    "\n",
    "import torch\n",
    "\n",
    "def analyze_gates(model, name=\"Model\"):\n",
    "    print(f\"\\n{name} gate analysis:\")\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'beta_proj') and hasattr(layer, 'g_proj'):\n",
    "            # Create random input to measure gate activation\n",
    "            with torch.no_grad():\n",
    "                x = torch.randn(1, 64, model.cfg.d_model, device=DEVICE, dtype=torch.bfloat16)\n",
    "                x_norm = layer.norm(x)\n",
    "                beta = torch.sigmoid(layer.beta_proj(x_norm))\n",
    "                g = torch.sigmoid(layer.g_proj(x_norm))\n",
    "                \n",
    "                print(f\"  Layer {i} (GDN):\")\n",
    "                print(f\"    β: mean={beta.mean():.4f}, std={beta.std():.4f}, range=[{beta.min():.3f}, {beta.max():.3f}]\")\n",
    "                print(f\"    g: mean={g.mean():.4f}, std={g.std():.4f}, range=[{g.min():.3f}, {g.max():.3f}]\")\n",
    "                \n",
    "                # Check bias values\n",
    "                beta_bias = layer.beta_proj.bias.data\n",
    "                g_bias = layer.g_proj.bias.data\n",
    "                print(f\"    β_bias: {beta_bias.mean():.4f} (init was -2.0)\")\n",
    "                print(f\"    g_bias: {g_bias.mean():.4f} (init was 3.0)\")\n",
    "\n",
    "analyze_gates(untrained_model, \"UNTRAINED\")\n",
    "analyze_gates(model, \"TRAINED\")\n",
    "\n",
    "# Key insight: \n",
    "# - β controls how much NEW info is written (high β = more writing)\n",
    "# - g controls how much OLD info is retained (high g = more retention)\n",
    "# For good memory: need low β (selective writing) + high g (retention)\n",
    "# If training increased β or decreased g, memory is hurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e70c0049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UNTRAINED - Gate values for special tokens:\n",
      "  marker_token (50251): β=0.1309, g=0.8242\n",
      "  cue_token (50250): β=0.1826, g=0.8516\n",
      "  random_token (500): β=0.1260, g=0.9062\n",
      "  random_token2 (1000): β=0.1221, g=0.9102\n",
      "\n",
      "TRAINED - Gate values for special tokens:\n",
      "  marker_token (50251): β=0.5547, g=0.9375\n",
      "  cue_token (50250): β=0.4023, g=0.8086\n",
      "  random_token (500): β=0.7148, g=0.9336\n",
      "  random_token2 (1000): β=0.7500, g=0.9727\n",
      "\n",
      "Marker token: 50251, Cue token: 50250\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WHAT β DO MARKER TOKENS PRODUCE?\n",
    "# =============================================================================\n",
    "# Check if marker/needle tokens get special treatment\n",
    "\n",
    "def check_marker_gates(model, name=\"Model\"):\n",
    "    gdn_layer = None\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'beta_proj'):\n",
    "            gdn_layer = layer\n",
    "            break\n",
    "    \n",
    "    if gdn_layer is None:\n",
    "        print(f\"{name}: No GDN layer found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{name} - Gate values for special tokens:\")\n",
    "    \n",
    "    tokens_to_check = {\n",
    "        'marker_token': cfg.marker_token,\n",
    "        'cue_token': cfg.cue_token,\n",
    "        'random_token': 500,\n",
    "        'random_token2': 1000,\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for token_name, token_id in tokens_to_check.items():\n",
    "            tok = torch.tensor([[token_id]], device=DEVICE)\n",
    "            emb = model.embed(tok)\n",
    "            x_norm = gdn_layer.norm(emb)\n",
    "            \n",
    "            beta = torch.sigmoid(gdn_layer.beta_proj(x_norm))\n",
    "            g = torch.sigmoid(gdn_layer.g_proj(x_norm))\n",
    "            \n",
    "            print(f\"  {token_name} ({token_id}): β={beta.mean():.4f}, g={g.mean():.4f}\")\n",
    "\n",
    "check_marker_gates(untrained_model, \"UNTRAINED\")\n",
    "check_marker_gates(model, \"TRAINED\")\n",
    "\n",
    "print(f\"\\nMarker token: {cfg.marker_token}, Cue token: {cfg.cue_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb2658a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UNTRAINED - Key similarity analysis:\n",
      "  Avg |cosine| between different token keys:\n",
      "    Per-head: ['0.105', '0.107', '0.102', '0.105', '0.107', '0.108', '0.106', '0.104']\n",
      "    Overall: 0.105\n",
      "    Max head: 0.108\n",
      "    Expected for random 64D vectors: 0.100\n",
      "    ⚠ Keys are LESS orthogonal than random (more interference)\n",
      "\n",
      "TRAINED - Key similarity analysis:\n",
      "  Avg |cosine| between different token keys:\n",
      "    Per-head: ['0.690', '0.593', '0.561', '0.398', '0.596', '0.384', '0.652', '0.786']\n",
      "    Overall: 0.582\n",
      "    Max head: 0.786\n",
      "    Expected for random 64D vectors: 0.100\n",
      "    ⚠ Keys are LESS orthogonal than random (more interference)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# KEY ORTHOGONALITY CHECK\n",
    "# =============================================================================\n",
    "# If keys for different tokens are orthogonal, writes don't interfere.\n",
    "# If keys are similar (high cosine), writes overwrite each other.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def check_key_similarity(model, n_tokens=50, name=\"Model\"):\n",
    "    gdn_layer = None\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'k_proj'):\n",
    "            gdn_layer = layer\n",
    "            break\n",
    "    \n",
    "    if gdn_layer is None:\n",
    "        print(f\"{name}: No GDN layer found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{name} - Key similarity analysis:\")\n",
    "    \n",
    "    # Sample random tokens and compute their keys\n",
    "    tokens = torch.randint(100, 10000, (1, n_tokens), device=DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        emb = model.embed(tokens)\n",
    "        x_norm = gdn_layer.norm(emb)\n",
    "        keys = gdn_layer.k_proj(x_norm).view(1, n_tokens, cfg.n_heads, cfg.head_dim)\n",
    "        keys = F.normalize(keys.float(), p=2, dim=-1)  # [1, T, H, K]\n",
    "        \n",
    "        # Compute pairwise cosine similarity (keys are already normalized)\n",
    "        # For each head, compute T×T similarity matrix\n",
    "        avg_sims = []\n",
    "        for h in range(cfg.n_heads):\n",
    "            k_h = keys[0, :, h, :]  # [T, K]\n",
    "            sim_matrix = k_h @ k_h.T  # [T, T] - all pairwise similarities\n",
    "            \n",
    "            # Get off-diagonal elements (exclude self-similarity)\n",
    "            mask = ~torch.eye(n_tokens, dtype=torch.bool, device=DEVICE)\n",
    "            off_diag_sims = sim_matrix[mask]\n",
    "            \n",
    "            avg_sim = off_diag_sims.abs().mean().item()\n",
    "            avg_sims.append(avg_sim)\n",
    "        \n",
    "        overall_avg = sum(avg_sims) / len(avg_sims)\n",
    "        max_avg = max(avg_sims)\n",
    "        \n",
    "        print(f\"  Avg |cosine| between different token keys:\")\n",
    "        print(f\"    Per-head: {[f'{s:.3f}' for s in avg_sims]}\")\n",
    "        print(f\"    Overall: {overall_avg:.3f}\")\n",
    "        print(f\"    Max head: {max_avg:.3f}\")\n",
    "        \n",
    "        # For perfect orthogonality, avg should be close to 0\n",
    "        # For random vectors in K dimensions, expected |cosine| ≈ sqrt(2/π)/sqrt(K)\n",
    "        expected_random = (2/3.14159)**0.5 / (cfg.head_dim**0.5)\n",
    "        print(f\"    Expected for random {cfg.head_dim}D vectors: {expected_random:.3f}\")\n",
    "        \n",
    "        if overall_avg < expected_random:\n",
    "            print(f\"    ✓ Keys are MORE orthogonal than random\")\n",
    "        else:\n",
    "            print(f\"    ⚠ Keys are LESS orthogonal than random (more interference)\")\n",
    "\n",
    "check_key_similarity(untrained_model, name=\"UNTRAINED\")\n",
    "check_key_similarity(model, name=\"TRAINED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89ebfd",
   "metadata": {},
   "source": [
    "## The Key Insight: Training Collapsed the Key Space!\n",
    "\n",
    "**Problem discovered:**\n",
    "- Untrained model: Key similarity = 0.105 (near-random, orthogonal)\n",
    "- Trained model: Key similarity = 0.582 (highly correlated, interfering)\n",
    "\n",
    "**What happened:**\n",
    "- Standard LM training optimizes for prediction, not memory\n",
    "- The model learned to project similar inputs to similar keys\n",
    "- This means EVERY token write interferes with previous writes\n",
    "- Early needles get overwritten by subsequent haystack tokens\n",
    "\n",
    "**Solution: Add regularization losses**\n",
    "1. **Key orthogonality loss**: Penalize high cosine similarity between keys\n",
    "2. **Beta sparsity loss**: Encourage low β on average (selective writing)\n",
    "\n",
    "Let's try training with these regularizations to preserve memory capability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0a532e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh model: 33.2M parameters\n",
      "\n",
      "--- INITIAL DIAGNOSTICS ---\n",
      "\n",
      "FRESH (before training) - Key similarity analysis:\n",
      "  Avg |cosine| between different token keys:\n",
      "    Per-head: ['0.107', '0.103', '0.104', '0.103', '0.106', '0.101', '0.109', '0.102']\n",
      "    Overall: 0.104\n",
      "    Max head: 0.109\n",
      "    Expected for random 64D vectors: 0.100\n",
      "    ⚠ Keys are LESS orthogonal than random (more interference)\n",
      "\n",
      "--- TRAINING WITH REGULARIZATION ---\n",
      "Training with regularization (500 steps)\n",
      "  LR: 0.0001\n",
      "  Weights: retrieval=1.0, key_orth=0.5, beta_sparse=0.3\n",
      "============================================================\n",
      "Step    0: LM=10.938, KO=0.027, BS=0.131, RET=11.438 (1.4 s/s)\n",
      "Step   50: LM=8.125, KO=0.031, BS=0.129, RET=0.961 (3.9 s/s)\n",
      "Step  100: LM=7.031, KO=0.026, BS=0.118, RET=0.590 (4.3 s/s)\n",
      "Step  150: LM=6.844, KO=0.022, BS=0.110, RET=0.400 (4.3 s/s)\n",
      "Step  200: LM=6.625, KO=0.026, BS=0.107, RET=0.312 (4.4 s/s)\n",
      "Step  250: LM=6.625, KO=0.026, BS=0.104, RET=0.238 (4.4 s/s)\n",
      "Step  300: LM=6.344, KO=0.027, BS=0.100, RET=0.176 (4.4 s/s)\n",
      "Step  350: LM=6.406, KO=0.029, BS=0.101, RET=0.129 (4.5 s/s)\n",
      "Step  400: LM=6.250, KO=0.030, BS=0.098, RET=0.096 (4.5 s/s)\n",
      "Step  450: LM=6.469, KO=0.020, BS=0.098, RET=0.073 (4.5 s/s)\n",
      "============================================================\n",
      "Training complete: 500 steps in 111.5s\n",
      "\n",
      "Final metrics:\n",
      "  Key orthogonality: 0.0236 (lower = more orthogonal)\n",
      "  Beta sparsity: 0.0972 (lower = more selective)\n",
      "\n",
      "--- POST-TRAINING DIAGNOSTICS ---\n",
      "\n",
      "FRESH (after regularized training) - Key similarity analysis:\n",
      "  Avg |cosine| between different token keys:\n",
      "    Per-head: ['0.105', '0.108', '0.107', '0.101', '0.110', '0.107', '0.106', '0.107']\n",
      "    Overall: 0.106\n",
      "    Max head: 0.110\n",
      "    Expected for random 64D vectors: 0.100\n",
      "    ⚠ Keys are LESS orthogonal than random (more interference)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FRESH MODEL WITH REGULARIZED TRAINING\n",
    "# =============================================================================\n",
    "# Create a new untrained model and train with key orthogonality + beta sparsity regularization\n",
    "\n",
    "import importlib\n",
    "import model as model_module\n",
    "analysis = importlib.reload(analysis)\n",
    "model_module = importlib.reload(model_module)\n",
    "\n",
    "from model import TransparentHybrid\n",
    "from analysis import train_with_key_reg, diagnose_state_mechanism\n",
    "\n",
    "# Create fresh model with bfloat16\n",
    "fresh_model = TransparentHybrid(cfg).to(DEVICE).to(torch.bfloat16)\n",
    "print(f\"Fresh model: {fresh_model.count_params()/1e6:.1f}M parameters\")\n",
    "\n",
    "# Check initial state\n",
    "print(\"\\n--- INITIAL DIAGNOSTICS ---\")\n",
    "check_key_similarity(fresh_model, name=\"FRESH (before training)\")\n",
    "\n",
    "# Train with regularization\n",
    "print(\"\\n--- TRAINING WITH REGULARIZATION ---\")\n",
    "history_reg = train_with_key_reg(\n",
    "    fresh_model, \n",
    "    data_loader, \n",
    "    steps=500,\n",
    "    lr=1e-4,  # Lower LR for stability\n",
    "    key_orth_weight=0.5,     # Strong orthogonality pressure\n",
    "    beta_sparsity_weight=0.3, # Moderate sparsity pressure\n",
    "    retrieval_weight=1.0,\n",
    "    log_interval=50\n",
    ")\n",
    "\n",
    "# Check after training\n",
    "print(\"\\n--- POST-TRAINING DIAGNOSTICS ---\")\n",
    "check_key_similarity(fresh_model, name=\"FRESH (after regularized training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a92b408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MECHANISM DIAGNOSTIC ---\n",
      "\n",
      "============================================================\n",
      "STATE MECHANISM DIAGNOSTIC\n",
      "============================================================\n",
      "\n",
      "1. STATE WRITING\n",
      "   State norm: 5.3750\n",
      "   State max:  0.1826\n",
      "   ✓ State is being written\n",
      "\n",
      "2. SINGLE-TOKEN SELF-RETRIEVAL (isolated GDN layer)\n",
      "   Avg cosine similarity: 1.0000\n",
      "   Per-head sims: ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00']\n",
      "   Magnitude ratios (ret/exp): ['0.07', '0.10', '0.03', '0.12', '0.04', '0.11', '0.16', '0.10']\n",
      "   ✓ Self-retrieval works!\n",
      "\n",
      "3. SWA RETRIEVAL PATH\n",
      "   Query norm (after ReLU): 7.0625\n",
      "   Query sparsity: 49.6% zeros\n",
      "   Retrieved norm: 1.9044\n",
      "   ✓ SWA retrieval produces non-zero output\n",
      "\n",
      "4. MULTI-TOKEN INTERFERENCE (10 tokens)\n",
      "   Avg similarity across all tokens: 0.7913\n",
      "   First token (most overwritten): 0.4656\n",
      "   Last token (most recent): 0.9852\n",
      "   Per-token: ['0.47', '0.71', '0.62', '0.69', '0.81', '0.86', '0.95', '0.90', '0.93', '0.99']\n",
      "   ✓ Multi-token retrieval works!\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "✓ All mechanisms working - architecture CAN retrieve from state\n",
      "\n",
      "--- MULTI-NEEDLE TEST ---\n",
      "\n",
      "============================================================\n",
      "MULTI-NEEDLE TEST: 3 needles in 512 tokens\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.0% (0/20)\n",
      "\n",
      "Per-Needle Breakdown:\n",
      "  Needle 0: 0.0% (0/0)\n",
      "  Needle 1: 0.0% (0/0)\n",
      "  Needle 2: 0.0% (0/0)\n",
      "\n",
      "Confusion Matrix (rows=query, cols=retrieved):\n",
      "         N 0 N 1 N 2 \n",
      "  Query0:   0   0   0 \n",
      "  Query1:   0   0   0 \n",
      "  Query2:   0   0   0 \n",
      "\n",
      "✗ FAIL: Model has single-slot memory (can't distinguish needles)\n",
      "\n",
      "--- COMPARE TO COLLAPSED (NO-REG) MODEL ---\n",
      "\n",
      "============================================================\n",
      "MULTI-NEEDLE TEST: 3 needles in 512 tokens\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.0% (0/20)\n",
      "\n",
      "Per-Needle Breakdown:\n",
      "  Needle 0: 0.0% (0/0)\n",
      "  Needle 1: 0.0% (0/0)\n",
      "  Needle 2: 0.0% (0/0)\n",
      "\n",
      "Confusion Matrix (rows=query, cols=retrieved):\n",
      "         N 0 N 1 N 2 \n",
      "  Query0:   0   0   0 \n",
      "  Query1:   0   0   0 \n",
      "  Query2:   0   0   0 \n",
      "\n",
      "✗ FAIL: Model has single-slot memory (can't distinguish needles)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEST REGULARIZED MODEL\n",
    "# =============================================================================\n",
    "# Check if the regularized model preserved memory capability\n",
    "\n",
    "# Deep diagnostic\n",
    "print(\"--- MECHANISM DIAGNOSTIC ---\")\n",
    "fresh_diag = diagnose_state_mechanism(fresh_model, verbose=True)\n",
    "\n",
    "# Multi-needle test  \n",
    "print(\"\\n--- MULTI-NEEDLE TEST ---\")\n",
    "from analysis import multi_needle_test\n",
    "multi_result = multi_needle_test(fresh_model, seq_len=512, n_needles=3, n_trials=20)\n",
    "\n",
    "# Compare to the collapsed (no-reg) model\n",
    "print(\"\\n--- COMPARE TO COLLAPSED (NO-REG) MODEL ---\")\n",
    "multi_result_old = multi_needle_test(model, seq_len=512, n_needles=3, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5675d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- REGULARIZED MODEL (fresh_model) ---\n",
      "\n",
      "Single needle test:\n",
      "  Accuracy: 100.0% (30/30)\n",
      "\n",
      "State vs SWA verification:\n",
      "\n",
      "============================================================\n",
      "STATE vs SWA VERIFICATION (window_size=64)\n",
      "============================================================\n",
      "\n",
      "Inside SWA window (≤64 tokens):\n",
      "  Accuracy: 0.0% (0/20)\n",
      "  → Could be SWA OR state\n",
      "\n",
      "Outside SWA window (>64 tokens):\n",
      "  Accuracy: 0.0% (0/20)\n",
      "  → MUST be state (SWA can't see)\n",
      "\n",
      "✗ NO STATE USAGE: Model relies only on SWA (0.0%)\n",
      "\n",
      "--- ORIGINAL TRAINED MODEL (model) ---\n",
      "\n",
      "Single needle test:\n",
      "  Accuracy: 0.0% (0/30)\n",
      "\n",
      "State vs SWA verification:\n",
      "\n",
      "============================================================\n",
      "STATE vs SWA VERIFICATION (window_size=64)\n",
      "============================================================\n",
      "\n",
      "Inside SWA window (≤64 tokens):\n",
      "  Accuracy: 0.0% (0/20)\n",
      "  → Could be SWA OR state\n",
      "\n",
      "Outside SWA window (>64 tokens):\n",
      "  Accuracy: 0.0% (0/20)\n",
      "  → MUST be state (SWA can't see)\n",
      "\n",
      "✗ NO STATE USAGE: Model relies only on SWA (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SINGLE NEEDLE TEST\n",
    "# =============================================================================\n",
    "# Does the basic NIAH work at all?\n",
    "\n",
    "from analysis import proper_niah_test, test_state_vs_swa\n",
    "\n",
    "print(\"--- REGULARIZED MODEL (fresh_model) ---\")\n",
    "print(\"\\nSingle needle test:\")\n",
    "niah_reg = proper_niah_test(fresh_model, seq_len=256, needle_pos=32, n_trials=30)\n",
    "\n",
    "print(\"\\nState vs SWA verification:\")\n",
    "swa_reg = test_state_vs_swa(fresh_model, seq_len=512, n_trials=20)\n",
    "\n",
    "print(\"\\n--- ORIGINAL TRAINED MODEL (model) ---\")\n",
    "print(\"\\nSingle needle test:\")\n",
    "niah_old = proper_niah_test(model, seq_len=256, needle_pos=32, n_trials=30)\n",
    "\n",
    "print(\"\\nState vs SWA verification:\")\n",
    "swa_old = test_state_vs_swa(model, seq_len=512, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcc5db24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- REGULARIZED MODEL (varied needle IDs) ---\n",
      "  Accuracy with varied needles: 0.0% (0/30)\n",
      "\n",
      "--- ORIGINAL TRAINED MODEL (varied needle IDs) ---\n",
      "  Accuracy with varied needles: 0.0% (0/30)\n",
      "\n",
      "--- UNTRAINED MODEL (varied needle IDs) ---\n",
      "  Accuracy with varied needles: 0.0% (0/30)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VERIFY RETRIEVAL VS MEMORIZATION\n",
    "# =============================================================================\n",
    "# Test if model actually retrieves or just memorized \"CUE → specific token\"\n",
    "\n",
    "def test_varied_needles(model, seq_len=256, needle_pos=32, n_trials=30):\n",
    "    \"\"\"Test with DIFFERENT needle IDs each trial to verify retrieval.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    \n",
    "    correct = 0\n",
    "    for trial in range(n_trials):\n",
    "        # Use different needle ID each trial\n",
    "        needle_id = cfg.vocab_size - 50 + (trial % 50)\n",
    "        \n",
    "        seq = torch.randint(0, cfg.vocab_size - 100, (1, seq_len), device=device)\n",
    "        seq[0, needle_pos] = cfg.marker_token\n",
    "        seq[0, needle_pos + 1] = needle_id\n",
    "        seq[0, -1] = cfg.cue_token\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _, _, _ = model(seq)\n",
    "        \n",
    "        pred = logits[0, -1].argmax().item()\n",
    "        if pred == needle_id:\n",
    "            correct += 1\n",
    "    \n",
    "    acc = correct / n_trials\n",
    "    print(f\"  Accuracy with varied needles: {acc*100:.1f}% ({correct}/{n_trials})\")\n",
    "    return acc\n",
    "\n",
    "print(\"--- REGULARIZED MODEL (varied needle IDs) ---\")\n",
    "varied_acc_reg = test_varied_needles(fresh_model, seq_len=256, needle_pos=32, n_trials=30)\n",
    "\n",
    "print(\"\\n--- ORIGINAL TRAINED MODEL (varied needle IDs) ---\")\n",
    "varied_acc_old = test_varied_needles(model, seq_len=256, needle_pos=32, n_trials=30)\n",
    "\n",
    "print(\"\\n--- UNTRAINED MODEL (varied needle IDs) ---\")\n",
    "varied_acc_untrained = test_varied_needles(untrained_model, seq_len=256, needle_pos=32, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c017fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh model v2: 33.2M parameters\n",
      "\n",
      "--- TRAINING WITH VARIED NEEDLES + REGULARIZATION ---\n",
      "Training with regularization (1000 steps)\n",
      "  LR: 0.0001\n",
      "  Weights: retrieval=2.0, key_orth=0.5, beta_sparse=0.2\n",
      "============================================================\n",
      "Step    0: LM=10.938, KO=0.033, BS=0.131, RET=11.000 (1.8 s/s)\n",
      "Step  100: LM=7.156, KO=0.024, BS=0.124, RET=10.750 (4.5 s/s)\n",
      "Step  200: LM=6.844, KO=0.027, BS=0.114, RET=9.438 (4.6 s/s)\n",
      "Step  300: LM=6.594, KO=0.021, BS=0.107, RET=8.250 (4.5 s/s)\n",
      "Step  400: LM=6.406, KO=0.030, BS=0.104, RET=8.250 (4.5 s/s)\n",
      "Step  500: LM=6.219, KO=0.027, BS=0.102, RET=8.875 (4.5 s/s)\n",
      "Step  600: LM=6.250, KO=0.022, BS=0.102, RET=7.281 (4.4 s/s)\n",
      "Step  700: LM=6.188, KO=0.025, BS=0.100, RET=7.250 (4.4 s/s)\n",
      "Step  800: LM=6.094, KO=0.046, BS=0.099, RET=6.250 (4.4 s/s)\n",
      "Step  900: LM=5.844, KO=0.028, BS=0.096, RET=5.750 (4.3 s/s)\n",
      "============================================================\n",
      "Training complete: 1000 steps in 229.8s\n",
      "\n",
      "Final metrics:\n",
      "  Key orthogonality: 0.0251 (lower = more orthogonal)\n",
      "  Beta sparsity: 0.0991 (lower = more selective)\n",
      "\n",
      "--- TEST WITH VARIED NEEDLES ---\n",
      "  Accuracy with varied needles: 0.0% (0/30)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# RETRAIN WITH FIXED RETRIEVAL LOSS (varied needle IDs)\n",
    "# =============================================================================\n",
    "# The previous training used fixed needle ID - model just memorized \"CUE → specific token\"\n",
    "# Now use varied needle IDs to force actual retrieval learning\n",
    "\n",
    "import importlib\n",
    "analysis = importlib.reload(analysis)\n",
    "from analysis import train_with_key_reg\n",
    "\n",
    "# Create a new fresh model\n",
    "fresh_model_v2 = TransparentHybrid(cfg).to(DEVICE).to(torch.bfloat16)\n",
    "print(f\"Fresh model v2: {fresh_model_v2.count_params()/1e6:.1f}M parameters\")\n",
    "\n",
    "# Train with fixed retrieval loss (varied needles) + regularization\n",
    "print(\"\\n--- TRAINING WITH VARIED NEEDLES + REGULARIZATION ---\")\n",
    "history_v2 = train_with_key_reg(\n",
    "    fresh_model_v2, \n",
    "    data_loader, \n",
    "    steps=1000,  # More steps\n",
    "    lr=1e-4,\n",
    "    key_orth_weight=0.5,\n",
    "    beta_sparsity_weight=0.2,\n",
    "    retrieval_weight=2.0,  # Higher retrieval weight\n",
    "    log_interval=100\n",
    ")\n",
    "\n",
    "# Test with varied needle IDs\n",
    "print(\"\\n--- TEST WITH VARIED NEEDLES ---\")\n",
    "varied_acc_v2 = test_varied_needles(fresh_model_v2, seq_len=256, needle_pos=32, n_trials=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea663a9",
   "metadata": {},
   "source": [
    "## THE FIX: Shifted Value Mode\n",
    "\n",
    "**The core problem was architectural:**\n",
    "- Original: Store `(k_t, v_t)` - key and value from SAME token\n",
    "- This can't learn MARKER → VALUE because at MARKER position, we haven't seen VALUE yet!\n",
    "\n",
    "**The fix:**\n",
    "- Shifted: Store `(k_t, v_{t+1})` - key from token t, value from token t+1\n",
    "- Now when we see MARKER, we store the VALUE that comes after\n",
    "- Query with key(MARKER) retrieves value(VALUE)\n",
    "\n",
    "This is the correct way to build associative memory in an autoregressive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac8e260a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shifted value mode: True\n",
      "Config: HybridConfig(GS, d=512, h=8, K=64, V=128)\n",
      "\n",
      "Shifted model: 33.2M parameters\n",
      "Forward pass OK: output=torch.Size([2, 64, 50257])\n",
      "\n",
      "--- MECHANISM DIAGNOSTIC (SHIFTED VALUE) ---\n",
      "\n",
      "============================================================\n",
      "STATE MECHANISM DIAGNOSTIC\n",
      "============================================================\n",
      "\n",
      "1. STATE WRITING\n",
      "   State norm: 5.4688\n",
      "   State max:  0.1504\n",
      "   ✓ State is being written\n",
      "\n",
      "2. SHIFTED-VALUE RETRIEVAL (key=token0, expect=value_of_token1)\n",
      "   Shifted value mode: True\n",
      "   Avg cosine similarity: 1.0000\n",
      "   Per-head sims: ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00']\n",
      "   Magnitude ratios (ret/exp): ['0.14', '0.10', '0.10', '0.15', '0.05', '0.13', '0.11', '0.15']\n",
      "   ✓ Retrieval works!\n",
      "\n",
      "3. SWA RETRIEVAL PATH\n",
      "   Query norm (after ReLU): 6.9688\n",
      "   Query sparsity: 49.4% zeros\n",
      "   Retrieved norm: 1.5725\n",
      "   ✓ SWA retrieval produces non-zero output\n",
      "\n",
      "4. MULTI-TOKEN INTERFERENCE (10 tokens)\n",
      "   Avg similarity across all tokens: 0.0501\n",
      "   First token (most overwritten): 0.0160\n",
      "   Last token (most recent): 0.2831\n",
      "   Per-token: ['0.02', '0.05', '-0.04', '0.11', '-0.04', '-0.06', '0.01', '0.03', '0.13', '0.28']\n",
      "   ✗ Multi-token retrieval fails\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "✓ All mechanisms working - architecture CAN retrieve from state\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEST SHIFTED VALUE MODE\n",
    "# =============================================================================\n",
    "# Create fresh model with shifted_value=True (now the default)\n",
    "\n",
    "import importlib\n",
    "import config as config_module\n",
    "import model as model_module\n",
    "import analysis as analysis_module\n",
    "\n",
    "config_module = importlib.reload(config_module)\n",
    "model_module = importlib.reload(model_module)\n",
    "analysis_module = importlib.reload(analysis_module)\n",
    "\n",
    "from config import HybridConfig\n",
    "from model import TransparentHybrid\n",
    "from analysis import diagnose_state_mechanism, train_with_key_reg\n",
    "\n",
    "# Create config with shifted_value=True (default)\n",
    "cfg_shifted = HybridConfig(\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    head_dim=64,\n",
    "    value_dim=128,\n",
    "    layer_pattern=\"GS\",\n",
    "    window_size=64,\n",
    "    chunk_size=64,\n",
    "    shifted_value=True,  # THE FIX\n",
    ")\n",
    "\n",
    "print(f\"Shifted value mode: {cfg_shifted.shifted_value}\")\n",
    "print(f\"Config: {cfg_shifted}\")\n",
    "\n",
    "# Create model\n",
    "shifted_model = TransparentHybrid(cfg_shifted).to(DEVICE).to(torch.bfloat16)\n",
    "print(f\"\\nShifted model: {shifted_model.count_params()/1e6:.1f}M parameters\")\n",
    "\n",
    "# Quick forward test\n",
    "x_test = torch.randint(0, 1000, (2, 64), device=DEVICE)\n",
    "with torch.no_grad():\n",
    "    logits, _, diags, state = shifted_model(x_test)\n",
    "print(f\"Forward pass OK: output={logits.shape}\")\n",
    "\n",
    "# Check the mechanism\n",
    "print(\"\\n--- MECHANISM DIAGNOSTIC (SHIFTED VALUE) ---\")\n",
    "shifted_diag = diagnose_state_mechanism(shifted_model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54b69ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- UNTRAINED SHIFTED MODEL ---\n",
      "\n",
      "1. Single needle with varied IDs:\n",
      "  Accuracy with varied needles: 0.0% (0/30)\n",
      "\n",
      "2. State vs SWA verification:\n",
      "\n",
      "============================================================\n",
      "STATE vs SWA VERIFICATION (window_size=64)\n",
      "============================================================\n",
      "\n",
      "Inside SWA window (≤64 tokens):\n",
      "  Accuracy: 0.0% (0/20)\n",
      "  → Could be SWA OR state\n",
      "\n",
      "Outside SWA window (>64 tokens):\n",
      "  Accuracy: 0.0% (0/20)\n",
      "  → MUST be state (SWA can't see)\n",
      "\n",
      "✗ NO STATE USAGE: Model relies only on SWA (0.0%)\n",
      "\n",
      "============================================================\n",
      "COMPARISON: Shifted vs Original (both untrained)\n",
      "============================================================\n",
      "Shifted model outside-window: 0.0%\n",
      "Original model outside-window: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEST SHIFTED MODEL ON NIAH (UNTRAINED)\n",
    "# =============================================================================\n",
    "# Even untrained, the architecture should show SOME retrieval capability\n",
    "# because the shifted value mode aligns key(MARKER) with value(NEEDLE)\n",
    "\n",
    "print(\"--- UNTRAINED SHIFTED MODEL ---\")\n",
    "print(\"\\n1. Single needle with varied IDs:\")\n",
    "varied_acc_shifted = test_varied_needles(shifted_model, seq_len=256, needle_pos=32, n_trials=30)\n",
    "\n",
    "print(\"\\n2. State vs SWA verification:\")\n",
    "from analysis import test_state_vs_swa\n",
    "swa_shifted = test_state_vs_swa(shifted_model, seq_len=512, n_trials=20)\n",
    "\n",
    "# The key question: Does outside-window work better now?\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Shifted vs Original (both untrained)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shifted model outside-window: {swa_shifted['outside_accuracy']*100:.1f}%\")\n",
    "print(f\"Original model outside-window: {untrained_result['outside_accuracy']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7077e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRAINING SHIFTED MODEL ---\n",
      "Training with regularization (1000 steps)\n",
      "  LR: 0.0001\n",
      "  Weights: retrieval=3.0, key_orth=0.3, beta_sparse=0.1\n",
      "============================================================\n",
      "Step    0: LM=10.938, KO=0.031, BS=0.127, RET=10.938 (0.4 s/s)\n",
      "Step  100: LM=7.125, KO=0.020, BS=0.116, RET=9.375 (4.2 s/s)\n",
      "Step  200: LM=6.875, KO=0.025, BS=0.114, RET=8.875 (4.3 s/s)\n",
      "Step  300: LM=6.438, KO=0.024, BS=0.111, RET=9.125 (4.3 s/s)\n",
      "Step  400: LM=6.438, KO=0.025, BS=0.110, RET=7.938 (4.3 s/s)\n",
      "Step  500: LM=6.500, KO=0.019, BS=0.111, RET=7.531 (4.4 s/s)\n",
      "Step  600: LM=6.375, KO=0.023, BS=0.110, RET=7.219 (4.4 s/s)\n",
      "Step  700: LM=6.281, KO=0.026, BS=0.108, RET=6.688 (4.4 s/s)\n",
      "Step  800: LM=5.906, KO=0.027, BS=0.105, RET=6.500 (4.4 s/s)\n",
      "Step  900: LM=5.938, KO=0.028, BS=0.105, RET=6.250 (4.4 s/s)\n",
      "============================================================\n",
      "Training complete: 1000 steps in 228.3s\n",
      "\n",
      "Final metrics:\n",
      "  Key orthogonality: 0.0237 (lower = more orthogonal)\n",
      "  Beta sparsity: 0.1050 (lower = more selective)\n",
      "\n",
      "--- POST-TRAINING TEST ---\n",
      "\n",
      "1. Varied needle test:\n",
      "  Accuracy with varied needles: 0.0% (0/30)\n",
      "\n",
      "2. State vs SWA:\n",
      "\n",
      "============================================================\n",
      "STATE vs SWA VERIFICATION (window_size=64)\n",
      "============================================================\n",
      "\n",
      "Inside SWA window (≤64 tokens):\n",
      "  Accuracy: 0.0% (0/20)\n",
      "  → Could be SWA OR state\n",
      "\n",
      "Outside SWA window (>64 tokens):\n",
      "  Accuracy: 0.0% (0/20)\n",
      "  → MUST be state (SWA can't see)\n",
      "\n",
      "✗ NO STATE USAGE: Model relies only on SWA (0.0%)\n",
      "\n",
      "3. Key similarity (should stay low):\n",
      "\n",
      "SHIFTED (after training) - Key similarity analysis:\n",
      "  Avg |cosine| between different token keys:\n",
      "    Per-head: ['0.108', '0.108', '0.113', '0.107', '0.106', '0.110', '0.108', '0.104']\n",
      "    Overall: 0.108\n",
      "    Max head: 0.113\n",
      "    Expected for random 64D vectors: 0.100\n",
      "    ⚠ Keys are LESS orthogonal than random (more interference)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRAIN SHIFTED MODEL\n",
    "# =============================================================================\n",
    "# Now train with regularization + varied needle IDs\n",
    "# The shifted value architecture should now be CAPABLE of learning retrieval\n",
    "\n",
    "print(\"--- TRAINING SHIFTED MODEL ---\")\n",
    "history_shifted = train_with_key_reg(\n",
    "    shifted_model, \n",
    "    data_loader, \n",
    "    steps=1000,\n",
    "    lr=1e-4,\n",
    "    key_orth_weight=0.3,      # Moderate orthogonality\n",
    "    beta_sparsity_weight=0.1, # Light sparsity \n",
    "    retrieval_weight=3.0,     # Strong retrieval signal\n",
    "    log_interval=100\n",
    ")\n",
    "\n",
    "# Test after training\n",
    "print(\"\\n--- POST-TRAINING TEST ---\")\n",
    "print(\"\\n1. Varied needle test:\")\n",
    "varied_acc_shifted_trained = test_varied_needles(shifted_model, seq_len=256, needle_pos=32, n_trials=30)\n",
    "\n",
    "print(\"\\n2. State vs SWA:\")\n",
    "swa_shifted_trained = test_state_vs_swa(shifted_model, seq_len=512, n_trials=20)\n",
    "\n",
    "print(\"\\n3. Key similarity (should stay low):\")\n",
    "check_key_similarity(shifted_model, name=\"SHIFTED (after training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28f14942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DEBUG: Shifted trained model ---\n",
      "Test setup:\n",
      "  Needle ID: 50207\n",
      "  Marker token: 50251 at pos 32\n",
      "  Cue token: 50250 at pos 255\n",
      "  Distance: 223 tokens\n",
      "\n",
      "Prediction at CUE position:\n",
      "  Predicted: 50164\n",
      "  Expected:  50207\n",
      "  Correct:   False\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Token 50164:  0.90%\n",
      "  2. Token 50186:  0.79%\n",
      "  3. Token 50183:  0.70%\n",
      "  4. Token 50178:  0.60%\n",
      "  5. Token 50169:  0.53%\n",
      "\n",
      "Needle ID (50207) probability: 0.0006%\n",
      "\n",
      "State diagnostics:\n",
      "  State norm: 10.5625\n",
      "  State max:  0.2695\n",
      "  Layer 0 (G): β=0.1562\n",
      "\n",
      "\n",
      "--- DEBUG: Original untrained model ---\n",
      "Test setup:\n",
      "  Needle ID: 50207\n",
      "  Marker token: 50251 at pos 32\n",
      "  Cue token: 50250 at pos 255\n",
      "  Distance: 223 tokens\n",
      "\n",
      "Prediction at CUE position:\n",
      "  Predicted: 9810\n",
      "  Expected:  50207\n",
      "  Correct:   False\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Token  9810:  0.01%\n",
      "  2. Token 33057:  0.01%\n",
      "  3. Token 49519:  0.01%\n",
      "  4. Token 18198:  0.01%\n",
      "  5. Token 44512:  0.01%\n",
      "\n",
      "Needle ID (50207) probability: 0.0008%\n",
      "\n",
      "State diagnostics:\n",
      "  State norm: 5.3750\n",
      "  State max:  0.2432\n",
      "  Layer 0 (G): β=0.1318\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEBUG: What is the model actually predicting?\n",
    "# =============================================================================\n",
    "\n",
    "def debug_niah_prediction(model, seq_len=256, needle_pos=32):\n",
    "    \"\"\"Debug a single NIAH prediction to see what's happening.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    \n",
    "    # Create test sequence\n",
    "    needle_id = cfg.vocab_size - 50\n",
    "    seq = torch.randint(0, cfg.vocab_size - 100, (1, seq_len), device=device)\n",
    "    seq[0, needle_pos] = cfg.marker_token\n",
    "    seq[0, needle_pos + 1] = needle_id\n",
    "    seq[0, -1] = cfg.cue_token\n",
    "    \n",
    "    print(f\"Test setup:\")\n",
    "    print(f\"  Needle ID: {needle_id}\")\n",
    "    print(f\"  Marker token: {cfg.marker_token} at pos {needle_pos}\")\n",
    "    print(f\"  Cue token: {cfg.cue_token} at pos {seq_len-1}\")\n",
    "    print(f\"  Distance: {seq_len - 1 - needle_pos} tokens\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, _, diags, final_state = model(seq)\n",
    "    \n",
    "    # Get prediction at CUE position\n",
    "    pred_logits = logits[0, -1]  # [vocab_size]\n",
    "    pred_token = pred_logits.argmax().item()\n",
    "    pred_probs = torch.softmax(pred_logits.float(), dim=-1)\n",
    "    \n",
    "    # Top 5 predictions\n",
    "    top5_probs, top5_tokens = pred_probs.topk(5)\n",
    "    \n",
    "    print(f\"\\nPrediction at CUE position:\")\n",
    "    print(f\"  Predicted: {pred_token}\")\n",
    "    print(f\"  Expected:  {needle_id}\")\n",
    "    print(f\"  Correct:   {pred_token == needle_id}\")\n",
    "    \n",
    "    print(f\"\\nTop 5 predictions:\")\n",
    "    for i, (prob, tok) in enumerate(zip(top5_probs, top5_tokens)):\n",
    "        marker = \" ← NEEDLE\" if tok.item() == needle_id else \"\"\n",
    "        marker = \" ← MARKER\" if tok.item() == cfg.marker_token else marker\n",
    "        marker = \" ← CUE\" if tok.item() == cfg.cue_token else marker\n",
    "        print(f\"  {i+1}. Token {tok.item():5d}: {prob.item()*100:5.2f}%{marker}\")\n",
    "    \n",
    "    # Check probability of correct answer\n",
    "    needle_prob = pred_probs[needle_id].item()\n",
    "    print(f\"\\nNeedle ID ({needle_id}) probability: {needle_prob*100:.4f}%\")\n",
    "    \n",
    "    # Check state\n",
    "    print(f\"\\nState diagnostics:\")\n",
    "    print(f\"  State norm: {final_state.norm().item():.4f}\")\n",
    "    print(f\"  State max:  {final_state.abs().max().item():.4f}\")\n",
    "    \n",
    "    # Check layer diagnostics\n",
    "    for d in diags:\n",
    "        if 'beta_mean' in d:\n",
    "            print(f\"  Layer {d['layer_idx']} ({d['layer']}): β={d['beta_mean']:.4f}\")\n",
    "\n",
    "print(\"--- DEBUG: Shifted trained model ---\")\n",
    "debug_niah_prediction(shifted_model)\n",
    "\n",
    "print(\"\\n\\n--- DEBUG: Original untrained model ---\")\n",
    "debug_niah_prediction(untrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3d0a79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PURE RETRIEVAL TRAINING ---\n",
      "Step    0: RET=10.6250\n",
      "Step   50: RET=5.8125\n",
      "Step  100: RET=4.1562\n",
      "Step  150: RET=4.4688\n",
      "Step  200: RET=3.9844\n",
      "Step  250: RET=3.6875\n",
      "Step  300: RET=3.7188\n",
      "Step  350: RET=2.2344\n",
      "Step  400: RET=3.6719\n",
      "Step  450: RET=1.8828\n",
      "\n",
      "--- TEST AFTER PURE RETRIEVAL TRAINING ---\n",
      "  Accuracy with varied needles: 0.0% (0/30)\n",
      "\n",
      "Debug prediction:\n",
      "Test setup:\n",
      "  Needle ID: 50207\n",
      "  Marker token: 50251 at pos 32\n",
      "  Cue token: 50250 at pos 255\n",
      "  Distance: 223 tokens\n",
      "\n",
      "Prediction at CUE position:\n",
      "  Predicted: 50163\n",
      "  Expected:  50207\n",
      "  Correct:   False\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Token 50163:  4.27%\n",
      "  2. Token 50204:  4.27%\n",
      "  3. Token 50183:  3.89%\n",
      "  4. Token 50158:  3.89%\n",
      "  5. Token 50178:  3.54%\n",
      "\n",
      "Needle ID (50207) probability: 0.0000%\n",
      "\n",
      "State diagnostics:\n",
      "  State norm: 2.9531\n",
      "  State max:  0.2256\n",
      "  Layer 0 (G): β=0.0272\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PURE RETRIEVAL TRAINING (No LM distraction)\n",
    "# =============================================================================\n",
    "# The issue: LM loss dominates, retrieval doesn't learn\n",
    "# Solution: Train ONLY on retrieval first, then add LM\n",
    "\n",
    "import importlib\n",
    "analysis = importlib.reload(analysis)\n",
    "from analysis import compute_retrieval_loss\n",
    "\n",
    "# Fresh shifted model\n",
    "cfg_shifted2 = HybridConfig(\n",
    "    d_model=512, n_heads=8, head_dim=64, value_dim=128,\n",
    "    layer_pattern=\"GS\", window_size=64, chunk_size=64,\n",
    "    shifted_value=True,\n",
    ")\n",
    "shifted_model2 = TransparentHybrid(cfg_shifted2).to(DEVICE).to(torch.bfloat16)\n",
    "\n",
    "# PURE RETRIEVAL TRAINING\n",
    "print(\"--- PURE RETRIEVAL TRAINING ---\")\n",
    "optimizer = torch.optim.AdamW(shifted_model2.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "shifted_model2.train()\n",
    "for step in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Only retrieval loss - no LM, no regularization\n",
    "    ret_loss = compute_retrieval_loss(shifted_model2, seq_len=256, batch_size=8)\n",
    "    ret_loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(shifted_model2.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step:4d}: RET={ret_loss.item():.4f}\")\n",
    "\n",
    "# Test\n",
    "print(\"\\n--- TEST AFTER PURE RETRIEVAL TRAINING ---\")\n",
    "shifted_model2.eval()\n",
    "varied_acc_pure = test_varied_needles(shifted_model2, seq_len=256, needle_pos=32, n_trials=30)\n",
    "\n",
    "print(\"\\nDebug prediction:\")\n",
    "debug_niah_prediction(shifted_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4843ce3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PURE RETRIEVAL TRAINING (wide vocab) ---\n",
      "Step    0: RET=10.8750\n",
      "Step  100: RET=10.8125\n",
      "Step  200: RET=11.0000\n",
      "Step  300: RET=11.0000\n",
      "Step  400: RET=11.1250\n",
      "Step  500: RET=11.3125\n",
      "Step  600: RET=11.0000\n",
      "Step  700: RET=10.6875\n",
      "Step  800: RET=9.1250\n",
      "Step  900: RET=6.7500\n",
      "\n",
      "--- TEST ---\n",
      "  Accuracy (full vocab range): 98.0% (49/50)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PURE RETRIEVAL TRAINING v2 (Wide vocab range)\n",
    "# =============================================================================\n",
    "# Previous issue: Model learned \"predict high vocab after CUE\" shortcut\n",
    "# Fix: Use needle IDs from FULL vocab range\n",
    "\n",
    "import importlib\n",
    "analysis = importlib.reload(analysis)\n",
    "from analysis import compute_retrieval_loss\n",
    "\n",
    "# Fresh shifted model\n",
    "cfg_shifted3 = HybridConfig(\n",
    "    d_model=512, n_heads=8, head_dim=64, value_dim=128,\n",
    "    layer_pattern=\"GS\", window_size=64, chunk_size=64,\n",
    "    shifted_value=True,\n",
    ")\n",
    "shifted_model3 = TransparentHybrid(cfg_shifted3).to(DEVICE).to(torch.bfloat16)\n",
    "\n",
    "def test_varied_needles_wide(model, seq_len=256, needle_pos=32, n_trials=30):\n",
    "    \"\"\"Test with needle IDs from FULL vocab range.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    \n",
    "    correct = 0\n",
    "    for trial in range(n_trials):\n",
    "        # Use needle from FULL range (100 to vocab_size-100)\n",
    "        needle_id = torch.randint(100, cfg.vocab_size - 100, (1,)).item()\n",
    "        \n",
    "        seq = torch.randint(0, cfg.vocab_size - 100, (1, seq_len), device=device)\n",
    "        seq[0, needle_pos] = cfg.marker_token\n",
    "        seq[0, needle_pos + 1] = needle_id\n",
    "        seq[0, -1] = cfg.cue_token\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _, _, _ = model(seq)\n",
    "        \n",
    "        pred = logits[0, -1].argmax().item()\n",
    "        if pred == needle_id:\n",
    "            correct += 1\n",
    "    \n",
    "    acc = correct / n_trials\n",
    "    print(f\"  Accuracy (full vocab range): {acc*100:.1f}% ({correct}/{n_trials})\")\n",
    "    return acc\n",
    "\n",
    "# PURE RETRIEVAL TRAINING\n",
    "print(\"--- PURE RETRIEVAL TRAINING (wide vocab) ---\")\n",
    "optimizer = torch.optim.AdamW(shifted_model3.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "shifted_model3.train()\n",
    "for step in range(1000):  # More steps\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    ret_loss = compute_retrieval_loss(shifted_model3, seq_len=256, batch_size=8)\n",
    "    ret_loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(shifted_model3.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step:4d}: RET={ret_loss.item():.4f}\")\n",
    "\n",
    "# Test\n",
    "print(\"\\n--- TEST ---\")\n",
    "shifted_model3.eval()\n",
    "acc_wide = test_varied_needles_wide(shifted_model3, seq_len=256, needle_pos=32, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae059895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFYING RETRIEVAL MECHANISM ===\n",
      "\n",
      "1. DIFFERENT NEEDLE POSITIONS:\n",
      "  Accuracy (full vocab range): 96.7% (29/30)\n",
      "     Needle at position 8: 96.7%\n",
      "  Accuracy (full vocab range): 100.0% (30/30)\n",
      "     Needle at position 32: 100.0%\n",
      "  Accuracy (full vocab range): 96.7% (29/30)\n",
      "     Needle at position 64: 96.7%\n",
      "  Accuracy (full vocab range): 93.3% (28/30)\n",
      "     Needle at position 128: 93.3%\n",
      "\n",
      "2. DIFFERENT SEQUENCE LENGTHS:\n",
      "  Accuracy (full vocab range): 96.7% (29/30)\n",
      "     Seq len 128: 96.7%\n",
      "  Accuracy (full vocab range): 100.0% (30/30)\n",
      "     Seq len 256: 100.0%\n",
      "  Accuracy (full vocab range): 93.3% (28/30)\n",
      "     Seq len 384: 93.3%\n",
      "  Accuracy (full vocab range): 90.0% (27/30)\n",
      "     Seq len 512: 90.0%\n",
      "\n",
      "3. ABLATION - NO MARKER (should fail):\n",
      "   Accuracy WITHOUT marker: 0.0%\n",
      "\n",
      "4. GATE (β) VALUES:\n",
      "   Available keys: dict_keys(['beta_mean', 'beta_max', 'g_mean', 'state_norm', 'state_max', 'layer', 'layer_idx'])\n",
      "\n",
      "==================================================\n",
      "CONCLUSION: Real retrieval mechanism is working!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VERIFY RETRIEVAL MECHANISM\n",
    "# =============================================================================\n",
    "# Confirm this is REAL state-based retrieval, not another shortcut\n",
    "\n",
    "print(\"=== VERIFYING RETRIEVAL MECHANISM ===\\n\")\n",
    "\n",
    "# Test 1: Different needle positions\n",
    "print(\"1. DIFFERENT NEEDLE POSITIONS:\")\n",
    "for needle_pos in [8, 32, 64, 128]:\n",
    "    acc = test_varied_needles_wide(shifted_model3, seq_len=256, needle_pos=needle_pos, n_trials=30)\n",
    "    print(f\"     Needle at position {needle_pos}: {acc*100:.1f}%\")\n",
    "\n",
    "# Test 2: Different sequence lengths  \n",
    "print(\"\\n2. DIFFERENT SEQUENCE LENGTHS:\")\n",
    "for seq_len in [128, 256, 384, 512]:\n",
    "    acc = test_varied_needles_wide(shifted_model3, seq_len=seq_len, needle_pos=32, n_trials=30)\n",
    "    print(f\"     Seq len {seq_len}: {acc*100:.1f}%\")\n",
    "\n",
    "# Test 3: Ablation - what happens without the MARKER?\n",
    "print(\"\\n3. ABLATION - NO MARKER (should fail):\")\n",
    "def test_no_marker(model, seq_len=256, needle_pos=32, n_trials=30):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    \n",
    "    correct = 0\n",
    "    for trial in range(n_trials):\n",
    "        needle_id = torch.randint(100, cfg.vocab_size - 100, (1,)).item()\n",
    "        \n",
    "        seq = torch.randint(0, cfg.vocab_size - 100, (1, seq_len), device=device)\n",
    "        # NO MARKER - just place needle at position\n",
    "        seq[0, needle_pos + 1] = needle_id\n",
    "        seq[0, -1] = cfg.cue_token\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _, _, _ = model(seq)\n",
    "        \n",
    "        pred = logits[0, -1].argmax().item()\n",
    "        if pred == needle_id:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / n_trials\n",
    "\n",
    "acc_no_marker = test_no_marker(shifted_model3, n_trials=50)\n",
    "print(f\"   Accuracy WITHOUT marker: {acc_no_marker*100:.1f}%\")\n",
    "\n",
    "# Test 4: Check β (gate) values during forward\n",
    "print(\"\\n4. GATE (β) VALUES:\")\n",
    "cfg = shifted_model3.cfg\n",
    "seq = torch.randint(0, cfg.vocab_size - 100, (1, 128), device=DEVICE)\n",
    "seq[0, 16] = cfg.marker_token\n",
    "seq[0, 32] = cfg.cue_token\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, _, layer_states, _ = shifted_model3(seq)\n",
    "\n",
    "# layer_states is a list of dicts\n",
    "gdn_state = layer_states[0]  # First layer (GDN)\n",
    "print(f\"   Available keys: {gdn_state.keys()}\")\n",
    "# Use 'beta' instead of 'β'\n",
    "if 'beta' in gdn_state:\n",
    "    beta = gdn_state['beta'][0].float()  # [T, H]\n",
    "    print(f\"   β at marker position:  {beta[16].mean().item():.4f}\")\n",
    "    print(f\"   β at regular position: {beta[8].mean().item():.4f}\")\n",
    "    print(f\"   β at cue position:     {beta[32].mean().item():.4f}\")\n",
    "    print(f\"   Mean β overall:        {beta.mean().item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONCLUSION: Real retrieval mechanism is working!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c742f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PART 1: STATE CAPACITY ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Theoretical state capacity: 65,536 floats/layer\n",
      "  = 8 heads × 64 keys × 128 values\n",
      "\n",
      "State matrix shape: torch.Size([8, 64, 128])\n",
      "State matrix stats:\n",
      "  Mean: -0.000254\n",
      "  Std:  0.075182\n",
      "  Max:  1.351562\n",
      "\n",
      "Effective rank per head (via SVD):\n",
      "  Head 0: rank=6, top singular=6.3107\n",
      "  Head 1: rank=14, top singular=7.9908\n",
      "  Head 2: rank=3, top singular=2.8393\n",
      "  Head 3: rank=6, top singular=14.8318\n",
      "  Head 4: rank=5, top singular=1.0326\n",
      "  Head 5: rank=6, top singular=0.6762\n",
      "  Head 6: rank=1, top singular=2.1289\n",
      "  Head 7: rank=6, top singular=0.9176\n",
      "\n",
      "UNTRAINED state stats:\n",
      "  Std: 0.023769\n",
      "  Max: 0.179688\n",
      "\n",
      "TRAINED state stats:\n",
      "  Std: 0.075182\n",
      "  Max: 1.351562\n",
      "\n",
      "State activity ratio (trained/untrained): 3.16x\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2: ADD LM TO RETRIEVAL-TRAINED MODEL\n",
    "# =============================================================================\n",
    "# shifted_model3 already has 98% retrieval. Now add LM capability.\n",
    "\n",
    "print(\"Starting with retrieval-trained model (98% NIAH)\")\n",
    "print(\"Adding structured LM training...\\n\")\n",
    "\n",
    "# Use shifted_model3 (already trained on retrieval)\n",
    "# Lower LR to not destroy retrieval\n",
    "optimizer = torch.optim.AdamW(shifted_model3.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# Structured patterns (learnable, unlike random)\n",
    "def make_structured_batch(batch_size=16, seq_len=128):\n",
    "    \"\"\"Patterns the model can actually learn.\"\"\"\n",
    "    x = torch.zeros(batch_size, seq_len, dtype=torch.long, device=DEVICE)\n",
    "    for b in range(batch_size):\n",
    "        # Repeating pattern: token followed by token+1\n",
    "        base = torch.randint(100, 1000, (seq_len // 2,), device=DEVICE)\n",
    "        for i, t in enumerate(base):\n",
    "            if 2*i+1 < seq_len:\n",
    "                x[b, 2*i] = t\n",
    "                x[b, 2*i+1] = (t + 1) % 1000 + 100\n",
    "    return x\n",
    "\n",
    "shifted_model3.train()\n",
    "for step in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x = make_structured_batch(batch_size=16, seq_len=128)\n",
    "    targets = x[:, 1:].contiguous()\n",
    "    \n",
    "    logits, loss, _, _ = shifted_model3(x[:, :-1], targets=targets)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(shifted_model3.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step}: LM loss = {loss.item():.4f}\")\n",
    "\n",
    "# Check retrieval still works\n",
    "print(\"\\n--- RETRIEVAL CHECK ---\")\n",
    "acc = test_varied_needles_wide(shifted_model3, seq_len=256, needle_pos=32, n_trials=30)\n",
    "print(f\"Retrieval after LM phase: {acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62c6dadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PART 2: JOINT LM + RETRIEVAL TRAINING\n",
      "============================================================\n",
      "\n",
      "Model params: 33,217,560\n",
      "\n",
      "--- Training Loop (LM + Retrieval) ---\n",
      "Step    0: LM=10.9375, RET=10.8750\n",
      "Step  100: LM=10.9375, RET=10.9375\n",
      "Step  200: LM=10.9375, RET=11.2500\n",
      "Step  300: LM=10.9375, RET=10.8750\n",
      "Step  400: LM=10.9375, RET=10.6250\n",
      "\n",
      "--- EVALUATION ---\n",
      "LM Perplexity: 56320.00\n",
      "  Accuracy (full vocab range): 0.0% (0/50)\n",
      "Retrieval Accuracy: 0.0%\n",
      "\n",
      "[Comparison] Pure retrieval model: 98%\n",
      "[Comparison] Joint model: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 2: JOINT LM + RETRIEVAL TRAINING\n",
    "# =============================================================================\n",
    "# The key question: Can the model do BOTH tasks simultaneously?\n",
    "# - Language modeling (next token prediction on regular text)\n",
    "# - Needle retrieval (MARKER → value storage → CUE → retrieval)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 2: JOINT LM + RETRIEVAL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from analysis import compute_retrieval_loss\n",
    "\n",
    "# Start fresh with a new model\n",
    "cfg_joint = HybridConfig(\n",
    "    d_model=512, n_heads=8, head_dim=64, value_dim=128,\n",
    "    layer_pattern=\"GS\", window_size=64, chunk_size=64,\n",
    "    shifted_value=True,\n",
    ")\n",
    "joint_model = TransparentHybrid(cfg_joint).to(DEVICE).to(torch.bfloat16)\n",
    "\n",
    "print(f\"\\nModel params: {joint_model.count_params():,}\")\n",
    "\n",
    "# Mixed training: LM + Retrieval\n",
    "optimizer = torch.optim.AdamW(joint_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "# LM data (simple random for now - replace with real data for better results)\n",
    "def get_lm_batch(batch_size=8, seq_len=256):\n",
    "    \"\"\"Generate random sequences for LM training.\"\"\"\n",
    "    x = torch.randint(0, cfg_joint.vocab_size - 100, (batch_size, seq_len), device=DEVICE)\n",
    "    targets = x.clone()\n",
    "    targets[:, :-1] = x[:, 1:]  # Shift for next-token prediction\n",
    "    targets[:, -1] = -100  # Ignore last position\n",
    "    return x, targets\n",
    "\n",
    "print(\"\\n--- Training Loop (LM + Retrieval) ---\")\n",
    "lm_losses = []\n",
    "ret_losses = []\n",
    "ret_weight = 1.0  # Balance retrieval vs LM\n",
    "\n",
    "for step in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    joint_model.train()\n",
    "    \n",
    "    # LM loss\n",
    "    x, targets = get_lm_batch()\n",
    "    logits, lm_loss, _, _ = joint_model(x, targets=targets)\n",
    "    \n",
    "    # Retrieval loss\n",
    "    ret_loss = compute_retrieval_loss(joint_model, seq_len=256, batch_size=4)\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = lm_loss + ret_weight * ret_loss\n",
    "    total_loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(joint_model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    lm_losses.append(lm_loss.item())\n",
    "    ret_losses.append(ret_loss.item())\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step:4d}: LM={lm_loss.item():.4f}, RET={ret_loss.item():.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n--- EVALUATION ---\")\n",
    "joint_model.eval()\n",
    "\n",
    "# Test LM perplexity\n",
    "with torch.no_grad():\n",
    "    x, targets = get_lm_batch(batch_size=32)\n",
    "    _, lm_loss_eval, _, _ = joint_model(x, targets=targets)\n",
    "    ppl = torch.exp(lm_loss_eval).item()\n",
    "    print(f\"LM Perplexity: {ppl:.2f}\")\n",
    "\n",
    "# Test retrieval\n",
    "acc_joint = test_varied_needles_wide(joint_model, seq_len=256, needle_pos=32, n_trials=50)\n",
    "print(f\"Retrieval Accuracy: {acc_joint*100:.1f}%\")\n",
    "\n",
    "# Compare with pure retrieval model\n",
    "print(f\"\\n[Comparison] Pure retrieval model: 98%\")\n",
    "print(f\"[Comparison] Joint model: {acc_joint*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01531cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PART 2b: CURRICULUM TRAINING\n",
      "============================================================\n",
      "\n",
      "--- PHASE 1: LM only (500 steps) ---\n",
      "Step    0: LM=10.9375\n",
      "Step  100: LM=10.9375\n",
      "Step  200: LM=10.9375\n",
      "Step  300: LM=10.9375\n",
      "Step  400: LM=10.8750\n",
      "After Phase 1 LM loss: 10.8750\n",
      "  Accuracy (full vocab range): 0.0% (0/30)\n",
      "After Phase 1 Retrieval: 0.0%\n",
      "\n",
      "--- PHASE 2: LM + Retrieval (500 steps) ---\n",
      "Step    0: LM=10.8750, RET=10.5000\n",
      "Step  100: LM=10.8750, RET=11.2500\n",
      "Step  200: LM=10.8750, RET=10.9375\n",
      "Step  300: LM=10.9375, RET=10.7500\n",
      "Step  400: LM=10.9375, RET=10.8750\n",
      "\n",
      "--- FINAL EVALUATION ---\n",
      "Final LM loss: 10.9375\n",
      "  Accuracy (full vocab range): 0.0% (0/50)\n",
      "Final Retrieval: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 2b: CURRICULUM - LM FIRST, THEN RETRIEVAL\n",
    "# =============================================================================\n",
    "# Maybe joint training from scratch is too hard.\n",
    "# Try: First train LM, then add retrieval gradually\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 2b: CURRICULUM TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fresh model\n",
    "cfg_curr = HybridConfig(\n",
    "    d_model=512, n_heads=8, head_dim=64, value_dim=128,\n",
    "    layer_pattern=\"GS\", window_size=64, chunk_size=64,\n",
    "    shifted_value=True,\n",
    ")\n",
    "curriculum_model = TransparentHybrid(cfg_curr).to(DEVICE).to(torch.bfloat16)\n",
    "\n",
    "optimizer = torch.optim.AdamW(curriculum_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "# Phase 1: Pure LM (500 steps)\n",
    "print(\"\\n--- PHASE 1: LM only (500 steps) ---\")\n",
    "for step in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    curriculum_model.train()\n",
    "    \n",
    "    x, targets = get_lm_batch()\n",
    "    _, lm_loss, _, _ = curriculum_model(x, targets=targets)\n",
    "    lm_loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(curriculum_model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step:4d}: LM={lm_loss.item():.4f}\")\n",
    "\n",
    "# Check LM\n",
    "with torch.no_grad():\n",
    "    curriculum_model.eval()\n",
    "    x, targets = get_lm_batch(batch_size=32)\n",
    "    _, lm_loss_eval, _, _ = curriculum_model(x, targets=targets)\n",
    "    print(f\"After Phase 1 LM loss: {lm_loss_eval.item():.4f}\")\n",
    "\n",
    "# Check retrieval (should be near 0 since not trained)\n",
    "acc_phase1 = test_varied_needles_wide(curriculum_model, n_trials=30)\n",
    "print(f\"After Phase 1 Retrieval: {acc_phase1*100:.1f}%\")\n",
    "\n",
    "# Phase 2: Add retrieval\n",
    "print(\"\\n--- PHASE 2: LM + Retrieval (500 steps) ---\")\n",
    "for step in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    curriculum_model.train()\n",
    "    \n",
    "    # LM loss\n",
    "    x, targets = get_lm_batch()\n",
    "    _, lm_loss, _, _ = curriculum_model(x, targets=targets)\n",
    "    \n",
    "    # Retrieval loss\n",
    "    ret_loss = compute_retrieval_loss(curriculum_model, seq_len=256, batch_size=4)\n",
    "    \n",
    "    # Combined - higher retrieval weight since LM already learned basics\n",
    "    total_loss = lm_loss + 2.0 * ret_loss\n",
    "    total_loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(curriculum_model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step:4d}: LM={lm_loss.item():.4f}, RET={ret_loss.item():.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n--- FINAL EVALUATION ---\")\n",
    "curriculum_model.eval()\n",
    "with torch.no_grad():\n",
    "    x, targets = get_lm_batch(batch_size=32)\n",
    "    _, lm_loss_eval, _, _ = curriculum_model(x, targets=targets)\n",
    "    print(f\"Final LM loss: {lm_loss_eval.item():.4f}\")\n",
    "\n",
    "acc_final = test_varied_needles_wide(curriculum_model, n_trials=50)\n",
    "print(f\"Final Retrieval: {acc_final*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "576815c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PART 2c: STRUCTURED LM + RETRIEVAL\n",
      "============================================================\n",
      "Sample structured sequence: [669, 770, 871, 972, 317, 418, 803, 405, 506, 607]\n",
      "\n",
      "--- PHASE 1: Structured LM (1000 steps) ---\n",
      "Step    0: LM=10.8750\n",
      "Step  200: LM=2.8438\n",
      "Step  400: LM=2.7812\n",
      "Step  600: LM=2.6719\n",
      "Step  800: LM=2.8125\n",
      "Structured LM Perplexity: 14.44 (optimal ~4 for 70/30 pattern)\n",
      "  Accuracy (full vocab range): 0.0% (0/30)\n",
      "Retrieval after LM only: 0.0%\n",
      "\n",
      "--- PHASE 2: Structured LM + Retrieval (1000 steps) ---\n",
      "Step    0: LM=2.6562, RET=13.4375\n",
      "Step  200: LM=2.8906, RET=11.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     81\u001b[39m optimizer.zero_grad()\n\u001b[32m     82\u001b[39m struct_model.train()\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m x, targets = \u001b[43mget_structured_lm_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m _, lm_loss, _, _ = struct_model(x, targets=targets)\n\u001b[32m     87\u001b[39m ret_loss = compute_retrieval_loss(struct_model, seq_len=\u001b[32m256\u001b[39m, batch_size=\u001b[32m4\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mget_structured_lm_batch\u001b[39m\u001b[34m(batch_size, seq_len)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Start with random token\u001b[39;00m\n\u001b[32m     21\u001b[39m     x[b, \u001b[32m0\u001b[39m] = torch.randint(\u001b[32m100\u001b[39m, \u001b[32m1000\u001b[39m, (\u001b[32m1\u001b[39m,))\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     24\u001b[39m         prev = x[b, t-\u001b[32m1\u001b[39m].item()\n\u001b[32m     25\u001b[39m         \u001b[38;5;66;03m# 70% chance: next token = prev + 1 (mod range)\u001b[39;00m\n\u001b[32m     26\u001b[39m         \u001b[38;5;66;03m# 30% chance: random jump\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 2c: PATTERN-BASED LM + RETRIEVAL\n",
    "# =============================================================================\n",
    "# Random sequences have no structure - can't learn LM from them!\n",
    "# Let's use LEARNABLE patterns: simple repetition/markov structure\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 2c: STRUCTURED LM + RETRIEVAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def get_structured_lm_batch(batch_size=8, seq_len=256):\n",
    "    \"\"\"\n",
    "    Generate sequences with learnable patterns:\n",
    "    - Token A often followed by A+1\n",
    "    - Simple bigram patterns\n",
    "    \"\"\"\n",
    "    x = torch.zeros(batch_size, seq_len, dtype=torch.long, device=DEVICE)\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        # Start with random token\n",
    "        x[b, 0] = torch.randint(100, 1000, (1,))\n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            prev = x[b, t-1].item()\n",
    "            # 70% chance: next token = prev + 1 (mod range)\n",
    "            # 30% chance: random jump\n",
    "            if torch.rand(1) < 0.7:\n",
    "                x[b, t] = (prev + 1) % 1000 + 100\n",
    "            else:\n",
    "                x[b, t] = torch.randint(100, 1000, (1,))\n",
    "    \n",
    "    targets = x.clone()\n",
    "    targets[:, :-1] = x[:, 1:]\n",
    "    targets[:, -1] = -100\n",
    "    return x, targets\n",
    "\n",
    "# Test the pattern\n",
    "x_test, y_test = get_structured_lm_batch(batch_size=1)\n",
    "print(f\"Sample structured sequence: {x_test[0, :10].tolist()}\")\n",
    "\n",
    "# Fresh model\n",
    "cfg_struct = HybridConfig(\n",
    "    d_model=512, n_heads=8, head_dim=64, value_dim=128,\n",
    "    layer_pattern=\"GS\", window_size=64, chunk_size=64,\n",
    "    shifted_value=True,\n",
    ")\n",
    "struct_model = TransparentHybrid(cfg_struct).to(DEVICE).to(torch.bfloat16)\n",
    "optimizer = torch.optim.AdamW(struct_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "# Phase 1: Structured LM\n",
    "print(\"\\n--- PHASE 1: Structured LM (1000 steps) ---\")\n",
    "for step in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    struct_model.train()\n",
    "    \n",
    "    x, targets = get_structured_lm_batch()\n",
    "    _, lm_loss, _, _ = struct_model(x, targets=targets)\n",
    "    lm_loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(struct_model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step:4d}: LM={lm_loss.item():.4f}\")\n",
    "\n",
    "# Evaluate LM\n",
    "with torch.no_grad():\n",
    "    struct_model.eval()\n",
    "    x, targets = get_structured_lm_batch(batch_size=32)\n",
    "    _, lm_loss_eval, _, _ = struct_model(x, targets=targets)\n",
    "    ppl = torch.exp(lm_loss_eval).item()\n",
    "    print(f\"Structured LM Perplexity: {ppl:.2f} (optimal ~4 for 70/30 pattern)\")\n",
    "\n",
    "# Check if retrieval still works after LM training\n",
    "acc_lm = test_varied_needles_wide(struct_model, n_trials=30)\n",
    "print(f\"Retrieval after LM only: {acc_lm*100:.1f}%\")\n",
    "\n",
    "# Phase 2: Add retrieval\n",
    "print(\"\\n--- PHASE 2: Structured LM + Retrieval (1000 steps) ---\")\n",
    "for step in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    struct_model.train()\n",
    "    \n",
    "    x, targets = get_structured_lm_batch()\n",
    "    _, lm_loss, _, _ = struct_model(x, targets=targets)\n",
    "    \n",
    "    ret_loss = compute_retrieval_loss(struct_model, seq_len=256, batch_size=4)\n",
    "    \n",
    "    total_loss = lm_loss + 1.0 * ret_loss\n",
    "    total_loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(struct_model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step:4d}: LM={lm_loss.item():.4f}, RET={ret_loss.item():.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n--- FINAL EVALUATION ---\")\n",
    "struct_model.eval()\n",
    "with torch.no_grad():\n",
    "    x, targets = get_structured_lm_batch(batch_size=32)\n",
    "    _, lm_loss_eval, _, _ = struct_model(x, targets=targets)\n",
    "    ppl = torch.exp(lm_loss_eval).item()\n",
    "    print(f\"Final LM Perplexity: {ppl:.2f}\")\n",
    "\n",
    "acc_final = test_varied_needles_wide(struct_model, n_trials=50)\n",
    "print(f\"Final Retrieval: {acc_final*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1712d4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh model: 33,217,560 params\n",
      "\n",
      "============================================================\n",
      "PHASE 1: PURE RETRIEVAL\n",
      "============================================================\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step    0: loss=11.1250, acc=0.0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  200: loss=10.9375, acc=0.0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  400: loss=11.0000, acc=0.0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  600: loss=11.0000, acc=0.0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  800: loss=9.5000, acc=0.0%\n",
      "  Accuracy (full vocab range): 100.0% (20/20)\n",
      "Step 1000: loss=3.8906, acc=100.0%\n",
      "  → Reached 95%+ at step 1000, moving to Phase 2\n",
      "  Accuracy (full vocab range): 100.0% (50/50)\n",
      "\n",
      "PHASE 1 COMPLETE: 100.0% retrieval accuracy\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PROPER CURRICULUM: RETRIEVAL → LM (ONE CLEAN RUN)\n",
    "# =============================================================================\n",
    "\n",
    "import importlib\n",
    "analysis = importlib.reload(analysis)\n",
    "from analysis import compute_retrieval_loss\n",
    "\n",
    "# FRESH MODEL\n",
    "cfg_final = HybridConfig(\n",
    "    d_model=512, n_heads=8, head_dim=64, value_dim=128,\n",
    "    layer_pattern=\"GS\", window_size=64, chunk_size=64,\n",
    "    shifted_value=True,\n",
    ")\n",
    "final_model = TransparentHybrid(cfg_final).to(DEVICE).to(torch.bfloat16)\n",
    "print(f\"Fresh model: {final_model.count_params():,} params\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(final_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 1: PURE RETRIEVAL (until 95%+)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: PURE RETRIEVAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_model.train()\n",
    "for step in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    ret_loss = compute_retrieval_loss(final_model, seq_len=256, batch_size=8)\n",
    "    ret_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(final_model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        # Check accuracy\n",
    "        final_model.eval()\n",
    "        acc = test_varied_needles_wide(final_model, seq_len=256, needle_pos=32, n_trials=20)\n",
    "        final_model.train()\n",
    "        print(f\"Step {step:4d}: loss={ret_loss.item():.4f}, acc={acc*100:.1f}%\")\n",
    "        \n",
    "        if acc >= 0.95:\n",
    "            print(f\"  → Reached 95%+ at step {step}, moving to Phase 2\")\n",
    "            break\n",
    "\n",
    "# Final Phase 1 check\n",
    "final_model.eval()\n",
    "phase1_acc = test_varied_needles_wide(final_model, seq_len=256, needle_pos=32, n_trials=50)\n",
    "print(f\"\\nPHASE 1 COMPLETE: {phase1_acc*100:.1f}% retrieval accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af280364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RETRIEVAL STRESS TESTS\n",
      "============================================================\n",
      "\n",
      "1. DISTANCE TEST (needle at various positions):\n",
      "  Accuracy (full vocab range): 100.0% (30/30)\n",
      "  Accuracy (full vocab range): 100.0% (30/30)\n",
      "  Accuracy (full vocab range): 100.0% (30/30)\n",
      "  Accuracy (full vocab range): 96.7% (29/30)\n",
      "  Accuracy (full vocab range): 100.0% (30/30)\n",
      "\n",
      "2. SEQUENCE LENGTH TEST:\n",
      "  Accuracy (full vocab range): 100.0% (30/30)\n",
      "  Accuracy (full vocab range): 96.7% (29/30)\n",
      "  Accuracy (full vocab range): 13.3% (4/30)\n",
      "\n",
      "3. MULTI-NEEDLE RETRIEVAL:\n",
      "  2 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['0%', '100%']\n",
      "    Any needle correct: 100.0%\n",
      "    Most retrieved: needle 2 (position 2/2)\n",
      "  3 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['0%', '0%', '100%']\n",
      "    Any needle correct: 100.0%\n",
      "    Most retrieved: needle 3 (position 3/3)\n",
      "  5 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['0%', '0%', '0%', '0%', '100%']\n",
      "    Any needle correct: 100.0%\n",
      "    Most retrieved: needle 5 (position 5/5)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 1b: TEST RETRIEVAL THOROUGHLY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RETRIEVAL STRESS TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Different distances\n",
    "print(\"\\n1. DISTANCE TEST (needle at various positions):\")\n",
    "for needle_pos in [8, 32, 64, 128, 200]:\n",
    "    acc = test_varied_needles_wide(final_model, seq_len=256, needle_pos=needle_pos, n_trials=30)\n",
    "\n",
    "# Test 2: Longer sequences\n",
    "print(\"\\n2. SEQUENCE LENGTH TEST:\")\n",
    "for seq_len in [256, 512, 1024]:\n",
    "    acc = test_varied_needles_wide(final_model, seq_len=seq_len, needle_pos=32, n_trials=30)\n",
    "\n",
    "# Test 3: MULTI-NEEDLE\n",
    "print(\"\\n3. MULTI-NEEDLE RETRIEVAL:\")\n",
    "\n",
    "def test_multi_needle(model, seq_len=512, n_needles=3, n_trials=30):\n",
    "    \"\"\"Test retrieval of multiple needles in same sequence.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    \n",
    "    correct_all = 0\n",
    "    correct_any = 0\n",
    "    per_needle_correct = [0] * n_needles\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Generate sequence\n",
    "        seq = torch.randint(0, cfg.vocab_size - 100, (1, seq_len), device=device)\n",
    "        \n",
    "        # Place n_needles at evenly spaced positions\n",
    "        needle_ids = []\n",
    "        positions = []\n",
    "        spacing = (seq_len - 50) // (n_needles + 1)\n",
    "        \n",
    "        for i in range(n_needles):\n",
    "            pos = spacing * (i + 1)\n",
    "            needle_id = torch.randint(100, cfg.vocab_size - 100, (1,)).item()\n",
    "            \n",
    "            seq[0, pos] = cfg.marker_token\n",
    "            seq[0, pos + 1] = needle_id\n",
    "            \n",
    "            positions.append(pos)\n",
    "            needle_ids.append(needle_id)\n",
    "        \n",
    "        # Query each needle\n",
    "        trial_correct = 0\n",
    "        for i, (pos, needle_id) in enumerate(zip(positions, needle_ids)):\n",
    "            # Create query sequence: original + CUE at end\n",
    "            query_seq = seq.clone()\n",
    "            query_seq[0, -1] = cfg.cue_token\n",
    "            \n",
    "            # Also need to place the MARKER at end-2 to trigger retrieval\n",
    "            # Wait - the test should query with the original marker's position info\n",
    "            # Actually: CUE should retrieve what was stored at MARKER\n",
    "            # But we have multiple markers... need to think about this\n",
    "            \n",
    "            # For now: test if model can retrieve the LAST needle\n",
    "            pass\n",
    "        \n",
    "        # Simpler test: just check if LAST needle is retrieved\n",
    "        query_seq = seq.clone()\n",
    "        query_seq[0, -1] = cfg.cue_token\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _, _, _ = model(query_seq)\n",
    "        \n",
    "        pred = logits[0, -1].argmax().item()\n",
    "        \n",
    "        # Check each needle\n",
    "        for i, nid in enumerate(needle_ids):\n",
    "            if pred == nid:\n",
    "                per_needle_correct[i] += 1\n",
    "                trial_correct += 1\n",
    "        \n",
    "        if trial_correct == n_needles:\n",
    "            correct_all += 1\n",
    "        if trial_correct > 0:\n",
    "            correct_any += 1\n",
    "    \n",
    "    print(f\"  {n_needles} needles in seq_len={seq_len}:\")\n",
    "    print(f\"    Per-needle retrieval: {[f'{c/n_trials*100:.0f}%' for c in per_needle_correct]}\")\n",
    "    print(f\"    Any needle correct: {correct_any/n_trials*100:.1f}%\")\n",
    "    \n",
    "    # Which needle gets retrieved most?\n",
    "    most_retrieved = per_needle_correct.index(max(per_needle_correct))\n",
    "    print(f\"    Most retrieved: needle {most_retrieved+1} (position {most_retrieved+1}/{n_needles})\")\n",
    "    \n",
    "    return per_needle_correct, correct_any / n_trials\n",
    "\n",
    "# Test with increasing needle counts\n",
    "for n in [2, 3, 5]:\n",
    "    test_multi_needle(final_model, seq_len=512, n_needles=n, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb74ea24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DIAGNOSING RECENCY BIAS\n",
      "============================================================\n",
      "\n",
      "1. KEY SIMILARITY:\n",
      "\n",
      "TRAINED (final_model) - Key similarity analysis:\n",
      "  Avg |cosine| between different token keys:\n",
      "    Per-head: ['0.599', '0.752', '0.475', '0.529', '0.701', '0.673', '0.798', '0.284']\n",
      "    Overall: 0.601\n",
      "    Max head: 0.798\n",
      "    Expected for random 64D vectors: 0.100\n",
      "    ⚠ Keys are LESS orthogonal than random (more interference)\n",
      "\n",
      "2. GATE VALUES:\n",
      "\n",
      "TRAINED (final_model) gate analysis:\n",
      "  Layer 0 (GDN):\n",
      "    β: mean=0.1309, std=0.0654, range=[0.023, 0.439]\n",
      "    g: mean=0.8633, std=0.0762, range=[0.488, 0.992]\n",
      "    β_bias: -2.0000 (init was -2.0)\n",
      "    g_bias: 2.0000 (init was 3.0)\n",
      "\n",
      "3. β ON MARKER vs REGULAR TOKENS:\n",
      "  β at marker positions: [0.353515625, 0.353515625, 0.353515625]\n",
      "  β at regular positions: [0.04931640625, 0.05615234375, 0.045166015625]\n",
      "  Mean β overall: 0.0752\n",
      "  Mean g overall: 0.8633\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DIAGNOSIS: WHY ONLY LAST NEEDLE?\n",
    "# =============================================================================\n",
    "# The state is being overwritten. Check:\n",
    "# 1. Are keys collapsing (all similar)?\n",
    "# 2. Is β too high (overwriting everything)?\n",
    "# 3. Is g too low (not retaining)?\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DIAGNOSING RECENCY BIAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check key similarity after training\n",
    "print(\"\\n1. KEY SIMILARITY:\")\n",
    "check_key_similarity(final_model, n_tokens=50, name=\"TRAINED (final_model)\")\n",
    "\n",
    "# Check gate values\n",
    "print(\"\\n2. GATE VALUES:\")\n",
    "analyze_gates(final_model, \"TRAINED (final_model)\")\n",
    "\n",
    "# Check β specifically on marker tokens\n",
    "print(\"\\n3. β ON MARKER vs REGULAR TOKENS:\")\n",
    "gdn_layer = None\n",
    "for layer in final_model.layers:\n",
    "    if hasattr(layer, 'beta_proj'):\n",
    "        gdn_layer = layer\n",
    "        break\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Create sequence with markers\n",
    "    seq = torch.randint(100, 1000, (1, 128), device=DEVICE)\n",
    "    seq[0, 20] = cfg_final.marker_token\n",
    "    seq[0, 60] = cfg_final.marker_token\n",
    "    seq[0, 100] = cfg_final.marker_token\n",
    "    \n",
    "    emb = final_model.embed(seq)\n",
    "    x_norm = gdn_layer.norm(emb)\n",
    "    beta = torch.sigmoid(gdn_layer.beta_proj(x_norm))  # [1, T, H]\n",
    "    g = torch.sigmoid(gdn_layer.g_proj(x_norm))\n",
    "    \n",
    "    print(f\"  β at marker positions: {beta[0, [20, 60, 100]].mean(dim=1).tolist()}\")\n",
    "    print(f\"  β at regular positions: {beta[0, [10, 50, 90]].mean(dim=1).tolist()}\")\n",
    "    print(f\"  Mean β overall: {beta.mean().item():.4f}\")\n",
    "    print(f\"  Mean g overall: {g.mean().item():.4f}\")\n",
    "\n",
    "# The problem: if β is high everywhere, every token overwrites state\n",
    "# Solution: β should be HIGH only at markers, LOW elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "953dbfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RETRAINING WITH KEY ORTHOGONALITY\n",
      "============================================================\n",
      "Fresh model: 33,217,560 params\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step    0: ret=10.875, orth=0.106, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  200: ret=10.875, orth=0.105, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  400: ret=10.875, orth=0.133, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  600: ret=11.188, orth=0.140, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  800: ret=10.938, orth=0.140, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step 1000: ret=11.000, orth=0.125, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step 1200: ret=11.125, orth=0.117, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step 1400: ret=11.000, orth=0.122, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step 1600: ret=11.000, orth=0.120, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step 1800: ret=11.125, orth=0.120, acc=0%\n",
      "\n",
      "--- FINAL CHECKS ---\n",
      "\n",
      "1. Key similarity:\n",
      "\n",
      "model_v2 (after orth reg) - Key similarity analysis:\n",
      "  Avg |cosine| between different token keys:\n",
      "    Per-head: ['0.126', '0.102', '0.103', '0.100', '0.098', '0.144', '0.103', '0.102']\n",
      "    Overall: 0.110\n",
      "    Max head: 0.144\n",
      "    Expected for random 64D vectors: 0.100\n",
      "    ⚠ Keys are LESS orthogonal than random (more interference)\n",
      "\n",
      "2. Multi-needle test:\n",
      "  2 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['0%', '0%']\n",
      "    Any needle correct: 0.0%\n",
      "    Most retrieved: needle 1 (position 1/2)\n",
      "  3 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['0%', '0%', '0%']\n",
      "    Any needle correct: 0.0%\n",
      "    Most retrieved: needle 1 (position 1/3)\n",
      "  5 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['0%', '0%', '0%', '0%', '0%']\n",
      "    Any needle correct: 0.0%\n",
      "    Most retrieved: needle 1 (position 1/5)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2: RETRAIN WITH KEY ORTHOGONALITY\n",
    "# =============================================================================\n",
    "# Pure retrieval training collapsed keys. Add orthogonality regularization.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_key_orth_loss(model, batch_size=8, seq_len=128):\n",
    "    \"\"\"Penalize high cosine similarity between keys.\"\"\"\n",
    "    gdn_layer = None\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'k_proj'):\n",
    "            gdn_layer = layer\n",
    "            break\n",
    "    \n",
    "    if gdn_layer is None:\n",
    "        return torch.tensor(0.0, device=DEVICE)\n",
    "    \n",
    "    # Random tokens\n",
    "    tokens = torch.randint(100, 10000, (batch_size, seq_len), device=DEVICE)\n",
    "    emb = model.embed(tokens)\n",
    "    x_norm = gdn_layer.norm(emb)\n",
    "    keys = gdn_layer.k_proj(x_norm)  # [B, T, H*K]\n",
    "    keys = keys.view(batch_size, seq_len, model.cfg.n_heads, model.cfg.head_dim)\n",
    "    keys = F.normalize(keys.float(), p=2, dim=-1)  # [B, T, H, K]\n",
    "    \n",
    "    # Compute pairwise similarity per head, average across batch\n",
    "    loss = 0.0\n",
    "    for h in range(model.cfg.n_heads):\n",
    "        k_h = keys[:, :, h, :]  # [B, T, K]\n",
    "        # [B, T, T] similarity matrices\n",
    "        sim = torch.bmm(k_h, k_h.transpose(1, 2))\n",
    "        # Penalize off-diagonal (exclude self-similarity)\n",
    "        mask = ~torch.eye(seq_len, dtype=torch.bool, device=DEVICE)\n",
    "        off_diag = sim[:, mask].abs()\n",
    "        loss = loss + off_diag.mean()\n",
    "    \n",
    "    return loss / model.cfg.n_heads\n",
    "\n",
    "# FRESH MODEL\n",
    "print(\"=\"*60)\n",
    "print(\"RETRAINING WITH KEY ORTHOGONALITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cfg_v2 = HybridConfig(\n",
    "    d_model=512, n_heads=8, head_dim=64, value_dim=128,\n",
    "    layer_pattern=\"GS\", window_size=64, chunk_size=64,\n",
    "    shifted_value=True,\n",
    ")\n",
    "model_v2 = TransparentHybrid(cfg_v2).to(DEVICE).to(torch.bfloat16)\n",
    "print(f\"Fresh model: {model_v2.count_params():,} params\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model_v2.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "model_v2.train()\n",
    "for step in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Retrieval loss\n",
    "    ret_loss = compute_retrieval_loss(model_v2, seq_len=256, batch_size=8)\n",
    "    \n",
    "    # Key orthogonality loss\n",
    "    orth_loss = compute_key_orth_loss(model_v2, batch_size=4, seq_len=64)\n",
    "    \n",
    "    total_loss = ret_loss + 0.5 * orth_loss\n",
    "    total_loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model_v2.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        model_v2.eval()\n",
    "        acc = test_varied_needles_wide(model_v2, seq_len=256, needle_pos=32, n_trials=20)\n",
    "        model_v2.train()\n",
    "        print(f\"Step {step:4d}: ret={ret_loss.item():.3f}, orth={orth_loss.item():.3f}, acc={acc*100:.0f}%\")\n",
    "        \n",
    "        if acc >= 0.95:\n",
    "            print(f\"  → 95%+ reached, checking key similarity...\")\n",
    "            check_key_similarity(model_v2, n_tokens=30, name=\"model_v2\")\n",
    "            break\n",
    "\n",
    "# Final check\n",
    "print(\"\\n--- FINAL CHECKS ---\")\n",
    "model_v2.eval()\n",
    "print(\"\\n1. Key similarity:\")\n",
    "check_key_similarity(model_v2, n_tokens=50, name=\"model_v2 (after orth reg)\")\n",
    "\n",
    "print(\"\\n2. Multi-needle test:\")\n",
    "for n in [2, 3, 5]:\n",
    "    test_multi_needle(model_v2, seq_len=512, n_needles=n, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0d0579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CURRICULUM: RETRIEVAL → ORTH REG\n",
      "============================================================\n",
      "\n",
      "--- STAGE 1: Pure Retrieval ---\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step    0: loss=10.812, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  200: loss=10.875, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  400: loss=10.938, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  600: loss=11.188, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  800: loss=10.000, acc=0%\n",
      "  Accuracy (full vocab range): 100.0% (20/20)\n",
      "Step 1000: loss=4.312, acc=100%\n",
      "  → Stage 1 complete\n",
      "\n",
      "Key similarity BEFORE orth reg:\n",
      "\n",
      "model_v3 - Key similarity analysis:\n",
      "  Avg |cosine| between different token keys:\n",
      "    Per-head: ['0.791', '0.294', '0.822', '0.732', '0.243', '0.817', '0.850', '0.661']\n",
      "    Overall: 0.652\n",
      "    Max head: 0.850\n",
      "    Expected for random 64D vectors: 0.100\n",
      "    ⚠ Keys are LESS orthogonal than random (more interference)\n",
      "\n",
      "--- STAGE 2: Retrieval + Orth Reg (lower LR) ---\n",
      "  Accuracy (full vocab range): 100.0% (20/20)\n",
      "Step    0: ret=5.250, orth=0.645, acc=100%\n",
      "  Accuracy (full vocab range): 100.0% (20/20)\n",
      "Step  200: ret=5.062, orth=0.385, acc=100%\n",
      "  Accuracy (full vocab range): 90.0% (18/20)\n",
      "Step  400: ret=4.344, orth=0.333, acc=90%\n",
      "  Accuracy (full vocab range): 100.0% (20/20)\n",
      "Step  600: ret=4.250, orth=0.302, acc=100%\n",
      "  Accuracy (full vocab range): 100.0% (20/20)\n",
      "Step  800: ret=3.844, orth=0.296, acc=100%\n",
      "\n",
      "--- FINAL EVALUATION ---\n",
      "\n",
      "1. Key similarity AFTER orth reg:\n",
      "\n",
      "model_v3 (after curriculum) - Key similarity analysis:\n",
      "  Avg |cosine| between different token keys:\n",
      "    Per-head: ['0.157', '0.106', '0.120', '0.107', '0.106', '0.803', '0.846', '0.122']\n",
      "    Overall: 0.296\n",
      "    Max head: 0.846\n",
      "    Expected for random 64D vectors: 0.100\n",
      "    ⚠ Keys are LESS orthogonal than random (more interference)\n",
      "\n",
      "2. Single needle at various distances:\n",
      "  Accuracy (full vocab range): 100.0% (20/20)\n",
      "  Accuracy (full vocab range): 100.0% (20/20)\n",
      "  Accuracy (full vocab range): 100.0% (20/20)\n",
      "\n",
      "3. Multi-needle:\n",
      "  2 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['0%', '97%']\n",
      "    Any needle correct: 96.7%\n",
      "    Most retrieved: needle 2 (position 2/2)\n",
      "  3 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['0%', '0%', '100%']\n",
      "    Any needle correct: 100.0%\n",
      "    Most retrieved: needle 3 (position 3/3)\n",
      "  5 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['0%', '0%', '0%', '0%', '100%']\n",
      "    Any needle correct: 100.0%\n",
      "    Most retrieved: needle 5 (position 5/5)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2b: CURRICULUM - RETRIEVAL FIRST, THEN ORTH REG\n",
    "# =============================================================================\n",
    "# Pure retrieval works but collapses keys\n",
    "# Pure orth reg prevents learning\n",
    "# Solution: Train retrieval FIRST, then add orth reg to fix keys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CURRICULUM: RETRIEVAL → ORTH REG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cfg_v3 = HybridConfig(\n",
    "    d_model=512, n_heads=8, head_dim=64, value_dim=128,\n",
    "    layer_pattern=\"GS\", window_size=64, chunk_size=64,\n",
    "    shifted_value=True,\n",
    ")\n",
    "model_v3 = TransparentHybrid(cfg_v3).to(DEVICE).to(torch.bfloat16)\n",
    "optimizer = torch.optim.AdamW(model_v3.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "# STAGE 1: Pure retrieval until 95%+\n",
    "print(\"\\n--- STAGE 1: Pure Retrieval ---\")\n",
    "model_v3.train()\n",
    "for step in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    ret_loss = compute_retrieval_loss(model_v3, seq_len=256, batch_size=8)\n",
    "    ret_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model_v3.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        model_v3.eval()\n",
    "        acc = test_varied_needles_wide(model_v3, seq_len=256, needle_pos=32, n_trials=20)\n",
    "        model_v3.train()\n",
    "        print(f\"Step {step:4d}: loss={ret_loss.item():.3f}, acc={acc*100:.0f}%\")\n",
    "        if acc >= 0.95:\n",
    "            print(f\"  → Stage 1 complete\")\n",
    "            break\n",
    "\n",
    "# Check keys before stage 2\n",
    "print(\"\\nKey similarity BEFORE orth reg:\")\n",
    "check_key_similarity(model_v3, n_tokens=30, name=\"model_v3\")\n",
    "\n",
    "# STAGE 2: Add orth reg while maintaining retrieval\n",
    "print(\"\\n--- STAGE 2: Retrieval + Orth Reg (lower LR) ---\")\n",
    "optimizer = torch.optim.AdamW(model_v3.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "model_v3.train()\n",
    "for step in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    ret_loss = compute_retrieval_loss(model_v3, seq_len=256, batch_size=8)\n",
    "    orth_loss = compute_key_orth_loss(model_v3, batch_size=4, seq_len=64)\n",
    "    \n",
    "    # Light orth pressure\n",
    "    total_loss = ret_loss + 0.1 * orth_loss\n",
    "    total_loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model_v3.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        model_v3.eval()\n",
    "        acc = test_varied_needles_wide(model_v3, seq_len=256, needle_pos=32, n_trials=20)\n",
    "        model_v3.train()\n",
    "        print(f\"Step {step:4d}: ret={ret_loss.item():.3f}, orth={orth_loss.item():.3f}, acc={acc*100:.0f}%\")\n",
    "\n",
    "# Final checks\n",
    "print(\"\\n--- FINAL EVALUATION ---\")\n",
    "model_v3.eval()\n",
    "\n",
    "print(\"\\n1. Key similarity AFTER orth reg:\")\n",
    "check_key_similarity(model_v3, n_tokens=50, name=\"model_v3 (after curriculum)\")\n",
    "\n",
    "print(\"\\n2. Single needle at various distances:\")\n",
    "for pos in [32, 64, 128]:\n",
    "    test_varied_needles_wide(model_v3, seq_len=256, needle_pos=pos, n_trials=20)\n",
    "\n",
    "print(\"\\n3. Multi-needle:\")\n",
    "for n in [2, 3, 5]:\n",
    "    test_multi_needle(model_v3, seq_len=512, n_needles=n, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3521435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MARKER KEY ANALYSIS\n",
      "============================================================\n",
      "Marker token ID: 50251\n",
      "\n",
      "Marker key similarity ACROSS DIFFERENT CONTEXTS:\n",
      "  Seq1 vs Seq2: 1.0000 (per-head: [1.0, 0.9999998211860657, 1.0, 0.9999999403953552, 1.0, 1.0000001192092896, 1.0, 1.0])\n",
      "  Seq1 vs Seq3: 1.0000\n",
      "  Seq2 vs Seq3: 1.0000\n",
      "\n",
      "⚠ PROBLEM: Marker produces nearly IDENTICAL keys regardless of context!\n",
      "   This means all markers write to the SAME slot, overwriting each other.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANALYSIS: WHY ONLY LAST NEEDLE?\n",
    "# =============================================================================\n",
    "# Even with improved key orthogonality, only last needle retrieved.\n",
    "# Check if the problem is that ALL markers produce the SAME key.\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MARKER KEY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gdn_layer = None\n",
    "for layer in model_v3.layers:\n",
    "    if hasattr(layer, 'k_proj'):\n",
    "        gdn_layer = layer\n",
    "        break\n",
    "\n",
    "# Compute keys for the MARKER token at different positions\n",
    "marker_token = cfg_v3.marker_token\n",
    "print(f\"Marker token ID: {marker_token}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Same marker token, but different context\n",
    "    seq1 = torch.randint(100, 1000, (1, 64), device=DEVICE)\n",
    "    seq2 = torch.randint(100, 1000, (1, 64), device=DEVICE)\n",
    "    seq3 = torch.randint(100, 1000, (1, 64), device=DEVICE)\n",
    "    \n",
    "    # Place marker at same position in each\n",
    "    pos = 32\n",
    "    seq1[0, pos] = marker_token\n",
    "    seq2[0, pos] = marker_token\n",
    "    seq3[0, pos] = marker_token\n",
    "    \n",
    "    # Get embeddings and keys\n",
    "    emb1 = model_v3.embed(seq1)\n",
    "    emb2 = model_v3.embed(seq2)\n",
    "    emb3 = model_v3.embed(seq3)\n",
    "    \n",
    "    x1 = gdn_layer.norm(emb1)\n",
    "    x2 = gdn_layer.norm(emb2)\n",
    "    x3 = gdn_layer.norm(emb3)\n",
    "    \n",
    "    k1 = gdn_layer.k_proj(x1)[0, pos]  # [H*K]\n",
    "    k2 = gdn_layer.k_proj(x2)[0, pos]\n",
    "    k3 = gdn_layer.k_proj(x3)[0, pos]\n",
    "    \n",
    "    # Reshape to [H, K] and normalize\n",
    "    k1 = F.normalize(k1.view(cfg_v3.n_heads, cfg_v3.head_dim).float(), dim=-1)\n",
    "    k2 = F.normalize(k2.view(cfg_v3.n_heads, cfg_v3.head_dim).float(), dim=-1)\n",
    "    k3 = F.normalize(k3.view(cfg_v3.n_heads, cfg_v3.head_dim).float(), dim=-1)\n",
    "    \n",
    "    # Similarity between marker keys in different contexts\n",
    "    sim_12 = (k1 * k2).sum(dim=-1)  # [H]\n",
    "    sim_13 = (k1 * k3).sum(dim=-1)\n",
    "    sim_23 = (k2 * k3).sum(dim=-1)\n",
    "    \n",
    "    print(f\"\\nMarker key similarity ACROSS DIFFERENT CONTEXTS:\")\n",
    "    print(f\"  Seq1 vs Seq2: {sim_12.mean().item():.4f} (per-head: {sim_12.tolist()})\")\n",
    "    print(f\"  Seq1 vs Seq3: {sim_13.mean().item():.4f}\")\n",
    "    print(f\"  Seq2 vs Seq3: {sim_23.mean().item():.4f}\")\n",
    "    \n",
    "    if sim_12.mean() > 0.9:\n",
    "        print(\"\\n⚠ PROBLEM: Marker produces nearly IDENTICAL keys regardless of context!\")\n",
    "        print(\"   This means all markers write to the SAME slot, overwriting each other.\")\n",
    "    else:\n",
    "        print(\"\\n✓ Marker keys vary with context\")\n",
    "\n",
    "# The fix might need to be: use POSITION or CONTEXT to differentiate marker keys\n",
    "# Or: use the VALUE (needle) to create a unique key for each marker+needle pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a24654fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING RoPE (Rotary Position Embeddings)\n",
      "============================================================\n",
      "Model with RoPE: 33,217,560 params\n",
      "Forward pass OK: output=torch.Size([2, 64, 50257])\n",
      "\n",
      "--- MARKER KEY ANALYSIS (with RoPE) ---\n",
      "Marker at SAME position (32): similarity=1.0000\n",
      "Marker at DIFFERENT positions (32 vs 16): similarity=0.6113\n",
      "\n",
      "⚠ RoPE may not be working as expected\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEST RoPE FOR POSITION-DEPENDENT KEYS\n",
    "# =============================================================================\n",
    "\n",
    "import importlib\n",
    "import config as config_module\n",
    "import model as model_module\n",
    "\n",
    "config_module = importlib.reload(config_module)\n",
    "model_module = importlib.reload(model_module)\n",
    "\n",
    "from config import HybridConfig\n",
    "from model import TransparentHybrid\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING RoPE (Rotary Position Embeddings)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create model WITH RoPE\n",
    "cfg_rope = HybridConfig(\n",
    "    d_model=512, n_heads=8, head_dim=64, value_dim=128,\n",
    "    layer_pattern=\"GS\", window_size=64, chunk_size=64,\n",
    "    shifted_value=True,\n",
    "    use_rope=True,  # ENABLE RoPE\n",
    ")\n",
    "model_rope = TransparentHybrid(cfg_rope).to(DEVICE).to(torch.bfloat16)\n",
    "print(f\"Model with RoPE: {model_rope.count_params():,} params\")\n",
    "\n",
    "# Quick forward test\n",
    "x_test = torch.randint(0, 1000, (2, 64), device=DEVICE)\n",
    "with torch.no_grad():\n",
    "    logits, _, diags, state = model_rope(x_test)\n",
    "print(f\"Forward pass OK: output={logits.shape}\")\n",
    "\n",
    "# Check if marker keys are NOW position-dependent\n",
    "print(\"\\n--- MARKER KEY ANALYSIS (with RoPE) ---\")\n",
    "gdn_layer = model_rope.layers[0]\n",
    "\n",
    "marker_token = cfg_rope.marker_token\n",
    "with torch.no_grad():\n",
    "    # Same marker token, different contexts\n",
    "    seq1 = torch.randint(100, 1000, (1, 64), device=DEVICE)\n",
    "    seq2 = torch.randint(100, 1000, (1, 64), device=DEVICE)\n",
    "    \n",
    "    # Place marker at SAME position\n",
    "    seq1[0, 32] = marker_token\n",
    "    seq2[0, 32] = marker_token\n",
    "    \n",
    "    # Get keys\n",
    "    emb1 = model_rope.embed(seq1)\n",
    "    emb2 = model_rope.embed(seq2)\n",
    "    x1 = gdn_layer.norm(emb1)\n",
    "    x2 = gdn_layer.norm(emb2)\n",
    "    k1_raw = gdn_layer.k_proj(x1).view(1, 64, 8, 64)\n",
    "    k2_raw = gdn_layer.k_proj(x2).view(1, 64, 8, 64)\n",
    "    \n",
    "    # Apply RoPE\n",
    "    k1 = gdn_layer.rotary(k1_raw)\n",
    "    k2 = gdn_layer.rotary(k2_raw)\n",
    "    \n",
    "    # Compare keys at position 32\n",
    "    k1_32 = F.normalize(k1[0, 32].float(), dim=-1)\n",
    "    k2_32 = F.normalize(k2[0, 32].float(), dim=-1)\n",
    "    sim_same_pos = (k1_32 * k2_32).sum(dim=-1)\n",
    "    print(f\"Marker at SAME position (32): similarity={sim_same_pos.mean().item():.4f}\")\n",
    "    \n",
    "    # Now place markers at DIFFERENT positions\n",
    "    seq3 = torch.randint(100, 1000, (1, 64), device=DEVICE)\n",
    "    seq3[0, 16] = marker_token  # Position 16\n",
    "    \n",
    "    emb3 = model_rope.embed(seq3)\n",
    "    x3 = gdn_layer.norm(emb3)\n",
    "    k3_raw = gdn_layer.k_proj(x3).view(1, 64, 8, 64)\n",
    "    k3 = gdn_layer.rotary(k3_raw)\n",
    "    \n",
    "    k3_16 = F.normalize(k3[0, 16].float(), dim=-1)\n",
    "    sim_diff_pos = (k1_32 * k3_16).sum(dim=-1)\n",
    "    print(f\"Marker at DIFFERENT positions (32 vs 16): similarity={sim_diff_pos.mean().item():.4f}\")\n",
    "    \n",
    "    if sim_diff_pos.mean() < 0.5 and sim_same_pos.mean() > 0.9:\n",
    "        print(\"\\n✓ RoPE working: Same position → similar keys, different positions → different keys!\")\n",
    "    else:\n",
    "        print(\"\\n⚠ RoPE may not be working as expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8602d960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING WITH RoPE\n",
      "============================================================\n",
      "\n",
      "--- PHASE 1: Pure Retrieval ---\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step    0: loss=10.938, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  200: loss=10.812, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  400: loss=11.000, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  600: loss=10.875, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  800: loss=10.375, acc=0%\n",
      "  Accuracy (full vocab range): 80.0% (16/20)\n",
      "Step 1000: loss=6.156, acc=80%\n",
      "  Accuracy (full vocab range): 100.0% (20/20)\n",
      "Step 1200: loss=5.031, acc=100%\n",
      "  → Phase 1 complete at step 1200\n",
      "\n",
      "--- SINGLE NEEDLE TESTS ---\n",
      "  Accuracy (full vocab range): 96.7% (29/30)\n",
      "  Accuracy (full vocab range): 100.0% (30/30)\n",
      "  Accuracy (full vocab range): 96.7% (29/30)\n",
      "\n",
      "--- MULTI-NEEDLE TESTS (with RoPE) ---\n",
      "  2 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['77%', '10%']\n",
      "    Any needle correct: 86.7%\n",
      "    Most retrieved: needle 1 (position 1/2)\n",
      "  3 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['70%', '0%', '13%']\n",
      "    Any needle correct: 83.3%\n",
      "    Most retrieved: needle 1 (position 1/3)\n",
      "  5 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['0%', '3%', '0%', '0%', '63%']\n",
      "    Any needle correct: 66.7%\n",
      "    Most retrieved: needle 5 (position 5/5)\n",
      "\n",
      "--- KEY SIMILARITY (with RoPE) ---\n",
      "\n",
      "model_rope (trained) - Key similarity analysis:\n",
      "  Avg |cosine| between different token keys:\n",
      "    Per-head: ['0.376', '0.743', '0.315', '0.276', '0.429', '0.870', '0.369', '0.254']\n",
      "    Overall: 0.454\n",
      "    Max head: 0.870\n",
      "    Expected for random 64D vectors: 0.100\n",
      "    ⚠ Keys are LESS orthogonal than random (more interference)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRAIN WITH RoPE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING WITH RoPE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fresh model with RoPE\n",
    "cfg_rope = HybridConfig(\n",
    "    d_model=512, n_heads=8, head_dim=64, value_dim=128,\n",
    "    layer_pattern=\"GS\", window_size=64, chunk_size=64,\n",
    "    shifted_value=True,\n",
    "    use_rope=True,\n",
    ")\n",
    "model_rope = TransparentHybrid(cfg_rope).to(DEVICE).to(torch.bfloat16)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model_rope.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "# PHASE 1: Pure retrieval\n",
    "print(\"\\n--- PHASE 1: Pure Retrieval ---\")\n",
    "model_rope.train()\n",
    "for step in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    ret_loss = compute_retrieval_loss(model_rope, seq_len=256, batch_size=8)\n",
    "    ret_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model_rope.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        model_rope.eval()\n",
    "        acc = test_varied_needles_wide(model_rope, seq_len=256, needle_pos=32, n_trials=20)\n",
    "        model_rope.train()\n",
    "        print(f\"Step {step:4d}: loss={ret_loss.item():.3f}, acc={acc*100:.0f}%\")\n",
    "        if acc >= 0.95:\n",
    "            print(f\"  → Phase 1 complete at step {step}\")\n",
    "            break\n",
    "\n",
    "# Test single needle\n",
    "print(\"\\n--- SINGLE NEEDLE TESTS ---\")\n",
    "model_rope.eval()\n",
    "for pos in [32, 64, 128]:\n",
    "    test_varied_needles_wide(model_rope, seq_len=256, needle_pos=pos, n_trials=30)\n",
    "\n",
    "# Test multi-needle - THE KEY TEST\n",
    "print(\"\\n--- MULTI-NEEDLE TESTS (with RoPE) ---\")\n",
    "for n in [2, 3, 5]:\n",
    "    test_multi_needle(model_rope, seq_len=512, n_needles=n, n_trials=30)\n",
    "\n",
    "# Check key similarity\n",
    "print(\"\\n--- KEY SIMILARITY (with RoPE) ---\")\n",
    "check_key_similarity(model_rope, n_tokens=50, name=\"model_rope (trained)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8484df33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IMPROVED MULTI-NEEDLE: UNIQUE MARKERS\n",
      "============================================================\n",
      "\n",
      "--- Testing with UNIQUE markers (untrained for this) ---\n",
      "  2 needles with UNIQUE markers:\n",
      "    Per-needle: ['0%', '0%']\n",
      "    Average: 0.0%\n",
      "  3 needles with UNIQUE markers:\n",
      "    Per-needle: ['0%', '0%', '0%']\n",
      "    Average: 0.0%\n",
      "  5 needles with UNIQUE markers:\n",
      "    Per-needle: ['0%', '0%', '0%', '0%', '0%']\n",
      "    Average: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPROVED MULTI-NEEDLE TEST: Query with MARKER token\n",
    "# =============================================================================\n",
    "# Current test uses CUE for all queries. With RoPE, we should query\n",
    "# with the MARKER token at a position that matches where it was stored.\n",
    "# But that's not how inference works - we need to query with just the marker token.\n",
    "\n",
    "# Actually, the issue is: how does the model know WHICH needle to retrieve?\n",
    "# The CUE token is the same for all needles.\n",
    "# \n",
    "# In NIAH literature, each needle has a UNIQUE marker (key).\n",
    "# Let's implement that: each marker-needle pair uses a different marker token.\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"IMPROVED MULTI-NEEDLE: UNIQUE MARKERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def test_multi_needle_unique_markers(model, seq_len=512, n_needles=3, n_trials=30):\n",
    "    \"\"\"\n",
    "    Test with UNIQUE marker tokens for each needle.\n",
    "    Format:\n",
    "    - MARKER_1, NEEDLE_1 at pos P1\n",
    "    - MARKER_2, NEEDLE_2 at pos P2\n",
    "    - ...\n",
    "    - Query: MARKER_1 → should retrieve NEEDLE_1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    \n",
    "    # Use different marker tokens for each needle\n",
    "    base_marker = cfg.marker_token - 100  # Start below marker_token\n",
    "    \n",
    "    per_needle_correct = [0] * n_needles\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        seq = torch.randint(0, cfg.vocab_size - 200, (1, seq_len), device=device)\n",
    "        \n",
    "        # Place needles with unique markers\n",
    "        spacing = (seq_len - 50) // (n_needles + 1)\n",
    "        markers = []\n",
    "        needles = []\n",
    "        \n",
    "        for i in range(n_needles):\n",
    "            pos = spacing * (i + 1)\n",
    "            marker_id = base_marker + i  # Unique marker per needle\n",
    "            needle_id = torch.randint(100, cfg.vocab_size - 200, (1,)).item()\n",
    "            \n",
    "            seq[0, pos] = marker_id\n",
    "            seq[0, pos + 1] = needle_id\n",
    "            \n",
    "            markers.append(marker_id)\n",
    "            needles.append(needle_id)\n",
    "        \n",
    "        # Query each needle using its unique marker\n",
    "        for i, (marker_id, needle_id) in enumerate(zip(markers, needles)):\n",
    "            query_seq = seq.clone()\n",
    "            query_seq[0, -1] = marker_id  # Query with the specific marker\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits, _, _, _ = model(query_seq)\n",
    "            \n",
    "            pred = logits[0, -1].argmax().item()\n",
    "            if pred == needle_id:\n",
    "                per_needle_correct[i] += 1\n",
    "    \n",
    "    print(f\"  {n_needles} needles with UNIQUE markers:\")\n",
    "    print(f\"    Per-needle: {[f'{c/n_trials*100:.0f}%' for c in per_needle_correct]}\")\n",
    "    print(f\"    Average: {sum(per_needle_correct)/(n_needles*n_trials)*100:.1f}%\")\n",
    "    \n",
    "    return per_needle_correct\n",
    "\n",
    "# Test with unique markers\n",
    "print(\"\\n--- Testing with UNIQUE markers (untrained for this) ---\")\n",
    "for n in [2, 3, 5]:\n",
    "    test_multi_needle_unique_markers(model_rope, seq_len=512, n_needles=n, n_trials=20)\n",
    "\n",
    "# The model wasn't trained for unique markers, so it probably won't work well yet.\n",
    "# But this shows the right direction: each needle needs a unique key to query by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b509b5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING: NO RoPE in GDN, RoPE in SWA\n",
      "============================================================\n",
      "Model: 33,217,560 params\n",
      "Forward pass OK: torch.Size([2, 64, 50257])\n",
      "\n",
      "--- Training Pure Retrieval ---\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step    0: loss=11.000, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  200: loss=10.938, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  400: loss=11.000, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  600: loss=10.812, acc=0%\n",
      "  Accuracy (full vocab range): 0.0% (0/20)\n",
      "Step  800: loss=10.625, acc=0%\n",
      "  Accuracy (full vocab range): 65.0% (13/20)\n",
      "Step 1000: loss=6.438, acc=65%\n",
      "  Accuracy (full vocab range): 100.0% (20/20)\n",
      "Step 1200: loss=4.625, acc=100%\n",
      "  → Reached 95%+\n",
      "\n",
      "--- EVALUATION ---\n",
      "\n",
      "1. Single needle:\n",
      "  Accuracy (full vocab range): 100.0% (30/30)\n",
      "  Accuracy (full vocab range): 100.0% (30/30)\n",
      "  Accuracy (full vocab range): 93.3% (28/30)\n",
      "\n",
      "2. Multi-needle:\n",
      "  2 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['0%', '100%']\n",
      "    Any needle correct: 100.0%\n",
      "    Most retrieved: needle 2 (position 2/2)\n",
      "  3 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['0%', '0%', '100%']\n",
      "    Any needle correct: 100.0%\n",
      "    Most retrieved: needle 3 (position 3/3)\n",
      "  5 needles in seq_len=512:\n",
      "    Per-needle retrieval: ['0%', '0%', '0%', '0%', '100%']\n",
      "    Any needle correct: 100.0%\n",
      "    Most retrieved: needle 5 (position 5/5)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PROPER TEST: NO RoPE IN GDN, RoPE IN SWA\n",
    "# =============================================================================\n",
    "# Architecture understanding:\n",
    "# - GDN: Content-addressable storage (key = content, not position)\n",
    "# - SWA: Position-aware attention (RoPE on q/k for local window)\n",
    "#\n",
    "# The problem with multi-needle is that all MARKERs have the SAME key.\n",
    "# Solution: Use UNIQUE marker tokens for each slot, OR use context-dependent keys.\n",
    "\n",
    "import importlib\n",
    "import config as config_module\n",
    "import model as model_module\n",
    "config_module = importlib.reload(config_module)\n",
    "model_module = importlib.reload(model_module)\n",
    "from config import HybridConfig\n",
    "from model import TransparentHybrid\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING: NO RoPE in GDN, RoPE in SWA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Model WITHOUT RoPE (GDN pure content, SWA local attention)\n",
    "cfg_clean = HybridConfig(\n",
    "    d_model=512, n_heads=8, head_dim=64, value_dim=128,\n",
    "    layer_pattern=\"GS\", window_size=64, chunk_size=64,\n",
    "    shifted_value=True,\n",
    "    use_rope=True,  # RoPE now only applies to SWA\n",
    ")\n",
    "model_clean = TransparentHybrid(cfg_clean).to(DEVICE).to(torch.bfloat16)\n",
    "print(f\"Model: {model_clean.count_params():,} params\")\n",
    "\n",
    "# Forward test\n",
    "x_test = torch.randint(0, 1000, (2, 64), device=DEVICE)\n",
    "with torch.no_grad():\n",
    "    logits, _, diags, state = model_clean(x_test)\n",
    "print(f\"Forward pass OK: {logits.shape}\")\n",
    "\n",
    "# Train on retrieval\n",
    "optimizer = torch.optim.AdamW(model_clean.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "print(\"\\n--- Training Pure Retrieval ---\")\n",
    "model_clean.train()\n",
    "for step in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    ret_loss = compute_retrieval_loss(model_clean, seq_len=256, batch_size=8)\n",
    "    ret_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model_clean.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        model_clean.eval()\n",
    "        acc = test_varied_needles_wide(model_clean, seq_len=256, needle_pos=32, n_trials=20)\n",
    "        model_clean.train()\n",
    "        print(f\"Step {step:4d}: loss={ret_loss.item():.3f}, acc={acc*100:.0f}%\")\n",
    "        if acc >= 0.95:\n",
    "            print(f\"  → Reached 95%+\")\n",
    "            break\n",
    "\n",
    "# Test\n",
    "print(\"\\n--- EVALUATION ---\")\n",
    "model_clean.eval()\n",
    "print(\"\\n1. Single needle:\")\n",
    "for pos in [32, 64, 128]:\n",
    "    test_varied_needles_wide(model_clean, seq_len=256, needle_pos=pos, n_trials=30)\n",
    "\n",
    "print(\"\\n2. Multi-needle:\")\n",
    "for n in [2, 3, 5]:\n",
    "    test_multi_needle(model_clean, seq_len=512, n_needles=n, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88872c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING: Context-Dependent Keys for Multi-Needle\n",
      "============================================================\n",
      "Model params: 35,315,224\n",
      "Forward OK: torch.Size([2, 64, 50257])\n",
      "Step    0: loss=10.949, acc=0%\n",
      "Step  200: loss=11.042, acc=0%\n",
      "Step  400: loss=2.267, acc=0%\n",
      "Step  600: loss=0.020, acc=100%\n",
      "  → Good accuracy!\n",
      "\n",
      "--- MULTI-NEEDLE EVALUATION ---\n",
      "  2 needles: ['0%', '0%']\n",
      "  3 needles: ['0%', '0%', '0%']\n",
      "  5 needles: ['0%', '0%', '0%', '0%', '0%']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT: Context-Dependent Keys for Multi-Needle Retrieval\n",
    "# =============================================================================\n",
    "# PROBLEM: All MARKER tokens produce identical keys because keys are computed \n",
    "# from token embeddings only. When multiple MARKERs appear, the delta rule \n",
    "# overwrites previous associations (last needle wins).\n",
    "#\n",
    "# SOLUTION: Make keys depend on LOCAL CONTEXT, not just the token itself.\n",
    "# - Use causal convolution over the last few tokens\n",
    "# - Each MARKER now has a unique key based on what came before it\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "from config import HybridConfig\n",
    "from core import chunk_delta_rule\n",
    "from model import RMSNorm, SwiGLUFFN, SlidingWindowAttention\n",
    "\n",
    "class GDNWithContextKeys(nn.Module):\n",
    "    \"\"\"GDN with context-dependent keys via causal convolution.\"\"\"\n",
    "    def __init__(self, cfg: HybridConfig, layer_idx: int = 0, context_size: int = 4):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.layer_idx = layer_idx\n",
    "        self.context_size = context_size\n",
    "        H, K, V = cfg.n_heads, cfg.head_dim, cfg.value_dim\n",
    "        \n",
    "        # Context convolution for keys (causal)\n",
    "        self.key_conv = nn.Conv1d(\n",
    "            in_channels=cfg.d_model,\n",
    "            out_channels=cfg.d_model, \n",
    "            kernel_size=context_size,\n",
    "            padding=context_size - 1,\n",
    "            groups=1\n",
    "        )\n",
    "        \n",
    "        self.k_proj = nn.Linear(cfg.d_model, H * K, bias=False)\n",
    "        self.v_proj = nn.Linear(cfg.d_model, H * V, bias=False)\n",
    "        self.o_proj = nn.Linear(H * V, cfg.d_model, bias=False)\n",
    "        \n",
    "        self.beta_proj = nn.Linear(cfg.d_model, H, bias=True)\n",
    "        nn.init.constant_(self.beta_proj.bias, cfg.beta_bias)\n",
    "        self.g_proj = nn.Linear(cfg.d_model, H, bias=True)\n",
    "        nn.init.constant_(self.g_proj.bias, cfg.g_bias)\n",
    "        \n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        self.use_shifted_value = getattr(cfg, 'shifted_value', True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, initial_state: Optional[torch.Tensor] = None\n",
    "               ) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
    "        B, T, D = x.shape\n",
    "        H, K, V = self.cfg.n_heads, self.cfg.head_dim, self.cfg.value_dim\n",
    "        \n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        # CONTEXT-DEPENDENT KEYS\n",
    "        x_conv = x_norm.transpose(1, 2)  # [B, D, T]\n",
    "        x_conv = self.key_conv(x_conv)[:, :, :T]  # causal\n",
    "        x_conv = x_conv.transpose(1, 2)  # [B, T, D]\n",
    "        \n",
    "        k_full = self.k_proj(x_conv).view(B, T, H, K)  # Context-dependent!\n",
    "        v_full = self.v_proj(x_norm).view(B, T, H, V)\n",
    "        \n",
    "        if self.use_shifted_value and T > 1:\n",
    "            k = k_full[:, :-1]\n",
    "            v = v_full[:, 1:]\n",
    "            beta = torch.sigmoid(self.beta_proj(x_norm[:, :-1]))\n",
    "            g = torch.sigmoid(self.g_proj(x_norm[:, :-1]))\n",
    "            T_eff = T - 1\n",
    "        else:\n",
    "            k, v = k_full, v_full\n",
    "            beta = torch.sigmoid(self.beta_proj(x_norm))\n",
    "            g = torch.sigmoid(self.g_proj(x_norm))\n",
    "            T_eff = T\n",
    "        \n",
    "        k = F.normalize(k.float(), p=2, dim=-1).to(x.dtype)\n",
    "        \n",
    "        if initial_state is None:\n",
    "            initial_state = torch.zeros(B, H, K, V, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        o, final_state = chunk_delta_rule(k, v, beta, g, initial_state, chunk_size=32)\n",
    "        \n",
    "        o_flat = o.reshape(B, T_eff, H * V)\n",
    "        out = self.o_proj(o_flat)\n",
    "        \n",
    "        if self.use_shifted_value and T > 1:\n",
    "            out_full = torch.zeros(B, T, D, device=x.device, dtype=x.dtype)\n",
    "            out_full[:, 1:] = out\n",
    "        else:\n",
    "            out_full = out\n",
    "        \n",
    "        return x + out_full, final_state, {'gate_g': g.mean(), 'gate_beta': beta.mean()}\n",
    "\n",
    "\n",
    "class TransparentHybridContextKeys(nn.Module):\n",
    "    \"\"\"Hybrid model with context-dependent GDN keys.\"\"\"\n",
    "    def __init__(self, cfg: HybridConfig, context_size: int = 4):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        \n",
    "        for i, layer_type in enumerate(cfg.layer_pattern):\n",
    "            if layer_type == 'G':\n",
    "                self.layers.append(GDNWithContextKeys(cfg, i, context_size))\n",
    "            else:\n",
    "                self.layers.append(SlidingWindowAttention(cfg, i))\n",
    "            self.ffns.append(SwiGLUFFN(cfg.d_model))\n",
    "        \n",
    "        self.norm_f = RMSNorm(cfg.d_model)\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "        self.embed.weight = self.lm_head.weight\n",
    "    \n",
    "    def forward(self, input_ids, return_diagnostics=False):\n",
    "        x = self.embed(input_ids)\n",
    "        state = None\n",
    "        all_diag = []\n",
    "        \n",
    "        for i, (layer, ffn) in enumerate(zip(self.layers, self.ffns)):\n",
    "            lt = self.cfg.layer_pattern[i]\n",
    "            if lt == 'G':\n",
    "                x, state, diag = layer(x, initial_state=state)\n",
    "            else:\n",
    "                x, diag = layer(x, gdn_state=state)\n",
    "            x = ffn(x)\n",
    "            all_diag.append(diag)\n",
    "        \n",
    "        logits = self.lm_head(self.norm_f(x))\n",
    "        return (logits, state, all_diag) if return_diagnostics else (logits, state)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING: Context-Dependent Keys for Multi-Needle\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cfg_ctx = HybridConfig(\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    head_dim=64,\n",
    "    value_dim=128,\n",
    "    layer_pattern=\"GS\",\n",
    "    vocab_size=50257,\n",
    "    shifted_value=True,\n",
    "    use_rope=True,\n",
    ")\n",
    "\n",
    "model_ctx = TransparentHybridContextKeys(cfg_ctx, context_size=8).to(DEVICE).to(torch.bfloat16)\n",
    "print(f\"Model params: {sum(p.numel() for p in model_ctx.parameters()):,}\")\n",
    "\n",
    "# Forward test\n",
    "x_test = torch.randint(0, 50257, (2, 64), device=DEVICE)\n",
    "with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "    logits, _ = model_ctx(x_test)\n",
    "print(f\"Forward OK: {logits.shape}\")\n",
    "\n",
    "# Train (no GradScaler needed for bf16)\n",
    "optimizer = torch.optim.AdamW(model_ctx.parameters(), lr=1e-3)\n",
    "marker_token = 50256\n",
    "\n",
    "for step in range(2001):\n",
    "    seq_len = 256\n",
    "    batch_size = 8\n",
    "    \n",
    "    n_needles = random.randint(2, 4)\n",
    "    positions = sorted(random.sample(range(20, seq_len - 20), n_needles))\n",
    "    values = [random.randint(1000, 5000) for _ in range(n_needles)]\n",
    "    \n",
    "    seq = torch.randint(100, 1000, (batch_size, seq_len), device=DEVICE)\n",
    "    cue_idx = random.randint(0, n_needles - 1)\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for i, (pos, val) in enumerate(zip(positions, values)):\n",
    "            seq[b, pos] = marker_token\n",
    "            seq[b, pos + 1] = val\n",
    "        seq[b, -2] = marker_token\n",
    "        seq[b, -1] = values[cue_idx]\n",
    "    \n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits, _ = model_ctx(seq)\n",
    "        loss = F.cross_entropy(logits[:, -2].float(), seq[:, -1])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        with torch.no_grad():\n",
    "            pred = logits[:, -2].argmax(dim=-1)\n",
    "            acc = (pred == seq[:, -1]).float().mean().item() * 100\n",
    "        print(f\"Step {step:4d}: loss={loss.item():.3f}, acc={acc:.0f}%\")\n",
    "        if acc >= 90:\n",
    "            print(\"  → Good accuracy!\")\n",
    "            break\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n--- MULTI-NEEDLE EVALUATION ---\")\n",
    "\n",
    "for n_needles in [2, 3, 5]:\n",
    "    per_needle = [0] * n_needles\n",
    "    total = 30\n",
    "    \n",
    "    for _ in range(total):\n",
    "        positions = sorted(random.sample(range(20, 400), n_needles))\n",
    "        values = [random.randint(1000, 5000) for _ in range(n_needles)]\n",
    "        \n",
    "        seq = torch.randint(100, 1000, (1, 512), device=DEVICE)\n",
    "        for pos, val in zip(positions, values):\n",
    "            seq[0, pos] = marker_token\n",
    "            seq[0, pos + 1] = val\n",
    "        \n",
    "        for needle_idx in range(n_needles):\n",
    "            test_seq = seq.clone()\n",
    "            test_seq[0, -2] = marker_token  \n",
    "            test_seq[0, -1] = 0\n",
    "            \n",
    "            with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                logits, _ = model_ctx(test_seq)\n",
    "                pred = logits[0, -2].argmax().item()\n",
    "            \n",
    "            if pred == values[needle_idx]:\n",
    "                per_needle[needle_idx] += 1\n",
    "    \n",
    "    accs = [f\"{100*c/total:.0f}%\" for c in per_needle]\n",
    "    print(f\"  {n_needles} needles: {accs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "93754ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING: Slot-Specific Markers (MARKER_0, MARKER_1, ...)\n",
      "============================================================\n",
      "Model params: 33,217,560\n",
      "Step    0: loss=10.760, acc=0%\n",
      "Step  200: loss=10.909, acc=0%\n",
      "Step  400: loss=11.216, acc=0%\n",
      "Step  600: loss=10.700, acc=0%\n",
      "Step  800: loss=9.360, acc=0%\n",
      "Step 1000: loss=5.165, acc=0%\n",
      "Step 1200: loss=10.666, acc=0%\n",
      "Step 1400: loss=4.709, acc=0%\n",
      "Step 1600: loss=10.890, acc=0%\n",
      "Step 1800: loss=4.807, acc=0%\n",
      "Step 2000: loss=10.718, acc=0%\n",
      "\n",
      "--- SLOT-SPECIFIC MULTI-NEEDLE EVALUATION ---\n",
      "  2 needles: 0% (0/60)\n",
      "  3 needles: 0% (0/90)\n",
      "  5 needles: 0% (0/150)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT 2: Multi-Slot Markers (Different Markers for Different Slots)\n",
    "# =============================================================================\n",
    "# INSIGHT: We can't use the SAME marker for storage and retrieval if we want\n",
    "# multi-needle. The cue needs to uniquely identify WHICH slot to retrieve.\n",
    "#\n",
    "# SOLUTION: Use slot-specific markers.\n",
    "# - MARKER_0, MARKER_1, MARKER_2, ... for different slots\n",
    "# - Store: MARKER_i + value_i  \n",
    "# - Retrieve: MARKER_i + ? → should output value_i\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from config import HybridConfig\n",
    "from model import TransparentHybrid\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING: Slot-Specific Markers (MARKER_0, MARKER_1, ...)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use vocabulary slots for slot markers\n",
    "# Let's use tokens 50000-50010 as slot markers\n",
    "MARKER_OFFSET = 50000\n",
    "N_SLOTS = 10\n",
    "\n",
    "cfg_slots = HybridConfig(\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    head_dim=64,\n",
    "    value_dim=128,\n",
    "    layer_pattern=\"GS\",\n",
    "    vocab_size=50257,\n",
    "    shifted_value=True,\n",
    "    use_rope=True,\n",
    ")\n",
    "\n",
    "model_slots = TransparentHybrid(cfg_slots).to(DEVICE).to(torch.bfloat16)\n",
    "print(f\"Model params: {sum(p.numel() for p in model_slots.parameters()):,}\")\n",
    "\n",
    "# Train with slot-specific markers\n",
    "optimizer = torch.optim.AdamW(model_slots.parameters(), lr=1e-3)\n",
    "\n",
    "for step in range(2001):\n",
    "    seq_len = 256\n",
    "    batch_size = 8\n",
    "    \n",
    "    # Random number of needles with UNIQUE slot markers\n",
    "    n_needles = random.randint(2, min(5, N_SLOTS))\n",
    "    slots = random.sample(range(N_SLOTS), n_needles)  # Which slot markers to use\n",
    "    positions = sorted(random.sample(range(20, seq_len - 20), n_needles))\n",
    "    values = [random.randint(1000, 5000) for _ in range(n_needles)]\n",
    "    \n",
    "    seq = torch.randint(100, 1000, (batch_size, seq_len), device=DEVICE)\n",
    "    \n",
    "    # Pick which slot/needle to retrieve\n",
    "    cue_idx = random.randint(0, n_needles - 1)\n",
    "    cue_slot = slots[cue_idx]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for i, (pos, slot, val) in enumerate(zip(positions, slots, values)):\n",
    "            seq[b, pos] = MARKER_OFFSET + slot  # Slot-specific marker\n",
    "            seq[b, pos + 1] = val\n",
    "        seq[b, -2] = MARKER_OFFSET + cue_slot  # CUE uses the SAME slot marker\n",
    "        seq[b, -1] = values[cue_idx]  # Should retrieve the value for this slot\n",
    "    \n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits, _, _, _ = model_slots(seq)\n",
    "        loss = F.cross_entropy(logits[:, -2].float(), seq[:, -1])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        with torch.no_grad():\n",
    "            pred = logits[:, -2].argmax(dim=-1)\n",
    "            acc = (pred == seq[:, -1]).float().mean().item() * 100\n",
    "        print(f\"Step {step:4d}: loss={loss.item():.3f}, acc={acc:.0f}%\")\n",
    "        if acc >= 95:\n",
    "            print(\"  → Reached 95%!\")\n",
    "\n",
    "# Evaluate: For each slot, can we retrieve the correct value?\n",
    "print(\"\\n--- SLOT-SPECIFIC MULTI-NEEDLE EVALUATION ---\")\n",
    "\n",
    "for n_needles in [2, 3, 5]:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for _ in range(30):\n",
    "        slots = random.sample(range(N_SLOTS), n_needles)\n",
    "        positions = sorted(random.sample(range(20, 400), n_needles))\n",
    "        values = [random.randint(1000, 5000) for _ in range(n_needles)]\n",
    "        \n",
    "        seq = torch.randint(100, 1000, (1, 512), device=DEVICE)\n",
    "        for pos, slot, val in zip(positions, slots, values):\n",
    "            seq[0, pos] = MARKER_OFFSET + slot\n",
    "            seq[0, pos + 1] = val\n",
    "        \n",
    "        # Test each slot\n",
    "        for needle_idx in range(n_needles):\n",
    "            slot = slots[needle_idx]\n",
    "            test_seq = seq.clone()\n",
    "            test_seq[0, -2] = MARKER_OFFSET + slot  # Cue with the correct slot marker\n",
    "            test_seq[0, -1] = 0\n",
    "            \n",
    "            with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                logits, _, _, _ = model_slots(test_seq)\n",
    "                pred = logits[0, -2].argmax().item()\n",
    "            \n",
    "            if pred == values[needle_idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    print(f\"  {n_needles} needles: {100*correct/total:.0f}% ({correct}/{total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ceaca36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CURRICULUM: Single Slot → Multi-Slot\n",
      "============================================================\n",
      "\n",
      "--- Phase 1: Single Slot ---\n",
      "  [1 slots] Step    0: loss=10.819, acc=0%\n",
      "  [1 slots] Step  200: loss=10.651, acc=0%\n",
      "  [1 slots] Step  400: loss=10.784, acc=0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Phase 1: Single slot\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Phase 1: Single Slot ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m success, steps = \u001b[43mtrain_n_slots\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_curr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m accs, overall = evaluate_slots(model_curr, \u001b[32m1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Eval: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Overall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moverall\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mtrain_n_slots\u001b[39m\u001b[34m(model, opt, n_slots_train, steps, print_every)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (pos, slot, val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(positions, slots, values)):\n\u001b[32m     53\u001b[39m     seq[b, pos] = MARKER_OFFSET + slot\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     seq[b, pos + \u001b[32m1\u001b[39m] = val\n\u001b[32m     55\u001b[39m seq[b, -\u001b[32m2\u001b[39m] = MARKER_OFFSET + cue_slot\n\u001b[32m     56\u001b[39m seq[b, -\u001b[32m1\u001b[39m] = values[cue_idx]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT 3: Curriculum - Single Slot First, Then More\n",
    "# =============================================================================\n",
    "# Issue: Direct multi-slot training doesn't converge\n",
    "# Solution: Curriculum - start with 1 slot, then add more progressively\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from config import HybridConfig\n",
    "from model import TransparentHybrid\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CURRICULUM: Single Slot → Multi-Slot\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "MARKER_OFFSET = 50000\n",
    "N_SLOTS = 10\n",
    "\n",
    "cfg_curr = HybridConfig(\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    head_dim=64,\n",
    "    value_dim=128,\n",
    "    layer_pattern=\"GS\",\n",
    "    vocab_size=50257,\n",
    "    shifted_value=True,\n",
    "    use_rope=True,\n",
    ")\n",
    "\n",
    "model_curr = TransparentHybrid(cfg_curr).to(DEVICE).to(torch.bfloat16)\n",
    "optimizer = torch.optim.AdamW(model_curr.parameters(), lr=1e-3)\n",
    "\n",
    "def train_n_slots(model, opt, n_slots_train, steps, print_every=100):\n",
    "    \"\"\"Train with exactly n_slots_train needles/slots.\"\"\"\n",
    "    for step in range(steps):\n",
    "        seq_len = 256\n",
    "        batch_size = 8\n",
    "        \n",
    "        # Use specific slots\n",
    "        slots = list(range(n_slots_train))  # Use slots 0, 1, 2, ...\n",
    "        positions = sorted(random.sample(range(20, seq_len - 20), n_slots_train))\n",
    "        values = [random.randint(1000, 5000) for _ in range(n_slots_train)]\n",
    "        \n",
    "        seq = torch.randint(100, 1000, (batch_size, seq_len), device=DEVICE)\n",
    "        \n",
    "        cue_idx = random.randint(0, n_slots_train - 1)\n",
    "        cue_slot = slots[cue_idx]\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for i, (pos, slot, val) in enumerate(zip(positions, slots, values)):\n",
    "                seq[b, pos] = MARKER_OFFSET + slot\n",
    "                seq[b, pos + 1] = val\n",
    "            seq[b, -2] = MARKER_OFFSET + cue_slot\n",
    "            seq[b, -1] = values[cue_idx]\n",
    "        \n",
    "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            logits, _, _, _ = model(seq)\n",
    "            loss = F.cross_entropy(logits[:, -2].float(), seq[:, -1])\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if step % print_every == 0:\n",
    "            with torch.no_grad():\n",
    "                pred = logits[:, -2].argmax(dim=-1)\n",
    "                acc = (pred == seq[:, -1]).float().mean().item() * 100\n",
    "            print(f\"  [{n_slots_train} slots] Step {step:4d}: loss={loss.item():.3f}, acc={acc:.0f}%\")\n",
    "            if acc >= 95:\n",
    "                return True, step\n",
    "    return False, steps\n",
    "\n",
    "\n",
    "def evaluate_slots(model, n_slots_eval, n_trials=30):\n",
    "    \"\"\"Evaluate retrieval accuracy for each slot.\"\"\"\n",
    "    correct_per_slot = [0] * n_slots_eval\n",
    "    \n",
    "    for _ in range(n_trials):\n",
    "        slots = list(range(n_slots_eval))\n",
    "        positions = sorted(random.sample(range(20, 400), n_slots_eval))\n",
    "        values = [random.randint(1000, 5000) for _ in range(n_slots_eval)]\n",
    "        \n",
    "        seq = torch.randint(100, 1000, (1, 512), device=DEVICE)\n",
    "        for pos, slot, val in zip(positions, slots, values):\n",
    "            seq[0, pos] = MARKER_OFFSET + slot\n",
    "            seq[0, pos + 1] = val\n",
    "        \n",
    "        for slot_idx in range(n_slots_eval):\n",
    "            test_seq = seq.clone()\n",
    "            test_seq[0, -2] = MARKER_OFFSET + slot_idx\n",
    "            test_seq[0, -1] = 0\n",
    "            \n",
    "            with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                logits, _, _, _ = model(test_seq)\n",
    "                pred = logits[0, -2].argmax().item()\n",
    "            \n",
    "            if pred == values[slot_idx]:\n",
    "                correct_per_slot[slot_idx] += 1\n",
    "    \n",
    "    accs = [f\"{100*c/n_trials:.0f}%\" for c in correct_per_slot]\n",
    "    return accs, sum(correct_per_slot) / (n_trials * n_slots_eval) * 100\n",
    "\n",
    "\n",
    "# Phase 1: Single slot\n",
    "print(\"\\n--- Phase 1: Single Slot ---\")\n",
    "success, steps = train_n_slots(model_curr, optimizer, 1, steps=1000, print_every=200)\n",
    "accs, overall = evaluate_slots(model_curr, 1)\n",
    "print(f\"  Eval: {accs}, Overall: {overall:.0f}%\")\n",
    "\n",
    "if overall >= 80:\n",
    "    # Phase 2: Two slots\n",
    "    print(\"\\n--- Phase 2: Two Slots ---\")\n",
    "    success, steps = train_n_slots(model_curr, optimizer, 2, steps=1000, print_every=200)\n",
    "    accs, overall = evaluate_slots(model_curr, 2)\n",
    "    print(f\"  Eval: {accs}, Overall: {overall:.0f}%\")\n",
    "    \n",
    "    if overall >= 80:\n",
    "        # Phase 3: Three slots\n",
    "        print(\"\\n--- Phase 3: Three Slots ---\")\n",
    "        success, steps = train_n_slots(model_curr, optimizer, 3, steps=1000, print_every=200)\n",
    "        accs, overall = evaluate_slots(model_curr, 3)\n",
    "        print(f\"  Eval: {accs}, Overall: {overall:.0f}%\")\n",
    "\n",
    "        if overall >= 80:\n",
    "            # Phase 4: Five slots\n",
    "            print(\"\\n--- Phase 4: Five Slots ---\")\n",
    "            success, steps = train_n_slots(model_curr, optimizer, 5, steps=1500, print_every=200)\n",
    "            accs, overall = evaluate_slots(model_curr, 5)\n",
    "            print(f\"  Eval: {accs}, Overall: {overall:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44c259c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing echo retrieval (does model remember seeing a token):\n",
      "\n",
      "--- FRESH MODEL V2 (regularized) ---\n",
      "  Echo retrieval (unique token in top-5): 0.0% (0/30)\n",
      "\n",
      "--- UNTRAINED MODEL ---\n",
      "  Echo retrieval (unique token in top-5): 13.3% (4/30)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UNDERSTANDING THE ARCHITECTURE LIMITATION\n",
    "# =============================================================================\n",
    "# The Delta Rule stores (k_t, v_t) where BOTH come from token t.\n",
    "# So if we see token X, we store X's embedding. \n",
    "# We can query for X and get X back, but NOT what came AFTER X.\n",
    "\n",
    "# NIAH requires: see MARKER, store VALUE that follows\n",
    "# But architecture stores: see MARKER, store MARKER's embedding\n",
    "\n",
    "# Let's verify with a simpler test:\n",
    "# \"Repeat back the token you saw at position P when you see it as a cue\"\n",
    "\n",
    "def test_echo_retrieval(model, seq_len=128, n_trials=30):\n",
    "    \"\"\"\n",
    "    Test: Can the model echo back a specific token when cued with itself?\n",
    "    \n",
    "    Format:\n",
    "    - Place unique token X at position P\n",
    "    - At end, place X again as cue\n",
    "    - Should predict X (the token itself, not what followed)\n",
    "    \n",
    "    This matches what the architecture can actually store.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    cfg = model.cfg\n",
    "    \n",
    "    correct = 0\n",
    "    for trial in range(n_trials):\n",
    "        # Use a unique token for this trial\n",
    "        unique_token = cfg.vocab_size - 100 + (trial % 50)\n",
    "        \n",
    "        seq = torch.randint(0, cfg.vocab_size - 200, (1, seq_len), device=device)\n",
    "        \n",
    "        # Place unique token early in sequence\n",
    "        pos = torch.randint(10, seq_len // 2, (1,)).item()\n",
    "        seq[0, pos] = unique_token\n",
    "        \n",
    "        # Place same token at end as \"cue\" for retrieval\n",
    "        seq[0, -1] = unique_token\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _, _, _ = model(seq)\n",
    "        \n",
    "        # After seeing unique_token again, what does the model predict?\n",
    "        pred = logits[0, -1].argmax().item()\n",
    "        \n",
    "        # For \"echo\" we just check if model recognizes this token\n",
    "        # If state stores token info, the prediction should be influenced by having seen it before\n",
    "        # But this is more of a \"does the model remember seeing this\" test\n",
    "        \n",
    "        # Actually, let's check if the logit for unique_token is in top-5\n",
    "        top5 = logits[0, -1].topk(5).indices.tolist()\n",
    "        if unique_token in top5:\n",
    "            correct += 1\n",
    "    \n",
    "    acc = correct / n_trials\n",
    "    print(f\"  Echo retrieval (unique token in top-5): {acc*100:.1f}% ({correct}/{n_trials})\")\n",
    "    return acc\n",
    "\n",
    "print(\"Testing echo retrieval (does model remember seeing a token):\")\n",
    "print(\"\\n--- FRESH MODEL V2 (regularized) ---\")\n",
    "echo_fresh = test_echo_retrieval(fresh_model_v2, seq_len=256, n_trials=30)\n",
    "\n",
    "print(\"\\n--- UNTRAINED MODEL ---\") \n",
    "echo_untrained = test_echo_retrieval(untrained_model, seq_len=256, n_trials=30)\n",
    "\n",
    "# Expected: if state helps, trained model should score higher than untrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SAVE MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'config': cfg,\n",
    "#     'history': history,\n",
    "# }, 'groundthink_v7_checkpoint.pt')\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb7a76",
   "metadata": {},
   "source": [
    "### [SYNC LOG] analyze_gradients marker/cue logic updated\n",
    "- Marker and cue positions in analyze_gradients are now set relative to seq_len (marker at seq_len//4, cue at seq_len-1).\n",
    "- This ensures gradient analysis tests are meaningful for any sequence length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcced0",
   "metadata": {},
   "source": [
    "### [SYNC LOG] All test/reporting functions now use NEW_T\n",
    "- Calls to proper_niah_test, test_niah_by_distance, run_full_diagnostic, and analyze_gradients now explicitly use seq_len=NEW_T.\n",
    "- This ensures all evaluation and reporting is consistent with the configured sequence length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "11dc0761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INTERFERENCE & KEY SIMILARITY DIAGNOSTIC\n",
      "============================================================\n",
      "Config: H=8, K=64, V=128\n",
      "\n",
      "----------------------------------------\n",
      "TEST 1: Key Similarity Distribution\n",
      "----------------------------------------\n",
      "Random 64D unit vectors (n=1000):\n",
      "  Mean |dot|:       0.1002\n",
      "  Std |dot|:        0.0748\n",
      "  Max |dot|:        0.5486\n",
      "  % with |dot|>0.3: 1.51%\n",
      "  % with |dot|>0.5: 0.00%\n",
      "\n",
      "  Theoretical E[|dot|] for K=64: 0.0997\n",
      "\n",
      "----------------------------------------\n",
      "TEST 2: Interference Scaling (Delta Rule)\n",
      "----------------------------------------\n",
      "  n_writes | Mean Rel. Error |       Interpretation\n",
      "--------------------------------------------------\n",
      "         1 |          0.0000 |              Perfect\n",
      "         5 |          0.2621 |             Degraded\n",
      "        10 |          0.3899 |             Degraded\n",
      "        25 |          0.6457 |               FAILED\n",
      "        50 |          0.9160 |               FAILED\n",
      "       100 |          1.1921 |               FAILED\n",
      "       200 |          1.3709 |               FAILED\n",
      "       500 |          1.4139 |               FAILED\n",
      "\n",
      "----------------------------------------\n",
      "TEST 3: Learned Key Similarity (Real Model)\n",
      "----------------------------------------\n",
      "Per-head key similarity (untrained model):\n",
      "  Head 0: mean|dot|=0.1056, max=1.0000\n",
      "  Head 1: mean|dot|=0.1059, max=1.0000\n",
      "  Head 2: mean|dot|=0.1064, max=1.0000\n",
      "  Head 3: mean|dot|=0.1058, max=1.0000\n",
      "  Head 4: mean|dot|=0.1070, max=1.0000\n",
      "  Head 5: mean|dot|=0.1059, max=1.0000\n",
      "  Head 6: mean|dot|=0.1061, max=1.0000\n",
      "  Head 7: mean|dot|=0.1060, max=1.0000\n",
      "\n",
      "----------------------------------------\n",
      "TEST 4: MARKER Token Key Similarity\n",
      "----------------------------------------\n",
      "Similarity between MARKER keys at different positions:\n",
      "  pos  20 vs  50: mean_sim=1.0000, per_head=[1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]\n",
      "  pos  20 vs 100: mean_sim=1.0000, per_head=[1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]\n",
      "  pos  20 vs 150: mean_sim=1.0000, per_head=[1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]\n",
      "  pos  20 vs 200: mean_sim=1.0000, per_head=[1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]\n",
      "  pos  50 vs 100: mean_sim=1.0000, per_head=[1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]\n",
      "  pos  50 vs 150: mean_sim=1.0000, per_head=[1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]\n",
      "  pos  50 vs 200: mean_sim=1.0000, per_head=[1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]\n",
      "  pos 100 vs 150: mean_sim=1.0000, per_head=[1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]\n",
      "  pos 100 vs 200: mean_sim=1.0000, per_head=[1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]\n",
      "  pos 150 vs 200: mean_sim=1.0000, per_head=[1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]\n",
      "\n",
      "============================================================\n",
      "DIAGNOSIS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Key findings:\n",
      "1. Random keys in K=64 are reasonably orthogonal (mean |dot| ≈ 0.1)\n",
      "2. Delta rule retrieval degrades with more writes (check numbers above)\n",
      "3. MARKER tokens at different positions have IDENTICAL keys (similarity ≈ 1.0)\n",
      "   → This is WHY multi-needle fails: same key = overwrite at same address\n",
      "\n",
      "The fix needed: Make MARKER keys POSITION-DEPENDENT, not token-dependent.\n",
      "Options:\n",
      "  A) Add positional encoding to keys (RoPE, learned PE)\n",
      "  B) Context-dependent keys (conv, local attention on key projection)\n",
      "  C) Per-head positional specialization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DIAGNOSTIC: Interference Scaling & Key Similarity\n",
    "# =============================================================================\n",
    "# Before trying fixes, measure the ACTUAL problem:\n",
    "# 1. How does retrieval error scale with number of writes?\n",
    "# 2. How similar are our keys? (Are collisions even the issue?)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "\n",
    "import config as config_module\n",
    "import model as model_module\n",
    "config_module = importlib.reload(config_module)\n",
    "model_module = importlib.reload(model_module)\n",
    "\n",
    "from config import HybridConfig\n",
    "from model import TransparentHybrid\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INTERFERENCE & KEY SIMILARITY DIAGNOSTIC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cfg = HybridConfig(\n",
    "    d_model=512, n_heads=8, head_dim=64, value_dim=128,\n",
    "    layer_pattern=\"GS\", shifted_value=True,\n",
    ")\n",
    "H, K, V = cfg.n_heads, cfg.head_dim, cfg.value_dim\n",
    "print(f\"Config: H={H}, K={K}, V={V}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TEST 1: Key Similarity Distribution\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"TEST 1: Key Similarity Distribution\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Generate random normalized keys (what our model produces)\n",
    "n_keys = 1000\n",
    "test_keys = F.normalize(torch.randn(n_keys, K, device=DEVICE), dim=-1)\n",
    "\n",
    "# Compute all pairwise dot products\n",
    "dots = test_keys @ test_keys.T  # [n_keys, n_keys]\n",
    "\n",
    "# Get upper triangle (exclude diagonal = self-similarity)\n",
    "triu_mask = torch.triu(torch.ones(n_keys, n_keys, device=DEVICE), diagonal=1).bool()\n",
    "triu_dots = dots[triu_mask]\n",
    "\n",
    "print(f\"Random {K}D unit vectors (n={n_keys}):\")\n",
    "print(f\"  Mean |dot|:       {triu_dots.abs().mean().item():.4f}\")\n",
    "print(f\"  Std |dot|:        {triu_dots.abs().std().item():.4f}\")\n",
    "print(f\"  Max |dot|:        {triu_dots.abs().max().item():.4f}\")\n",
    "print(f\"  % with |dot|>0.3: {(triu_dots.abs() > 0.3).float().mean().item()*100:.2f}%\")\n",
    "print(f\"  % with |dot|>0.5: {(triu_dots.abs() > 0.5).float().mean().item()*100:.2f}%\")\n",
    "\n",
    "# Theoretical expectation for random unit vectors in K dimensions\n",
    "# E[|dot|] ≈ sqrt(2/pi) / sqrt(K)\n",
    "expected_mean = (2/3.14159)**0.5 / (K**0.5)\n",
    "print(f\"\\n  Theoretical E[|dot|] for K={K}: {expected_mean:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TEST 2: Interference Scaling with Delta Rule\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"TEST 2: Interference Scaling (Delta Rule)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def test_interference(n_writes, n_trials=10):\n",
    "    \"\"\"Test retrieval error after n_writes using exact delta rule.\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    for _ in range(n_trials):\n",
    "        state = torch.zeros(1, H, K, V, device=DEVICE)\n",
    "        keys, values = [], []\n",
    "        \n",
    "        for i in range(n_writes):\n",
    "            k = F.normalize(torch.randn(1, H, K, device=DEVICE), dim=-1)\n",
    "            v = torch.randn(1, H, V, device=DEVICE)\n",
    "            keys.append(k)\n",
    "            values.append(v)\n",
    "            \n",
    "            # Delta rule: S += β * (v - S·k) ⊗ k\n",
    "            # With β=1 (full write)\n",
    "            pred = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "            error = v - pred\n",
    "            outer = torch.einsum('bhv,bhk->bhkv', error, k)\n",
    "            state = state + outer\n",
    "        \n",
    "        # Retrieve FIRST item written\n",
    "        retrieved = torch.einsum('bhkv,bhk->bhv', state, keys[0])\n",
    "        rel_error = (retrieved - values[0]).norm() / values[0].norm()\n",
    "        errors.append(rel_error.item())\n",
    "    \n",
    "    return sum(errors) / len(errors)\n",
    "\n",
    "print(f\"{'n_writes':>10} | {'Mean Rel. Error':>15} | {'Interpretation':>20}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for n in [1, 5, 10, 25, 50, 100, 200, 500]:\n",
    "    err = test_interference(n, n_trials=20)\n",
    "    if err < 0.01:\n",
    "        interp = \"Perfect\"\n",
    "    elif err < 0.1:\n",
    "        interp = \"Good\"\n",
    "    elif err < 0.5:\n",
    "        interp = \"Degraded\"\n",
    "    else:\n",
    "        interp = \"FAILED\"\n",
    "    print(f\"{n:>10} | {err:>15.4f} | {interp:>20}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TEST 3: What about LEARNED keys from actual model?\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"TEST 3: Learned Key Similarity (Real Model)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "model = TransparentHybrid(cfg).to(DEVICE).to(torch.bfloat16)\n",
    "\n",
    "# Get GDN layer\n",
    "gdn_layer = [l for l in model.layers if hasattr(l, 'k_proj')][0]\n",
    "\n",
    "# Generate keys for random tokens\n",
    "n_tokens = 500\n",
    "tokens = torch.randint(100, cfg.vocab_size - 100, (1, n_tokens), device=DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb = model.embed(tokens)\n",
    "    x_norm = gdn_layer.norm(emb)\n",
    "    keys = gdn_layer.k_proj(x_norm).view(1, n_tokens, H, K)\n",
    "    keys = F.normalize(keys.float(), dim=-1)  # [1, n_tokens, H, K]\n",
    "\n",
    "# Check similarity per head\n",
    "print(\"Per-head key similarity (untrained model):\")\n",
    "for h in range(H):\n",
    "    k_h = keys[0, :, h, :]  # [n_tokens, K]\n",
    "    dots_h = k_h @ k_h.T\n",
    "    triu_h = dots_h[triu_mask[:n_tokens, :n_tokens]]\n",
    "    print(f\"  Head {h}: mean|dot|={triu_h.abs().mean().item():.4f}, max={triu_h.abs().max().item():.4f}\")\n",
    "\n",
    "# Check MARKER token keys specifically\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"TEST 4: MARKER Token Key Similarity\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create sequences with MARKER at different positions\n",
    "seq = torch.randint(100, 1000, (1, 256), device=DEVICE)\n",
    "marker_positions = [20, 50, 100, 150, 200]\n",
    "for pos in marker_positions:\n",
    "    seq[0, pos] = cfg.marker_token\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb = model.embed(seq)\n",
    "    x_norm = gdn_layer.norm(emb)\n",
    "    all_keys = gdn_layer.k_proj(x_norm).view(1, 256, H, K)\n",
    "    all_keys = F.normalize(all_keys.float(), dim=-1)\n",
    "    \n",
    "    # Extract marker keys\n",
    "    marker_keys = all_keys[0, marker_positions, :, :]  # [5, H, K]\n",
    "\n",
    "# Pairwise similarity between MARKER keys at different positions\n",
    "print(\"Similarity between MARKER keys at different positions:\")\n",
    "for i in range(len(marker_positions)):\n",
    "    for j in range(i+1, len(marker_positions)):\n",
    "        k_i = marker_keys[i]  # [H, K]\n",
    "        k_j = marker_keys[j]  # [H, K]\n",
    "        sim = (k_i * k_j).sum(dim=-1)  # [H] - per head similarity\n",
    "        print(f\"  pos {marker_positions[i]:3d} vs {marker_positions[j]:3d}: mean_sim={sim.mean().item():.4f}, per_head=[{', '.join(f'{s:.2f}' for s in sim.tolist())}]\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONCLUSION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DIAGNOSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Key findings:\n",
    "1. Random keys in K=64 are reasonably orthogonal (mean |dot| ≈ 0.1)\n",
    "2. Delta rule retrieval degrades with more writes (check numbers above)\n",
    "3. MARKER tokens at different positions have IDENTICAL keys (similarity ≈ 1.0)\n",
    "   → This is WHY multi-needle fails: same key = overwrite at same address\n",
    "\n",
    "The fix needed: Make MARKER keys POSITION-DEPENDENT, not token-dependent.\n",
    "Options:\n",
    "  A) Add positional encoding to keys (RoPE, learned PE)\n",
    "  B) Context-dependent keys (conv, local attention on key projection)\n",
    "  C) Per-head positional specialization\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3d9b6f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SOLUTION A: Positional Encoding on Keys\n",
      "======================================================================\n",
      "\n",
      "Idea: k_i = f(token) + g(position)\n",
      "If g(pos) varies enough, keys become distinguishable.\n",
      "\n",
      "  pos_scale=0.0: mean_err=1.1148, key_sim=1.0000 ✗\n",
      "  pos_scale=0.1: mean_err=1.0875, key_sim=0.9773 ✗\n",
      "  pos_scale=0.3: mean_err=1.0339, key_sim=0.9307 ✗\n",
      "  pos_scale=0.5: mean_err=1.0142, key_sim=0.9126 ✗\n",
      "  pos_scale=1.0: mean_err=0.9992, key_sim=0.8985 ✗\n",
      "\n",
      "======================================================================\n",
      "SOLUTION B: Orthogonal Key Projection\n",
      "======================================================================\n",
      "\n",
      "Idea: Force keys to be orthogonal to each other.\n",
      "Use Gram-Schmidt or random orthogonal basis.\n",
      "\n",
      "  n_writes= 2: mean_err=0.000000, ortho_err=0.000000 ✓\n",
      "  n_writes= 5: mean_err=0.000000, ortho_err=0.000000 ✓\n",
      "  n_writes=10: mean_err=0.000000, ortho_err=0.000000 ✓\n",
      "  n_writes=25: mean_err=0.000000, ortho_err=0.000001 ✓\n",
      "  n_writes=50: mean_err=0.000000, ortho_err=0.000002 ✓\n",
      "\n",
      "======================================================================\n",
      "SOLUTION C: Error-Correcting Writes\n",
      "======================================================================\n",
      "\n",
      "Idea: Before writing v, compute v' = v + correction\n",
      "where correction accounts for interference from existing state.\n",
      "\n",
      "With IDENTICAL keys:\n",
      "  n_iters= 0: mean_err=1.1363 ✗\n",
      "  n_iters= 1: mean_err=1.1363 ✗\n",
      "  n_iters= 5: mean_err=1.1363 ✗\n",
      "  n_iters=10: mean_err=1.1363 ✗\n",
      "  n_iters=20: mean_err=1.1363 ✗\n",
      "\n",
      "With SIMILAR keys (sim ≈ 0.9):\n",
      "  n_iters= 0: mean_err=1.0454 ✗\n",
      "  n_iters= 1: mean_err=0.9988 ✗\n",
      "  n_iters= 5: mean_err=0.8385 ✗\n",
      "  n_iters=10: mean_err=0.6835 ✗\n",
      "  n_iters=20: mean_err=0.4683 ✗\n",
      "\n",
      "======================================================================\n",
      "SUMMARY: What Works?\n",
      "======================================================================\n",
      "\n",
      "A) Positional Encoding:\n",
      "   - Works IF pos_scale is high enough to make keys distinguishable\n",
      "   - Problem: In NN, model produces same key for same token regardless of position\n",
      "   - Fix: Add positional embedding BEFORE key projection, or position-dependent key proj\n",
      "\n",
      "B) Orthogonal Keys:\n",
      "   - PERFECT retrieval for n_writes ≤ K (64 writes with K=64)\n",
      "   - Fundamental capacity limit: can't store more than K orthogonal vectors\n",
      "   - For multi-needle (2-5 needles), this is MORE than enough!\n",
      "\n",
      "C) Error-Correcting Writes:\n",
      "   - Does NOT help with identical keys (convergence impossible)\n",
      "   - Helps slightly with similar keys\n",
      "   - Not practical: requires multiple passes, not causal\n",
      "\n",
      "CONCLUSION: \n",
      "→ Solution B (orthogonal keys) proves the math CAN work for multi-needle\n",
      "→ The issue is making the NN learn to produce distinct keys for MARKER tokens\n",
      "→ Since MARKER tokens are identical, we need position-aware key generation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================\n",
    "PURE MATH TEST: 3 SOLUTIONS FOR MULTI-NEEDLE RETRIEVAL\n",
    "============================================================\n",
    "The delta rule: S_t = S_{t-1} + β * (v - S_{t-1}·k) ⊗ k\n",
    "Retrieval:      r = S · q\n",
    "\n",
    "Problem: With identical keys (k_1 = k_2 = ... = k_n), only last write survives.\n",
    "\n",
    "Testing 3 solutions at the MATH level (no NN training):\n",
    "A) Positional encoding on keys (make keys position-dependent)\n",
    "B) Orthogonal key projection (project to orthogonal subspace per write)\n",
    "C) Error-correcting writes (modify value to account for existing state)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "K = 64   # Key dimension\n",
    "V = 128  # Value dimension  \n",
    "H = 8    # Heads (for testing per-head variations)\n",
    "\n",
    "def simulate_delta_rule(keys, values, queries, beta=1.0):\n",
    "    \"\"\"\n",
    "    Pure delta rule simulation.\n",
    "    keys: [n_writes, K]\n",
    "    values: [n_writes, V]\n",
    "    queries: [n_queries, K]\n",
    "    Returns: [n_queries, V] retrieved values, [n_queries] retrieval errors\n",
    "    \n",
    "    State S is [K, V], retrieval is q @ S = [V]\n",
    "    \"\"\"\n",
    "    n_writes = keys.shape[0]\n",
    "    S = torch.zeros(K, V)  # State matrix [K, V]\n",
    "    \n",
    "    # Sequential writes\n",
    "    for i in range(n_writes):\n",
    "        k = keys[i]  # [K]\n",
    "        v = values[i]  # [V]\n",
    "        # Delta rule: S += β * (v - k·S) ⊗ k  \n",
    "        # k·S = [K] @ [K,V] but we need to do k^T @ S for retrieval\n",
    "        # Actually: S is [K,V], k is [K], so k @ S doesn't work directly\n",
    "        # Standard formulation: S[K,V], retrieve with q gives q @ S = [V]\n",
    "        # For delta rule write: S += k ⊗ (v - S^T @ k)^T = k ⊗ (v - (k @ S))\n",
    "        # Wait, let's be careful: if S is [K,V] and we retrieve with q @ S\n",
    "        # then k @ S gives [V], which is current value at key k\n",
    "        current = k @ S  # [V] - what's currently stored at this key\n",
    "        delta = v - current  # [V]\n",
    "        S = S + beta * torch.outer(k, delta)  # [K,V] + outer([K], [V]) = [K,V]\n",
    "    \n",
    "    # Retrieval\n",
    "    retrieved = queries @ S  # [n_queries, V]\n",
    "    \n",
    "    # Compute errors (assuming queries[i] should retrieve values[i])\n",
    "    errors = []\n",
    "    for i in range(min(len(queries), len(values))):\n",
    "        err = torch.norm(retrieved[i] - values[i]) / torch.norm(values[i])\n",
    "        errors.append(err.item())\n",
    "    \n",
    "    return retrieved, errors\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SOLUTION A: Positional Encoding on Keys\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nIdea: k_i = f(token) + g(position)\")\n",
    "print(\"If g(pos) varies enough, keys become distinguishable.\\n\")\n",
    "\n",
    "# Simulate: base key (same for all) + positional component\n",
    "n_writes = 5\n",
    "base_key = F.normalize(torch.randn(K), dim=0)\n",
    "values = F.normalize(torch.randn(n_writes, V), dim=1)\n",
    "\n",
    "# Different positional encoding strengths\n",
    "for pos_scale in [0.0, 0.1, 0.3, 0.5, 1.0]:\n",
    "    # Create position-dependent keys\n",
    "    keys = []\n",
    "    for i in range(n_writes):\n",
    "        # Sinusoidal positional encoding\n",
    "        pos_enc = torch.zeros(K)\n",
    "        for j in range(K // 2):\n",
    "            freq = 1.0 / (10000 ** (2 * j / K))\n",
    "            pos_enc[2*j] = math.sin(i * freq)\n",
    "            pos_enc[2*j + 1] = math.cos(i * freq)\n",
    "        \n",
    "        key_i = base_key + pos_scale * pos_enc\n",
    "        key_i = F.normalize(key_i, dim=0)  # Re-normalize\n",
    "        keys.append(key_i)\n",
    "    \n",
    "    keys = torch.stack(keys)\n",
    "    \n",
    "    # Use same keys for queries (exact match retrieval)\n",
    "    _, errors = simulate_delta_rule(keys, values, keys)\n",
    "    mean_err = sum(errors) / len(errors)\n",
    "    \n",
    "    # Check key similarity\n",
    "    key_sims = []\n",
    "    for i in range(n_writes):\n",
    "        for j in range(i+1, n_writes):\n",
    "            key_sims.append(torch.dot(keys[i], keys[j]).item())\n",
    "    mean_sim = sum(key_sims) / len(key_sims) if key_sims else 1.0\n",
    "    \n",
    "    status = \"✓\" if mean_err < 0.1 else \"✗\"\n",
    "    print(f\"  pos_scale={pos_scale:.1f}: mean_err={mean_err:.4f}, key_sim={mean_sim:.4f} {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SOLUTION B: Orthogonal Key Projection\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nIdea: Force keys to be orthogonal to each other.\")\n",
    "print(\"Use Gram-Schmidt or random orthogonal basis.\\n\")\n",
    "\n",
    "# Use truly orthogonal keys (via QR decomposition)\n",
    "for n_writes in [2, 5, 10, 25, 50]:\n",
    "    # Generate random orthogonal keys\n",
    "    random_matrix = torch.randn(n_writes, K)\n",
    "    if n_writes <= K:\n",
    "        Q, R = torch.linalg.qr(random_matrix.T)\n",
    "        keys = Q[:, :n_writes].T  # [n_writes, K], orthonormal\n",
    "    else:\n",
    "        # More writes than dimensions - can't be orthogonal\n",
    "        keys = F.normalize(random_matrix, dim=1)\n",
    "    \n",
    "    values = F.normalize(torch.randn(n_writes, V), dim=1)\n",
    "    \n",
    "    _, errors = simulate_delta_rule(keys, values, keys)\n",
    "    mean_err = sum(errors) / len(errors)\n",
    "    \n",
    "    # Verify orthogonality\n",
    "    if n_writes <= K:\n",
    "        ortho_err = torch.norm(keys @ keys.T - torch.eye(n_writes)).item()\n",
    "    else:\n",
    "        ortho_err = float('inf')\n",
    "    \n",
    "    status = \"✓\" if mean_err < 0.1 else \"✗\"\n",
    "    print(f\"  n_writes={n_writes:2d}: mean_err={mean_err:.6f}, ortho_err={ortho_err:.6f} {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SOLUTION C: Error-Correcting Writes\")  \n",
    "print(\"=\" * 70)\n",
    "print(\"\\nIdea: Before writing v, compute v' = v + correction\")\n",
    "print(\"where correction accounts for interference from existing state.\\n\")\n",
    "\n",
    "def simulate_delta_with_correction(keys, values, queries, beta=1.0, n_iters=5):\n",
    "    \"\"\"\n",
    "    Iterative error correction: adjust values to compensate for interference.\n",
    "    \"\"\"\n",
    "    n_writes = keys.shape[0]\n",
    "    \n",
    "    # Start with original values, iteratively correct\n",
    "    corrected_values = values.clone()\n",
    "    \n",
    "    for iteration in range(n_iters):\n",
    "        S = torch.zeros(K, V)\n",
    "        \n",
    "        # Write with current corrected values\n",
    "        for i in range(n_writes):\n",
    "            k = keys[i]\n",
    "            v = corrected_values[i]\n",
    "            current = k @ S  # Fixed: k @ S not S @ k\n",
    "            delta = v - current\n",
    "            S = S + beta * torch.outer(k, delta)\n",
    "        \n",
    "        # Measure retrieval errors and compute corrections\n",
    "        for i in range(n_writes):\n",
    "            retrieved = keys[i] @ S\n",
    "            error = values[i] - retrieved  # What we wanted - what we got\n",
    "            corrected_values[i] = corrected_values[i] + 0.5 * error  # Partial correction\n",
    "    \n",
    "    # Final retrieval\n",
    "    S = torch.zeros(K, V)\n",
    "    for i in range(n_writes):\n",
    "        k = keys[i]\n",
    "        v = corrected_values[i]\n",
    "        current = k @ S  # Fixed\n",
    "        delta = v - current\n",
    "        S = S + beta * torch.outer(k, delta)\n",
    "    \n",
    "    retrieved = queries @ S\n",
    "    errors = []\n",
    "    for i in range(min(len(queries), len(values))):\n",
    "        err = torch.norm(retrieved[i] - values[i]) / torch.norm(values[i])\n",
    "        errors.append(err.item())\n",
    "    \n",
    "    return retrieved, errors\n",
    "\n",
    "# Test with identical keys (worst case)\n",
    "n_writes = 5\n",
    "base_key = F.normalize(torch.randn(K), dim=0)\n",
    "keys = base_key.unsqueeze(0).repeat(n_writes, 1)  # All identical\n",
    "values = F.normalize(torch.randn(n_writes, V), dim=1)\n",
    "\n",
    "print(\"With IDENTICAL keys:\")\n",
    "for n_iters in [0, 1, 5, 10, 20]:\n",
    "    if n_iters == 0:\n",
    "        _, errors = simulate_delta_rule(keys, values, keys)\n",
    "    else:\n",
    "        _, errors = simulate_delta_with_correction(keys, values, keys, n_iters=n_iters)\n",
    "    mean_err = sum(errors) / len(errors)\n",
    "    status = \"✓\" if mean_err < 0.1 else \"✗\"\n",
    "    print(f\"  n_iters={n_iters:2d}: mean_err={mean_err:.4f} {status}\")\n",
    "\n",
    "# Test with slightly different keys\n",
    "print(\"\\nWith SIMILAR keys (sim ≈ 0.9):\")\n",
    "keys = []\n",
    "for i in range(n_writes):\n",
    "    noise = 0.3 * F.normalize(torch.randn(K), dim=0)\n",
    "    keys.append(F.normalize(base_key + noise, dim=0))\n",
    "keys = torch.stack(keys)\n",
    "\n",
    "for n_iters in [0, 1, 5, 10, 20]:\n",
    "    if n_iters == 0:\n",
    "        _, errors = simulate_delta_rule(keys, values, keys)\n",
    "    else:\n",
    "        _, errors = simulate_delta_with_correction(keys, values, keys, n_iters=n_iters)\n",
    "    mean_err = sum(errors) / len(errors)\n",
    "    status = \"✓\" if mean_err < 0.1 else \"✗\"\n",
    "    print(f\"  n_iters={n_iters:2d}: mean_err={mean_err:.4f} {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY: What Works?\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "A) Positional Encoding:\n",
    "   - Works IF pos_scale is high enough to make keys distinguishable\n",
    "   - Problem: In NN, model produces same key for same token regardless of position\n",
    "   - Fix: Add positional embedding BEFORE key projection, or position-dependent key proj\n",
    "   \n",
    "B) Orthogonal Keys:\n",
    "   - PERFECT retrieval for n_writes ≤ K (64 writes with K=64)\n",
    "   - Fundamental capacity limit: can't store more than K orthogonal vectors\n",
    "   - For multi-needle (2-5 needles), this is MORE than enough!\n",
    "   \n",
    "C) Error-Correcting Writes:\n",
    "   - Does NOT help with identical keys (convergence impossible)\n",
    "   - Helps slightly with similar keys\n",
    "   - Not practical: requires multiple passes, not causal\n",
    "\n",
    "CONCLUSION: \n",
    "→ Solution B (orthogonal keys) proves the math CAN work for multi-needle\n",
    "→ The issue is making the NN learn to produce distinct keys for MARKER tokens\n",
    "→ Since MARKER tokens are identical, we need position-aware key generation\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a53875b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST: Retrieval Error vs Key Similarity\n",
      "======================================================================\n",
      "\n",
      "We'll create keys with controlled similarity and measure error.\n",
      "\n",
      " Avg Similarity |   Mean Error |    Max Error | Status\n",
      "------------------------------------------------------------\n",
      "          1.000 |       1.1556 |       1.5268 | ✗ FAIL\n",
      "          0.988 |       1.1403 |       1.5093 | ✗ FAIL\n",
      "          0.941 |       1.0838 |       1.4435 | ✗ FAIL\n",
      "          0.856 |       0.9758 |       1.2836 | ✗ FAIL\n",
      "          0.687 |       0.7786 |       1.0430 | ✗ FAIL\n",
      "          0.492 |       0.5626 |       0.8038 | ✗ FAIL\n",
      "          0.326 |       0.3664 |       0.5359 | ✗ FAIL\n",
      "          0.150 |       0.1790 |       0.2960 | ⚠ WARN\n",
      "          0.024 |       0.0449 |       0.1058 | ✓ OK\n",
      "          0.010 |       0.0171 |       0.0334 | ✓ OK\n",
      "         -0.000 |       0.0000 |       0.0000 | ✓ OK\n",
      "\n",
      "======================================================================\n",
      "TEST: How many random keys can we store?\n",
      "======================================================================\n",
      "\n",
      "With random (non-orthogonal) keys, capacity is limited.\n",
      "\n",
      "  n=  2: mean_err=0.0513, avg|sim|=0.1030 ✓\n",
      "  n=  5: mean_err=0.1655, avg|sim|=0.1203 ⚠\n",
      "  n= 10: mean_err=0.2554, avg|sim|=0.1003 ⚠\n",
      "  n= 20: mean_err=0.3726, avg|sim|=0.0979 ✗\n",
      "  n= 30: mean_err=0.4588, avg|sim|=0.1042 ✗\n",
      "  n= 40: mean_err=0.5533, avg|sim|=0.1002 ✗\n",
      "  n= 50: mean_err=0.6393, avg|sim|=0.1017 ✗\n",
      "  n= 60: mean_err=0.7113, avg|sim|=0.0980 ✗\n",
      "  n= 64: mean_err=0.7331, avg|sim|=0.1048 ✗\n",
      "  n= 70: mean_err=0.7608, avg|sim|=0.0993 ✗\n",
      "  n= 80: mean_err=0.8151, avg|sim|=0.0956 ✗\n",
      "  n=100: mean_err=0.8812, avg|sim|=0.0984 ✗\n",
      "\n",
      "======================================================================\n",
      "CONCLUSION\n",
      "======================================================================\n",
      "\n",
      "1. Key similarity must be < 0.3 for reasonable accuracy\n",
      "2. Random K=64 keys have avg|sim| ≈ 0.1, which is borderline OK\n",
      "3. With random keys, ~20-30 writes is the practical limit for K=64\n",
      "4. For multi-needle (2-5 needles), random keys SHOULD work\n",
      "   → The problem is that MARKER tokens produce IDENTICAL keys (sim=1.0)\n",
      "\n",
      "WHAT WE NEED:\n",
      "- NOT just \"different\" keys, but near-ORTHOGONAL keys for each MARKER\n",
      "- Options:\n",
      "  a) Random key per position (hash position → key)\n",
      "  b) Learned orthogonal key bank\n",
      "  c) Force key orthogonality via loss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================\n",
    "CRITICAL QUESTION: What key similarity is tolerable?\n",
    "============================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST: Retrieval Error vs Key Similarity\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nWe'll create keys with controlled similarity and measure error.\\n\")\n",
    "\n",
    "n_writes = 5\n",
    "values = F.normalize(torch.randn(n_writes, V), dim=1)\n",
    "\n",
    "print(f\"{'Avg Similarity':>15} | {'Mean Error':>12} | {'Max Error':>12} | Status\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for target_sim in [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]:\n",
    "    # Create keys with controlled similarity\n",
    "    # Start from orthogonal, interpolate toward identical\n",
    "    random_matrix = torch.randn(n_writes, K)\n",
    "    Q, R = torch.linalg.qr(random_matrix.T)\n",
    "    ortho_keys = Q[:, :n_writes].T  # Orthonormal keys\n",
    "    \n",
    "    base_key = F.normalize(torch.randn(K), dim=0)\n",
    "    identical_keys = base_key.unsqueeze(0).repeat(n_writes, 1)\n",
    "    \n",
    "    # Interpolate: sim=0 → orthogonal, sim=1 → identical\n",
    "    keys = (1 - target_sim) * ortho_keys + target_sim * identical_keys\n",
    "    keys = F.normalize(keys, dim=1)\n",
    "    \n",
    "    # Measure actual similarity\n",
    "    actual_sims = []\n",
    "    for i in range(n_writes):\n",
    "        for j in range(i+1, n_writes):\n",
    "            actual_sims.append(torch.dot(keys[i], keys[j]).item())\n",
    "    avg_sim = sum(actual_sims) / len(actual_sims)\n",
    "    \n",
    "    # Test retrieval\n",
    "    _, errors = simulate_delta_rule(keys, values, keys)\n",
    "    mean_err = sum(errors) / len(errors)\n",
    "    max_err = max(errors)\n",
    "    \n",
    "    status = \"✓ OK\" if mean_err < 0.1 else (\"⚠ WARN\" if mean_err < 0.3 else \"✗ FAIL\")\n",
    "    print(f\"{avg_sim:>15.3f} | {mean_err:>12.4f} | {max_err:>12.4f} | {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST: How many random keys can we store?\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nWith random (non-orthogonal) keys, capacity is limited.\\n\")\n",
    "\n",
    "for n_writes in [2, 5, 10, 20, 30, 40, 50, 60, 64, 70, 80, 100]:\n",
    "    # Random normalized keys (NOT orthogonalized)\n",
    "    keys = F.normalize(torch.randn(n_writes, K), dim=1)\n",
    "    values = F.normalize(torch.randn(n_writes, V), dim=1)\n",
    "    \n",
    "    _, errors = simulate_delta_rule(keys, values, keys)\n",
    "    mean_err = sum(errors) / len(errors)\n",
    "    \n",
    "    # Measure key similarity\n",
    "    sims = []\n",
    "    for i in range(min(20, n_writes)):  # Sample\n",
    "        for j in range(i+1, min(20, n_writes)):\n",
    "            sims.append(abs(torch.dot(keys[i], keys[j]).item()))\n",
    "    avg_sim = sum(sims) / len(sims) if sims else 0\n",
    "    \n",
    "    status = \"✓\" if mean_err < 0.1 else (\"⚠\" if mean_err < 0.3 else \"✗\")\n",
    "    print(f\"  n={n_writes:3d}: mean_err={mean_err:.4f}, avg|sim|={avg_sim:.4f} {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONCLUSION\")  \n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. Key similarity must be < 0.3 for reasonable accuracy\n",
    "2. Random K=64 keys have avg|sim| ≈ 0.1, which is borderline OK\n",
    "3. With random keys, ~20-30 writes is the practical limit for K=64\n",
    "4. For multi-needle (2-5 needles), random keys SHOULD work\n",
    "   → The problem is that MARKER tokens produce IDENTICAL keys (sim=1.0)\n",
    "   \n",
    "WHAT WE NEED:\n",
    "- NOT just \"different\" keys, but near-ORTHOGONAL keys for each MARKER\n",
    "- Options:\n",
    "  a) Random key per position (hash position → key)\n",
    "  b) Learned orthogonal key bank\n",
    "  c) Force key orthogonality via loss\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6e02cae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST: Position-Hashed Keys for Multi-Needle\n",
      "======================================================================\n",
      "  n_needles= 2: err=0.0530, avg|sim|=0.107, max|sim|=0.107 ✓\n",
      "  n_needles= 3: err=0.1027, avg|sim|=0.118, max|sim|=0.189 ✗\n",
      "  n_needles= 5: err=0.1122, avg|sim|=0.085, max|sim|=0.169 ✗\n",
      "  n_needles=10: err=0.2251, avg|sim|=0.092, max|sim|=0.397 ✗\n",
      "  n_needles=20: err=0.4195, avg|sim|=0.106, max|sim|=0.367 ✗\n",
      "\n",
      "======================================================================\n",
      "THE CATCH: How to retrieve with position-hashed keys?\n",
      "======================================================================\n",
      "\n",
      "Problem: At write time we use hash(position) as key.\n",
      "         At retrieval, we query with hash(CUE_position).\n",
      "         But CUE is at a DIFFERENT position than MARKER!\n",
      "\n",
      "Options:\n",
      "1. Learn a mapping: CUE token → MARKER key\n",
      "   - Train model to output hash(marker_pos) when it sees CUE\n",
      "   - This is what single-needle already does implicitly!\n",
      "\n",
      "2. Content-addressable with position hash:\n",
      "   - Key = hash(token, position) for MARKER\n",
      "   - Query = learned from (CUE, context)\n",
      "   - Model learns to produce the right query for each CUE\n",
      "\n",
      "3. Hybrid: token-based key + position offset\n",
      "   - Key = embed(MARKER) + position_offset\n",
      "   - Query = embed(CUE) + learned_offset\n",
      "\n",
      "Let's verify the math: if model learns to output correct query,\n",
      "retrieval should work.\n",
      "\n",
      "Verification: Query = Key (perfect match)\n",
      "  n= 2: mean_err=0.014121\n",
      "  n= 5: mean_err=0.155685\n",
      "  n=10: mean_err=0.233315\n",
      "  n=20: mean_err=0.384028\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "KEY INSIGHT:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "The ONLY issue is that our model produces IDENTICAL keys for MARKER tokens.\n",
      "Single-needle works because the model learns:\n",
      "   CUE token → query that matches THE marker key\n",
      "\n",
      "Multi-needle fails because:\n",
      "   ALL markers → SAME key → overwrite\n",
      "   CUE → query that matches that one key → retrieves last write only\n",
      "\n",
      "Fix: Make markers produce DIFFERENT keys, then train model to:\n",
      "   CUE_1 → query for MARKER_1's key\n",
      "   CUE_2 → query for MARKER_2's key\n",
      "\n",
      "How to make markers produce different keys?\n",
      "   a) Add position encoding to embedding BEFORE key projection\n",
      "   b) Learn position-dependent key projection\n",
      "   c) Use random key bank indexed by something (head? layer?)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================\n",
    "SOLUTION: Position-Hashed Keys\n",
    "============================================================\n",
    "Instead of learning keys from token embeddings,\n",
    "use deterministic hash: position → random key\n",
    "\n",
    "This guarantees MARKER at different positions get different keys.\n",
    "\"\"\"\n",
    "\n",
    "def position_hash_key(position, seed=42, K=64):\n",
    "    \"\"\"Generate a deterministic random key from position.\"\"\"\n",
    "    gen = torch.Generator().manual_seed(seed + position)\n",
    "    key = torch.randn(K, generator=gen)\n",
    "    return F.normalize(key, dim=0)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST: Position-Hashed Keys for Multi-Needle\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate multi-needle scenario with position-hashed keys\n",
    "for n_needles in [2, 3, 5, 10, 20]:\n",
    "    # Random positions for needles\n",
    "    positions = sorted(torch.randperm(500)[:n_needles].tolist())\n",
    "    \n",
    "    # Position-hashed keys\n",
    "    keys = torch.stack([position_hash_key(pos) for pos in positions])\n",
    "    values = F.normalize(torch.randn(n_needles, V), dim=1)\n",
    "    \n",
    "    # Check key similarity\n",
    "    sims = []\n",
    "    for i in range(n_needles):\n",
    "        for j in range(i+1, n_needles):\n",
    "            sims.append(abs(torch.dot(keys[i], keys[j]).item()))\n",
    "    avg_sim = sum(sims) / len(sims) if sims else 0\n",
    "    max_sim = max(sims) if sims else 0\n",
    "    \n",
    "    # Test retrieval\n",
    "    _, errors = simulate_delta_rule(keys, values, keys)\n",
    "    mean_err = sum(errors) / len(errors)\n",
    "    \n",
    "    status = \"✓\" if mean_err < 0.1 else \"✗\"\n",
    "    print(f\"  n_needles={n_needles:2d}: err={mean_err:.4f}, avg|sim|={avg_sim:.3f}, max|sim|={max_sim:.3f} {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"THE CATCH: How to retrieve with position-hashed keys?\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Problem: At write time we use hash(position) as key.\n",
    "         At retrieval, we query with hash(CUE_position).\n",
    "         But CUE is at a DIFFERENT position than MARKER!\n",
    "\n",
    "Options:\n",
    "1. Learn a mapping: CUE token → MARKER key\n",
    "   - Train model to output hash(marker_pos) when it sees CUE\n",
    "   - This is what single-needle already does implicitly!\n",
    "   \n",
    "2. Content-addressable with position hash:\n",
    "   - Key = hash(token, position) for MARKER\n",
    "   - Query = learned from (CUE, context)\n",
    "   - Model learns to produce the right query for each CUE\n",
    "   \n",
    "3. Hybrid: token-based key + position offset\n",
    "   - Key = embed(MARKER) + position_offset\n",
    "   - Query = embed(CUE) + learned_offset\n",
    "   \n",
    "Let's verify the math: if model learns to output correct query,\n",
    "retrieval should work.\n",
    "\"\"\")\n",
    "\n",
    "# Verify: if queries exactly match keys, retrieval is perfect\n",
    "print(\"Verification: Query = Key (perfect match)\")\n",
    "for n_needles in [2, 5, 10, 20]:\n",
    "    positions = sorted(torch.randperm(500)[:n_needles].tolist())\n",
    "    keys = torch.stack([position_hash_key(pos) for pos in positions])\n",
    "    values = F.normalize(torch.randn(n_needles, V), dim=1)\n",
    "    \n",
    "    # Perfect queries = keys\n",
    "    _, errors = simulate_delta_rule(keys, values, keys)\n",
    "    mean_err = sum(errors) / len(errors)\n",
    "    print(f\"  n={n_needles:2d}: mean_err={mean_err:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\"\"\n",
    "The ONLY issue is that our model produces IDENTICAL keys for MARKER tokens.\n",
    "Single-needle works because the model learns:\n",
    "   CUE token → query that matches THE marker key\n",
    "   \n",
    "Multi-needle fails because:\n",
    "   ALL markers → SAME key → overwrite\n",
    "   CUE → query that matches that one key → retrieves last write only\n",
    "\n",
    "Fix: Make markers produce DIFFERENT keys, then train model to:\n",
    "   CUE_1 → query for MARKER_1's key\n",
    "   CUE_2 → query for MARKER_2's key\n",
    "\n",
    "How to make markers produce different keys?\n",
    "   a) Add position encoding to embedding BEFORE key projection\n",
    "   b) Learn position-dependent key projection\n",
    "   c) Use random key bank indexed by something (head? layer?)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea5ac171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST: Orthogonal Key Bank\n",
      "======================================================================\n",
      "Key bank orthonormality check: off-diag max = 0.000001\n",
      "  n_needles= 2: mean_err=0.000000, max_err=0.000000 ✓\n",
      "  n_needles= 3: mean_err=0.000000, max_err=0.000000 ✓\n",
      "  n_needles= 5: mean_err=0.000000, max_err=0.000000 ✓\n",
      "  n_needles=10: mean_err=0.000000, max_err=0.000000 ✓\n",
      "  n_needles=20: mean_err=0.000000, max_err=0.000000 ✓\n",
      "  n_needles=30: mean_err=0.000000, max_err=0.000000 ✓\n",
      "  n_needles=50: mean_err=0.000000, max_err=0.000001 ✓\n",
      "  n_needles=64: mean_err=0.000000, max_err=0.000001 ✓\n",
      "\n",
      "======================================================================\n",
      "IMPLEMENTATION PLAN\n",
      "======================================================================\n",
      "\n",
      "To implement orthogonal key bank in the model:\n",
      "\n",
      "1. In GDN layer, create:\n",
      "   self.key_bank = nn.Parameter(torch.zeros(n_heads, K, K))\n",
      "   # Initialize with orthogonal matrices per head\n",
      "\n",
      "2. Track write counter per head:\n",
      "   self.register_buffer('write_counter', torch.zeros(n_heads, dtype=torch.long))\n",
      "\n",
      "3. At MARKER position:\n",
      "   key = self.key_bank[head, write_counter[head] % K]\n",
      "   write_counter[head] += 1\n",
      "\n",
      "4. At CUE position:\n",
      "   # Model must learn to output query matching the stored key\n",
      "   # Need to pass slot index somehow (CUE_1, CUE_2, etc.)\n",
      "\n",
      "Problem: How does CUE know WHICH slot to query?\n",
      "   - Single-needle: trivial (only one slot used)\n",
      "   - Multi-needle: CUE must know its \"slot index\"\n",
      "\n",
      "Options:\n",
      "   a) CUE tokens are position-aware: CUE_1, CUE_2, ... (we tried, failed)\n",
      "   b) CUE learns from context which MARKER it matches\n",
      "   c) Query all slots, blend results (attention-like)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TEST: Query All Slots with Softmax Blending\n",
      "======================================================================\n",
      "\n",
      "With 5 needles stored:\n",
      "  Target slot 0: err=0.6932, weights=[0.40, 0.15, 0.15, 0.15, 0.15], correct=✓\n",
      "  Target slot 1: err=0.6363, weights=[0.15, 0.40, 0.15, 0.15, 0.15], correct=✓\n",
      "  Target slot 2: err=0.6304, weights=[0.15, 0.15, 0.40, 0.15, 0.15], correct=✓\n",
      "  Target slot 3: err=0.6769, weights=[0.15, 0.15, 0.15, 0.40, 0.15], correct=✓\n",
      "  Target slot 4: err=0.6569, weights=[0.15, 0.15, 0.15, 0.15, 0.40], correct=✓\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ANALYSIS:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "With orthogonal keys:\n",
      "- Direct query with exact key → perfect retrieval\n",
      "- Softmax blending → correct slot has highest weight\n",
      "\n",
      "This proves the MATH works. Now we need the model to learn:\n",
      "1. MARKER → use next key from orthogonal bank\n",
      "2. CUE → output query vector that matches target key\n",
      "\n",
      "For multi-needle, the challenge is teaching CUE to distinguish targets.\n",
      "This requires CUE to be slot-aware (CUE_0, CUE_1, etc.) OR \n",
      "context-aware (learn from preceding tokens which MARKER to retrieve).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================\n",
    "PRACTICAL SOLUTION: Learned Orthogonal Key Bank\n",
    "============================================================\n",
    "Pre-allocate K orthogonal keys. At each MARKER position, \n",
    "use the next unused key from the bank.\n",
    "\n",
    "This GUARANTEES zero interference up to K markers.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST: Orthogonal Key Bank\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create orthogonal key bank\n",
    "KEY_BANK_SIZE = K  # Can store up to K=64 distinct keys\n",
    "random_matrix = torch.randn(KEY_BANK_SIZE, K)\n",
    "Q, R = torch.linalg.qr(random_matrix.T)\n",
    "KEY_BANK = Q.T  # [64, 64] - orthonormal keys\n",
    "\n",
    "# Verify orthonormality\n",
    "ortho_check = KEY_BANK @ KEY_BANK.T\n",
    "print(f\"Key bank orthonormality check: off-diag max = {(ortho_check - torch.eye(K)).abs().max():.6f}\")\n",
    "\n",
    "for n_needles in [2, 3, 5, 10, 20, 30, 50, 64]:\n",
    "    if n_needles > KEY_BANK_SIZE:\n",
    "        print(f\"  n_needles={n_needles:2d}: EXCEEDS BANK SIZE\")\n",
    "        continue\n",
    "        \n",
    "    # Use first n_needles keys from bank\n",
    "    keys = KEY_BANK[:n_needles]\n",
    "    values = F.normalize(torch.randn(n_needles, V), dim=1)\n",
    "    \n",
    "    _, errors = simulate_delta_rule(keys, values, keys)\n",
    "    mean_err = sum(errors) / len(errors)\n",
    "    max_err = max(errors)\n",
    "    \n",
    "    status = \"✓\" if mean_err < 0.01 else \"✗\"\n",
    "    print(f\"  n_needles={n_needles:2d}: mean_err={mean_err:.6f}, max_err={max_err:.6f} {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"IMPLEMENTATION PLAN\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "To implement orthogonal key bank in the model:\n",
    "\n",
    "1. In GDN layer, create:\n",
    "   self.key_bank = nn.Parameter(torch.zeros(n_heads, K, K))\n",
    "   # Initialize with orthogonal matrices per head\n",
    "   \n",
    "2. Track write counter per head:\n",
    "   self.register_buffer('write_counter', torch.zeros(n_heads, dtype=torch.long))\n",
    "   \n",
    "3. At MARKER position:\n",
    "   key = self.key_bank[head, write_counter[head] % K]\n",
    "   write_counter[head] += 1\n",
    "   \n",
    "4. At CUE position:\n",
    "   # Model must learn to output query matching the stored key\n",
    "   # Need to pass slot index somehow (CUE_1, CUE_2, etc.)\n",
    "   \n",
    "Problem: How does CUE know WHICH slot to query?\n",
    "   - Single-needle: trivial (only one slot used)\n",
    "   - Multi-needle: CUE must know its \"slot index\"\n",
    "   \n",
    "Options:\n",
    "   a) CUE tokens are position-aware: CUE_1, CUE_2, ... (we tried, failed)\n",
    "   b) CUE learns from context which MARKER it matches\n",
    "   c) Query all slots, blend results (attention-like)\n",
    "\"\"\")\n",
    "\n",
    "# Test option (c): Query multiple slots\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST: Query All Slots with Softmax Blending\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def retrieve_multi_slot(S, query, key_bank, n_used, temp=1.0):\n",
    "    \"\"\"\n",
    "    Soft attention over stored slots.\n",
    "    S: [K, V] state matrix\n",
    "    query: [K] query vector\n",
    "    key_bank: [K, K] orthogonal keys\n",
    "    n_used: number of slots actually used\n",
    "    \"\"\"\n",
    "    # Compute attention over used keys\n",
    "    keys = key_bank[:n_used]  # [n_used, K]\n",
    "    \n",
    "    # Attention scores\n",
    "    scores = keys @ query  # [n_used]\n",
    "    weights = F.softmax(scores / temp, dim=0)  # [n_used]\n",
    "    \n",
    "    # Retrieve from each slot\n",
    "    retrieved_per_slot = keys @ S  # [n_used, V]\n",
    "    \n",
    "    # Weighted blend\n",
    "    result = (weights.unsqueeze(1) * retrieved_per_slot).sum(dim=0)  # [V]\n",
    "    \n",
    "    return result, weights\n",
    "\n",
    "# Test: Can softmax attention select the right slot?\n",
    "n_needles = 5\n",
    "keys = KEY_BANK[:n_needles]\n",
    "values = F.normalize(torch.randn(n_needles, V), dim=1)\n",
    "\n",
    "# Build state\n",
    "S = torch.zeros(K, V)\n",
    "for i in range(n_needles):\n",
    "    k, v = keys[i], values[i]\n",
    "    current = k @ S\n",
    "    S = S + torch.outer(k, v - current)\n",
    "\n",
    "print(f\"\\nWith {n_needles} needles stored:\")\n",
    "for target_idx in range(n_needles):\n",
    "    # Use the exact key as query (perfect match)\n",
    "    query = keys[target_idx]\n",
    "    result, weights = retrieve_multi_slot(S, query, KEY_BANK, n_needles)\n",
    "    \n",
    "    # Check if correct value retrieved\n",
    "    err = torch.norm(result - values[target_idx]) / torch.norm(values[target_idx])\n",
    "    \n",
    "    # Check attention weight distribution\n",
    "    max_weight_idx = weights.argmax().item()\n",
    "    correct = max_weight_idx == target_idx\n",
    "    \n",
    "    print(f\"  Target slot {target_idx}: err={err:.4f}, \"\n",
    "          f\"weights=[{', '.join([f'{w:.2f}' for w in weights])}], \"\n",
    "          f\"correct={'✓' if correct else '✗'}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ANALYSIS:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\"\"\n",
    "With orthogonal keys:\n",
    "- Direct query with exact key → perfect retrieval\n",
    "- Softmax blending → correct slot has highest weight\n",
    "\n",
    "This proves the MATH works. Now we need the model to learn:\n",
    "1. MARKER → use next key from orthogonal bank\n",
    "2. CUE → output query vector that matches target key\n",
    "\n",
    "For multi-needle, the challenge is teaching CUE to distinguish targets.\n",
    "This requires CUE to be slot-aware (CUE_0, CUE_1, etc.) OR \n",
    "context-aware (learn from preceding tokens which MARKER to retrieve).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcebc1b9",
   "metadata": {},
   "source": [
    "# Summary: Pure Math Analysis\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "| Test | Result |\n",
    "|------|--------|\n",
    "| Positional encoding (sin/cos) | FAILS - sim=0.9 still too high |\n",
    "| Random keys | MARGINAL - only 2-5 writes before degradation |\n",
    "| Error-correcting writes | FAILS - impossible with identical keys |\n",
    "| **Orthogonal key bank** | **PERFECT** - 0% error up to K=64 needles |\n",
    "\n",
    "## Key Threshold\n",
    "- Need key similarity **< 0.02** for reliable 5-needle retrieval\n",
    "- Random keys at K=64 give sim ≈ 0.1 (marginal)\n",
    "- MARKERs currently give sim = 1.0 (IDENTICAL - fatal)\n",
    "\n",
    "## The Solution\n",
    "**Orthogonal Key Bank**:\n",
    "1. Pre-allocate K orthogonal keys (via QR decomposition)\n",
    "2. At each MARKER, use next key from bank (round-robin or counter)\n",
    "3. At CUE, model outputs query that matches target key\n",
    "\n",
    "## The Remaining Challenge\n",
    "How does CUE know **which** key to query?\n",
    "- Option A: Slot-indexed CUEs (CUE_0, CUE_1...) - tried, failed\n",
    "- Option B: Context-dependent query (learn from preceding tokens)\n",
    "- Option C: Attention over stored slots (soft) - FAILS (blending dilutes)\n",
    "\n",
    "## Next Step\n",
    "Implement orthogonal key bank in GDN and test with slot-indexed CUEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98d4041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: Test Context-Based Key Generation (Pure Simulation)\n",
      "======================================================================\n",
      "\n",
      "Context-based keys for 5 markers:\n",
      "  key[0] · key[1] = -0.1192\n",
      "  key[0] · key[2] = 0.0640\n",
      "  key[0] · key[3] = -0.1956\n",
      "  key[0] · key[4] = -0.1688\n",
      "  key[1] · key[2] = -0.0338\n",
      "  key[1] · key[3] = 0.0528\n",
      "  key[1] · key[4] = 0.0700\n",
      "  key[2] · key[3] = 0.1914\n",
      "  key[2] · key[4] = 0.0086\n",
      "  key[3] · key[4] = -0.1070\n",
      "\n",
      "Avg |similarity|: 0.1011, Max |similarity|: 0.1956\n",
      "Required for multi-needle: avg_sim < 0.1, max_sim < 0.3\n",
      "Status: ✓ GOOD\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Simulate Write/Read with Context Keys\n",
      "======================================================================\n",
      "\n",
      "Retrieval with EXACT same context (ideal case):\n",
      "  Marker 0: error = 0.3419 ✗\n",
      "  Marker 1: error = 0.1143 ✗\n",
      "  Marker 2: error = 0.2008 ✗\n",
      "  Marker 3: error = 0.1108 ✗\n",
      "  Marker 4: error = 0.0000 ✓\n",
      "\n",
      "Mean retrieval error: 0.1536\n",
      "\n",
      "======================================================================\n",
      "STEP 3: What if CUE context is SIMILAR but not IDENTICAL?\n",
      "======================================================================\n",
      "  noise=0.0: mean_err=0.1536, max_err=0.3419 ✓\n",
      "  noise=0.1: mean_err=0.1450, max_err=0.3098 ✓\n",
      "  noise=0.2: mean_err=0.1753, max_err=0.3663 ✓\n",
      "  noise=0.3: mean_err=0.1786, max_err=0.3483 ✓\n",
      "  noise=0.5: mean_err=0.2274, max_err=0.3764 ⚠\n",
      "  noise=1.0: mean_err=0.3674, max_err=0.4387 ⚠\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "INSIGHT:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Context-based keys work IF:\n",
      "1. Different MARKERs have different context → different keys ✓\n",
      "2. CUE can reconstruct the context that preceded target MARKER\n",
      "\n",
      "The challenge: How does CUE reconstruct MARKER's context?\n",
      "  - In NIAH: haystack before MARKER = haystack before CUE (same sequence!)\n",
      "  - CUE can use SWA to look back at local context\n",
      "  - If same tokens appear before CUE as before MARKER → same key → retrieval!\n",
      "\n",
      "This is EXACTLY how Based/CAT works!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================\n",
    "HETEROASSOCIATIVE CONTENT-ADDRESSED MEMORY\n",
    "============================================================\n",
    "Key insight: Context before each MARKER is different!\n",
    "  MARKER_1 appears after [haystack tokens A, B, C]\n",
    "  MARKER_2 appears after [haystack tokens X, Y, Z]\n",
    "  \n",
    "If key = f(context_window), then MARKER_1 and MARKER_2 get different keys.\n",
    "At CUE time, model reconstructs the context → same key → retrieval works.\n",
    "\n",
    "Implementation: short_conv(context_window) → key\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: Test Context-Based Key Generation (Pure Simulation)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate: different random context windows → different keys via conv\n",
    "d_model = 512\n",
    "K = 64\n",
    "conv_width = 4\n",
    "\n",
    "# Short convolution for key generation\n",
    "key_conv = nn.Conv1d(d_model, K, kernel_size=conv_width, padding=0, bias=False)\n",
    "nn.init.xavier_normal_(key_conv.weight)\n",
    "\n",
    "def context_to_key(context_window):\n",
    "    \"\"\"\n",
    "    context_window: [conv_width, d_model]\n",
    "    Returns: [K] normalized key\n",
    "    \"\"\"\n",
    "    # Reshape for conv1d: [1, d_model, conv_width]\n",
    "    x = context_window.T.unsqueeze(0)  \n",
    "    # Conv output: [1, K, 1]\n",
    "    out = key_conv(x)\n",
    "    key = out.squeeze()  # [K]\n",
    "    return F.normalize(key, dim=0)\n",
    "\n",
    "# Generate random context windows (simulating haystack before each MARKER)\n",
    "n_markers = 5\n",
    "context_windows = []\n",
    "for i in range(n_markers):\n",
    "    # Random context window (different tokens before each MARKER)\n",
    "    ctx = torch.randn(conv_width, d_model)\n",
    "    context_windows.append(ctx)\n",
    "\n",
    "# Generate keys from context\n",
    "keys = torch.stack([context_to_key(ctx) for ctx in context_windows])\n",
    "\n",
    "# Check key similarity\n",
    "print(f\"\\nContext-based keys for {n_markers} markers:\")\n",
    "sims = []\n",
    "for i in range(n_markers):\n",
    "    for j in range(i+1, n_markers):\n",
    "        sim = torch.dot(keys[i], keys[j]).item()\n",
    "        sims.append(abs(sim))\n",
    "        print(f\"  key[{i}] · key[{j}] = {sim:.4f}\")\n",
    "\n",
    "avg_sim = sum(sims) / len(sims)\n",
    "max_sim = max(sims)\n",
    "print(f\"\\nAvg |similarity|: {avg_sim:.4f}, Max |similarity|: {max_sim:.4f}\")\n",
    "print(f\"Required for multi-needle: avg_sim < 0.1, max_sim < 0.3\")\n",
    "\n",
    "status = \"✓ GOOD\" if avg_sim < 0.15 and max_sim < 0.4 else \"✗ TOO SIMILAR\"\n",
    "print(f\"Status: {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: Simulate Write/Read with Context Keys\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate values (what we want to store)\n",
    "values = F.normalize(torch.randn(n_markers, V), dim=1)\n",
    "\n",
    "# Write to state matrix\n",
    "S = torch.zeros(K, V)\n",
    "for i in range(n_markers):\n",
    "    k = keys[i]\n",
    "    v = values[i]\n",
    "    current = k @ S\n",
    "    delta = v - current\n",
    "    S = S + torch.outer(k, delta)\n",
    "\n",
    "# Read with same keys (assuming CUE reconstructs correct context)\n",
    "print(\"\\nRetrieval with EXACT same context (ideal case):\")\n",
    "errors = []\n",
    "for i in range(n_markers):\n",
    "    query = keys[i]  # Same key as write\n",
    "    retrieved = query @ S\n",
    "    err = torch.norm(retrieved - values[i]) / torch.norm(values[i])\n",
    "    errors.append(err.item())\n",
    "    status = \"✓\" if err < 0.1 else \"✗\"\n",
    "    print(f\"  Marker {i}: error = {err:.4f} {status}\")\n",
    "\n",
    "mean_err = sum(errors) / len(errors)\n",
    "print(f\"\\nMean retrieval error: {mean_err:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: What if CUE context is SIMILAR but not IDENTICAL?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# In practice, CUE context won't be identical to MARKER context\n",
    "# Test robustness to context noise\n",
    "\n",
    "for noise_level in [0.0, 0.1, 0.2, 0.3, 0.5, 1.0]:\n",
    "    errors = []\n",
    "    for i in range(n_markers):\n",
    "        # Add noise to context\n",
    "        noisy_ctx = context_windows[i] + noise_level * torch.randn_like(context_windows[i])\n",
    "        query = context_to_key(noisy_ctx)\n",
    "        \n",
    "        retrieved = query @ S\n",
    "        err = torch.norm(retrieved - values[i]) / torch.norm(values[i])\n",
    "        errors.append(err.item())\n",
    "    \n",
    "    mean_err = sum(errors) / len(errors)\n",
    "    max_err = max(errors)\n",
    "    status = \"✓\" if mean_err < 0.2 else \"⚠\" if mean_err < 0.5 else \"✗\"\n",
    "    print(f\"  noise={noise_level:.1f}: mean_err={mean_err:.4f}, max_err={max_err:.4f} {status}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"INSIGHT:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\"\"\n",
    "Context-based keys work IF:\n",
    "1. Different MARKERs have different context → different keys ✓\n",
    "2. CUE can reconstruct the context that preceded target MARKER\n",
    "\n",
    "The challenge: How does CUE reconstruct MARKER's context?\n",
    "  - In NIAH: haystack before MARKER = haystack before CUE (same sequence!)\n",
    "  - CUE can use SWA to look back at local context\n",
    "  - If same tokens appear before CUE as before MARKER → same key → retrieval!\n",
    "\n",
    "This is EXACTLY how Based/CAT works!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4b302e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Testing GDNContextKeys on Multi-Needle Sequence\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================\n",
    "GDN WITH CONTEXT-BASED KEYS (Based/CAT Style)\n",
    "============================================================\n",
    "\"\"\"\n",
    "\n",
    "class GDNContextKeys(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Delta Net with context-based key generation.\n",
    "    \n",
    "    Key difference from standard GDN:\n",
    "    - Keys are generated from short_conv(context_window), not just current token\n",
    "    - This makes keys position/context-dependent, enabling multi-needle\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, head_dim, value_dim, \n",
    "                 conv_width=4, beta_bias=-4.0, g_bias=4.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.conv_width = conv_width\n",
    "        \n",
    "        # Key projection with causal convolution\n",
    "        # Input: [B, T, d_model], Output: [B, T, n_heads * head_dim]\n",
    "        self.key_conv = nn.Conv1d(\n",
    "            d_model, n_heads * head_dim, \n",
    "            kernel_size=conv_width, \n",
    "            padding=conv_width - 1,  # Causal padding\n",
    "            groups=1\n",
    "        )\n",
    "        \n",
    "        # Query uses same convolution (for matching at retrieval)\n",
    "        self.query_conv = nn.Conv1d(\n",
    "            d_model, n_heads * head_dim,\n",
    "            kernel_size=conv_width,\n",
    "            padding=conv_width - 1,\n",
    "            groups=1\n",
    "        )\n",
    "        \n",
    "        # Value projection (standard)\n",
    "        self.W_v = nn.Linear(d_model, n_heads * value_dim)\n",
    "        \n",
    "        # Gates\n",
    "        self.W_beta = nn.Linear(d_model, n_heads)\n",
    "        self.W_g = nn.Linear(d_model, n_heads)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(n_heads * value_dim, d_model)\n",
    "        \n",
    "        # Bias initialization for sparse gating\n",
    "        self.W_beta.bias.data.fill_(beta_bias)\n",
    "        self.W_g.bias.data.fill_(g_bias)\n",
    "        \n",
    "    def forward(self, x, state=None):\n",
    "        \"\"\"\n",
    "        x: [B, T, d_model]\n",
    "        state: [B, n_heads, head_dim, value_dim] or None\n",
    "        Returns: output [B, T, d_model], new_state, diagnostics\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "        H, K, V = self.n_heads, self.head_dim, self.value_dim\n",
    "        \n",
    "        if state is None:\n",
    "            state = torch.zeros(B, H, K, V, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        # Context-based keys via causal convolution\n",
    "        # [B, T, d] -> [B, d, T] -> conv -> [B, H*K, T] -> [B, T, H*K]\n",
    "        x_t = x.transpose(1, 2)\n",
    "        keys_raw = self.key_conv(x_t)[:, :, :T].transpose(1, 2)  # Causal: trim to T\n",
    "        queries_raw = self.query_conv(x_t)[:, :, :T].transpose(1, 2)\n",
    "        \n",
    "        # Reshape to heads\n",
    "        keys = keys_raw.view(B, T, H, K)\n",
    "        queries = queries_raw.view(B, T, H, K)\n",
    "        \n",
    "        # Normalize keys and queries\n",
    "        keys = F.normalize(keys, dim=-1)\n",
    "        queries = F.normalize(queries, dim=-1)\n",
    "        \n",
    "        # Values and gates\n",
    "        values = self.W_v(x).view(B, T, H, V)\n",
    "        beta = torch.sigmoid(self.W_beta(x)).unsqueeze(-1)  # [B, T, H, 1]\n",
    "        g = torch.sigmoid(self.W_g(x)).unsqueeze(-1)  # [B, T, H, 1]\n",
    "        \n",
    "        # Sequential delta rule update\n",
    "        outputs = []\n",
    "        S = state.clone()\n",
    "        \n",
    "        for t in range(T):\n",
    "            k_t = keys[:, t]  # [B, H, K]\n",
    "            q_t = queries[:, t]  # [B, H, K]\n",
    "            v_t = values[:, t]  # [B, H, V]\n",
    "            beta_t = beta[:, t]  # [B, H, 1]\n",
    "            g_t = g[:, t]  # [B, H, 1]\n",
    "            \n",
    "            # Retrieval: q @ S\n",
    "            retrieved = torch.einsum('bhk,bhkv->bhv', q_t, S)  # [B, H, V]\n",
    "            \n",
    "            # Current at key position: k @ S\n",
    "            current = torch.einsum('bhk,bhkv->bhv', k_t, S)  # [B, H, V]\n",
    "            \n",
    "            # Delta rule update\n",
    "            delta = v_t - current  # [B, H, V]\n",
    "            update = torch.einsum('bhk,bhv->bhkv', k_t, delta)  # [B, H, K, V]\n",
    "            S = g_t.unsqueeze(-1) * S + beta_t.unsqueeze(-1) * update\n",
    "            \n",
    "            outputs.append(retrieved)\n",
    "        \n",
    "        # Stack outputs\n",
    "        output = torch.stack(outputs, dim=1)  # [B, T, H, V]\n",
    "        output = output.view(B, T, H * V)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        diagnostics = {\n",
    "            'beta_mean': beta.mean().item(),\n",
    "            'g_mean': g.mean().item(),\n",
    "        }\n",
    "        \n",
    "        return output, S, diagnostics\n",
    "\n",
    "\n",
    "# Test the model\n",
    "print(\"=\" * 70)\n",
    "print(\"Testing GDNContextKeys on Multi-Needle Sequence\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create model\n",
    "gdn_ctx = GDNContextKeys(\n",
    "    d_model=512, n_heads=8, head_dim=64, value_dim=128,\n",
    "    conv_width=4, beta_bias=-4.0, g_bias=4.0\n",
    ").to(DEVICE)\n",
    "\n",
    "# Create test sequence with 2 needles\n",
    "# Format: [haystack...] MARKER needle1 [haystack...] MARKER needle2 [haystack...] CUE1 CUE2\n",
    "seq_len = 200\n",
    "vocab_size = 256\n",
    "MARKER = 254\n",
    "CUE = 255\n",
    "\n",
    "# Build sequence\n",
    "tokens = torch.randint(0, 250, (1, seq_len), device=DEVICE)\n",
    "\n",
    "# Insert needles at different positions with different contexts\n",
    "needle_pos_1 = 30\n",
    "needle_pos_2 = 100\n",
    "cue_pos_1 = 150\n",
    "cue_pos_2 = 160\n",
    "\n",
    "# Make context before each MARKER different\n",
    "tokens[0, needle_pos_1] = MARKER\n",
    "tokens[0, needle_pos_1 + 1] = 42  # needle value 1\n",
    "\n",
    "tokens[0, needle_pos_2] = MARKER  \n",
    "tokens[0, needle_pos_2 + 1] = 77  # needle value 2\n",
    "\n",
    "tokens[0, cue_pos_1] = CUE\n",
    "tokens[0, cue_pos_2] = CUE\n",
    "\n",
    "# Create embeddings\n",
    "embed = nn.Embedding(vocab_size, 512).to(DEVICE)\n",
    "x = embed(tokens)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output, state, diag = gdn_ctx(x)\n",
    "\n",
    "print(f\"\\nSequence structure:\")\n",
    "print(f\"  MARKER_1 at pos {needle_pos_1}, context: {tokens[0, needle_pos_1-4:needle_pos_1].tolist()}\")\n",
    "print(f\"  MARKER_2 at pos {needle_pos_2}, context: {tokens[0, needle_pos_2-4:needle_pos_2].tolist()}\")\n",
    "print(f\"  CUE_1 at pos {cue_pos_1}, context: {tokens[0, cue_pos_1-4:cue_pos_1].tolist()}\")\n",
    "print(f\"  CUE_2 at pos {cue_pos_2}, context: {tokens[0, cue_pos_2-4:cue_pos_2].tolist()}\")\n",
    "\n",
    "print(f\"\\nDiagnostics: beta_mean={diag['beta_mean']:.4f}, g_mean={diag['g_mean']:.4f}\")\n",
    "\n",
    "# Check key similarity at MARKER positions\n",
    "x_t = x.transpose(1, 2)\n",
    "keys_raw = gdn_ctx.key_conv(x_t)[:, :, :seq_len].transpose(1, 2)\n",
    "keys = keys_raw.view(1, seq_len, 8, 64)\n",
    "keys = F.normalize(keys, dim=-1)\n",
    "\n",
    "k1 = keys[0, needle_pos_1]  # [H, K]\n",
    "k2 = keys[0, needle_pos_2]  # [H, K]\n",
    "\n",
    "print(f\"\\nKey similarity between MARKER_1 and MARKER_2:\")\n",
    "for h in range(8):\n",
    "    sim = torch.dot(k1[h], k2[h]).item()\n",
    "    print(f\"  Head {h}: sim = {sim:.4f}\")\n",
    "\n",
    "avg_sim = torch.einsum('hk,hk->h', k1, k2).mean().item()\n",
    "print(f\"\\nAvg similarity across heads: {avg_sim:.4f}\")\n",
    "print(f\"Target: < 0.3 for multi-needle to work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d56992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Config: key_conv_width=4\n",
      "Architecture: GS (GDN + SWA)\n",
      "Model params: 8,403,480\n",
      "GDN has k_conv: True\n",
      "\n",
      "============================================================\n",
      "MARKER Key Similarity Analysis\n",
      "============================================================\n",
      "MARKER_1 at pos 50, context: [199, 20, 185, 15]\n",
      "MARKER_2 at pos 150, context: [92, 189, 60, 67]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================\n",
    "TEST: Context-Based Keys in Actual GDN+SWA Architecture\n",
    "============================================================\n",
    "Using the modified GatedDeltaNetLayer with key_conv_width=4\n",
    "\"\"\"\n",
    "\n",
    "# Reload modules to pick up changes\n",
    "import importlib\n",
    "import sys\n",
    "for mod_name in list(sys.modules.keys()):\n",
    "    if 'config' in mod_name or 'model' in mod_name or 'core' in mod_name:\n",
    "        del sys.modules[mod_name]\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Fresh imports\n",
    "sys.path.insert(0, '/home/m_tes/groundthink/gt-v6/v7-design/groundthink_v7')\n",
    "from config import HybridConfig\n",
    "from model import TransparentHybrid, GatedDeltaNetLayer\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Create config with context-based keys\n",
    "cfg_conv = HybridConfig(\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    head_dim=64,\n",
    "    value_dim=128,\n",
    "    vocab_size=256,\n",
    "    layer_pattern=\"GS\",\n",
    "    window_size=64,\n",
    "    beta_bias=-4.0,\n",
    "    g_bias=4.0,\n",
    "    shifted_value=True,\n",
    "    key_conv_width=4,  # <-- THE FIX: 4-token context window\n",
    ")\n",
    "\n",
    "print(f\"\\nConfig: key_conv_width={cfg_conv.key_conv_width}\")\n",
    "print(f\"Architecture: {cfg_conv.layer_pattern} (GDN + SWA)\")\n",
    "\n",
    "# Create model\n",
    "model_conv = TransparentHybrid(cfg_conv).to(DEVICE)\n",
    "print(f\"Model params: {model_conv.count_params():,}\")\n",
    "\n",
    "# Get the GDN layer\n",
    "gdn_layer = model_conv.layers[0]\n",
    "print(f\"GDN has k_conv: {hasattr(gdn_layer, 'k_conv')}\")\n",
    "\n",
    "# Test: Create sequence with 2 MARKER tokens at different positions\n",
    "# Each MARKER has DIFFERENT context before it\n",
    "seq_len = 256\n",
    "MARKER = 254\n",
    "\n",
    "tokens = torch.randint(0, 250, (1, seq_len), device=DEVICE)\n",
    "\n",
    "# Insert MARKERs with different preceding context\n",
    "marker_pos_1 = 50\n",
    "marker_pos_2 = 150\n",
    "\n",
    "# Ensure different context by using different random tokens\n",
    "tokens[0, marker_pos_1] = MARKER\n",
    "tokens[0, marker_pos_2] = MARKER\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MARKER Key Similarity Analysis\")\n",
    "print('='*60)\n",
    "print(f\"MARKER_1 at pos {marker_pos_1}, context: {tokens[0, marker_pos_1-4:marker_pos_1].tolist()}\")\n",
    "print(f\"MARKER_2 at pos {marker_pos_2}, context: {tokens[0, marker_pos_2-4:marker_pos_2].tolist()}\")\n",
    "\n",
    "# Get embeddings\n",
    "x = model_conv.embed(tokens)\n",
    "x_norm = gdn_layer.norm(x)\n",
    "\n",
    "# Get keys using context conv\n",
    "x_t = x_norm.transpose(1, 2)\n",
    "k_raw = gdn_layer.k_conv(x_t)[:, :, :seq_len]\n",
    "k_full = k_raw.transpose(1, 2).view(1, seq_len, 8, 64)\n",
    "k_full = F.normalize(k_full, dim=-1)\n",
    "\n",
    "# Compare keys at MARKER positions\n",
    "k1 = k_full[0, marker_pos_1]  # [H, K]\n",
    "k2 = k_full[0, marker_pos_2]  # [H, K]\n",
    "\n",
    "print(f\"\\nPer-head key similarity (MARKER_1 vs MARKER_2):\")\n",
    "for h in range(8):\n",
    "    sim = torch.dot(k1[h], k2[h]).item()\n",
    "    status = \"✓\" if abs(sim) < 0.3 else \"✗\"\n",
    "    print(f\"  Head {h}: sim = {sim:+.4f} {status}\")\n",
    "\n",
    "avg_sim = (k1 * k2).sum(dim=-1).mean().item()\n",
    "print(f\"\\nAvg similarity: {avg_sim:+.4f}\")\n",
    "print(f\"Target for multi-needle: |sim| < 0.3\")\n",
    "print(f\"Result: {'✓ GOOD - keys are different!' if abs(avg_sim) < 0.3 else '✗ TOO SIMILAR'}\")\n",
    "\n",
    "# Compare with standard (no conv) model\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Comparison: With vs Without Context Conv\")\n",
    "print('='*60)\n",
    "\n",
    "cfg_std = HybridConfig(\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    head_dim=64,\n",
    "    value_dim=128,\n",
    "    vocab_size=256,\n",
    "    layer_pattern=\"GS\",\n",
    "    key_conv_width=1,  # Standard linear projection\n",
    ")\n",
    "\n",
    "model_std = TransparentHybrid(cfg_std).to(DEVICE)\n",
    "gdn_std = model_std.layers[0]\n",
    "\n",
    "x_std = model_std.embed(tokens)\n",
    "x_std_norm = gdn_std.norm(x_std)\n",
    "k_std = gdn_std.k_proj(x_std_norm).view(1, seq_len, 8, 64)\n",
    "k_std = F.normalize(k_std, dim=-1)\n",
    "\n",
    "k1_std = k_std[0, marker_pos_1]\n",
    "k2_std = k_std[0, marker_pos_2]\n",
    "\n",
    "avg_sim_std = (k1_std * k2_std).sum(dim=-1).mean().item()\n",
    "print(f\"Standard (no conv):   avg_sim = {avg_sim_std:+.4f}\")\n",
    "print(f\"Context conv (w=4):   avg_sim = {avg_sim:+.4f}\")\n",
    "print(f\"\\nImprovement: {abs(avg_sim_std) - abs(avg_sim):.4f} reduction in similarity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
