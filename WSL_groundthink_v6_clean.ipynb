{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GroundThink V6 - Hybrid GatedDeltaNet + SWA (WSL Local)\n",
        "\n",
        "**Gated Delta Rule:** `Sₜ = αₜ Sₜ₋₁ + βₜ Δₜ`\n",
        "- `αₜ` (gate): rapid forgetting from Mamba2\n",
        "- `βₜΔₜ` (delta): targeted updates from DeltaNet\n",
        "\n",
        "**Architecture:** GatedDeltaNet (FLA) + SlidingWindowAttention (flash_attn)\n",
        "\n",
        "**Required Environment:**\n",
        "- PyTorch nightly (cu126)\n",
        "- flash-attn (prebuilt wheel)\n",
        "- flash-linear-attention 0.4.2+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.12.3 (main, Jan  8 2026, 11:30:50) [GCC 13.3.0]\n",
            "PyTorch: 2.11.0.dev20260128+cu128\n",
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
            "Compute Capability: (8, 9)\n",
            "flash_attn: 2.8.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/m_tes/groundthink/gt-v6/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FLA GatedDeltaNet: OK\n",
            "\n",
            "✓ Environment ready\n"
          ]
        }
      ],
      "source": [
        "# CELL 0: VERIFY ENVIRONMENT\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Compute Capability: {torch.cuda.get_device_capability(0)}\")\n",
        "\n",
        "# Verify flash_attn\n",
        "try:\n",
        "    from flash_attn import flash_attn_func\n",
        "    import flash_attn\n",
        "    print(f\"flash_attn: {flash_attn.__version__}\")\n",
        "    FLASH_ATTN_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    print(f\"flash_attn: NOT AVAILABLE - {e}\")\n",
        "    FLASH_ATTN_AVAILABLE = False\n",
        "\n",
        "# Verify FLA\n",
        "try:\n",
        "    from fla.layers import GatedDeltaNet\n",
        "    print(\"FLA GatedDeltaNet: OK\")\n",
        "except ImportError as e:\n",
        "    raise ImportError(f\"FLA not available: {e}\")\n",
        "\n",
        "# Enable TF32 for Ampere+\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "print(\"\\n✓ Environment ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA GeForce RTX 4050 Laptop GPU (Compute 8.9, 6.4GB)\n",
            "FlashAttention: ENABLED\n",
            "Training dtype: torch.bfloat16\n",
            "\n",
            "Config: d=256, layers=12, SWA@[3, 7, 11]\n"
          ]
        }
      ],
      "source": [
        "# CELL 1: CONFIG\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Hardware detection\n",
        "USE_FLASH = False\n",
        "DTYPE = torch.float32\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    major, minor = torch.cuda.get_device_capability(0)\n",
        "    print(f\"GPU: {props.name} (Compute {major}.{minor}, {props.total_memory/1e9:.1f}GB)\")\n",
        "    \n",
        "    # FlashAttention requires Ampere+ (sm_80+)\n",
        "    if major >= 8 and FLASH_ATTN_AVAILABLE:\n",
        "        USE_FLASH = True\n",
        "        print(\"FlashAttention: ENABLED\")\n",
        "    else:\n",
        "        print(f\"FlashAttention: DISABLED (need Ampere+ and flash_attn installed)\")\n",
        "    \n",
        "    # bfloat16 for Ampere+, float16 for older\n",
        "    DTYPE = torch.bfloat16 if major >= 8 else torch.float16\n",
        "    print(f\"Training dtype: {DTYPE}\")\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    vocab_size: int = 50257\n",
        "    d_model: int = 256        # Small for RTX 4050\n",
        "    n_layers: int = 12\n",
        "    n_heads: int = 8\n",
        "    head_dim: int = 32\n",
        "    attn_interval: int = 4    # SWA every 4th layer (3:1 ratio)\n",
        "    window_size: int = 512\n",
        "    expand_k: float = 1.0\n",
        "    expand_v: float = 2.0\n",
        "    use_gradient_checkpointing: bool = True\n",
        "    tie_weights: bool = True\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "    \n",
        "    def get_swa_layer_indices(self):\n",
        "        return [i for i in range(self.n_layers) if i % self.attn_interval == (self.attn_interval - 1)]\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    dataset_name: str = \"HuggingFaceFW/fineweb-edu\"\n",
        "    dataset_subset: str = \"sample-10BT\"\n",
        "    target_tokens: int = 10_000_000  # Smaller for local\n",
        "    batch_size: int = 2\n",
        "    seq_len: int = 512\n",
        "    accum_steps: int = 2\n",
        "    steps: int = 5000\n",
        "    warmup_ratio: float = 0.1\n",
        "    lr: float = 3e-4\n",
        "    weight_decay: float = 0.1\n",
        "    grad_clip: float = 1.0\n",
        "    betas: tuple = (0.9, 0.95)\n",
        "    log_interval: int = 50\n",
        "    grad_log_interval: int = 500\n",
        "    niah_checkpoints: List[int] = field(default_factory=lambda: [500, 1000, 2000, 3000, 5000])\n",
        "    \n",
        "    @property\n",
        "    def warmup_steps(self): return int(self.steps * self.warmup_ratio)\n",
        "    @property\n",
        "    def effective_batch_size(self): return self.batch_size * self.accum_steps\n",
        "\n",
        "MODEL_CFG = ModelConfig()\n",
        "TRAIN_CFG = TrainConfig()\n",
        "print(f\"\\nConfig: d={MODEL_CFG.d_model}, layers={MODEL_CFG.n_layers}, SWA@{MODEL_CFG.get_swa_layer_indices()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model components defined\n"
          ]
        }
      ],
      "source": [
        "# CELL 2: MODEL COMPONENTS\n",
        "from xml.parsers.expat import model\n",
        "from transformers import AutoTokenizer\n",
        "from fla.layers import GatedDeltaNet\n",
        "\n",
        "if USE_FLASH:\n",
        "    from flash_attn import flash_attn_func\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        norm = x.float().pow(2).mean(-1, keepdim=True).add(self.eps).rsqrt()\n",
        "        return (x.float() * norm).type_as(x) * self.weight\n",
        "\n",
        "\n",
        "class SwiGLUFFN(nn.Module):\n",
        "    def __init__(self, d_model, expansion=8/3):\n",
        "        super().__init__()\n",
        "        hidden = ((int(d_model * expansion) + 63) // 64) * 64\n",
        "        self.w1 = nn.Linear(d_model, hidden, bias=False)\n",
        "        self.w3 = nn.Linear(d_model, hidden, bias=False)\n",
        "        self.w2 = nn.Linear(hidden, d_model, bias=False)\n",
        "        self.norm = RMSNorm(d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.norm(x)\n",
        "        return x + self.w2(F.silu(self.w1(h)) * self.w3(h))\n",
        "\n",
        "\n",
        "class SlidingWindowAttention(nn.Module):\n",
        "    \"\"\"SWA with KV-Cache for inference.\"\"\"\n",
        "    def __init__(self, d_model, n_heads, window_size, layer_idx=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.window_size = window_size\n",
        "        \n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "    \n",
        "    def forward(self, x, past_key_values=None, use_cache=False):\n",
        "        B, T, D = x.shape\n",
        "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
        "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
        "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
        "        \n",
        "        current_cache = None\n",
        "        if use_cache:\n",
        "            if past_key_values is not None:\n",
        "                pk, pv = past_key_values\n",
        "                k = torch.cat([pk, k], dim=1)\n",
        "                v = torch.cat([pv, v], dim=1)\n",
        "            current_cache = (k[:, -self.window_size:].detach(), v[:, -self.window_size:].detach())\n",
        "        \n",
        "        # Inference mode with cache\n",
        "        if use_cache and past_key_values is not None:\n",
        "            q_t, k_t, v_t = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "            out = F.scaled_dot_product_attention(q_t, k_t, v_t, is_causal=False)\n",
        "            out = out.transpose(1, 2)\n",
        "        elif USE_FLASH:\n",
        "            # Training with FlashAttention\n",
        "            out = flash_attn_func(q, k, v, causal=True, window_size=(self.window_size, 0))\n",
        "        else:\n",
        "            # Manual sliding window fallback\n",
        "            q_t, k_t, v_t = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "            mask = torch.ones(T, T, device=x.device, dtype=torch.bool).triu(1)\n",
        "            mask |= torch.ones(T, T, device=x.device, dtype=torch.bool).tril(-self.window_size - 1)\n",
        "            attn = (q_t @ k_t.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "            attn = attn.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "            out = (F.softmax(attn, dim=-1) @ v_t).transpose(1, 2)\n",
        "        \n",
        "        return self.out_proj(out.reshape(B, T, D)), current_cache\n",
        "\n",
        "\n",
        "class HybridBlock(nn.Module):\n",
        "    \"\"\"GatedDeltaNet or SlidingWindowAttention block.\"\"\"\n",
        "    def __init__(self, d_model, is_attention, n_heads=8, window_size=512,\n",
        "                 expand_k=1.0, expand_v=2.0, layer_idx=0):\n",
        "        super().__init__()\n",
        "        self.is_attention = is_attention\n",
        "        self.layer_idx = layer_idx\n",
        "        self.norm = RMSNorm(d_model)\n",
        "        \n",
        "        if is_attention:\n",
        "            self.layer = SlidingWindowAttention(d_model, n_heads, window_size, layer_idx)\n",
        "        else:\n",
        "            # GatedDeltaNet: Sₜ = αₜ Sₜ₋₁ + βₜ Δₜ\n",
        "            self.layer = GatedDeltaNet(\n",
        "                hidden_size=d_model,\n",
        "                expand_k=expand_k,\n",
        "                expand_v=expand_v,\n",
        "                layer_idx=layer_idx\n",
        "            )\n",
        "    \n",
        "    def forward(self, x, past_state=None, use_cache=False):\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "        new_state = None\n",
        "        \n",
        "        if self.is_attention:\n",
        "            x, new_state = self.layer(x, past_key_values=past_state, use_cache=use_cache)\n",
        "        else:\n",
        "            # GatedDeltaNet always returns (output, state) tuple\n",
        "            if use_cache:\n",
        "                x, new_state = self.layer(x, initial_state=past_state, use_cache=True, output_final_state=True)\n",
        "            else:\n",
        "                out = self.layer(x)\n",
        "                if isinstance(out, tuple):\n",
        "                    x = out[0]\n",
        "                    new_state = out[-1] if len(out) > 1 else None\n",
        "                else:\n",
        "                    x = out\n",
        "        \n",
        "        return residual + x, new_state\n",
        "\n",
        "\n",
        "class GroundThinkLM(nn.Module):\n",
        "    \"\"\"Hybrid LM: GatedDeltaNet + SlidingWindowAttention\"\"\"\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
        "        nn.init.normal_(self.embed.weight, mean=0.0, std=0.02)\n",
        "        print(f\"New embed std: {self.embed.weight.std().item():.4f}\")\n",
        "\n",
        "        swa_indices = set(cfg.get_swa_layer_indices())\n",
        "        self._swa_indices = swa_indices\n",
        "        \n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.ffns = nn.ModuleList()\n",
        "        for i in range(cfg.n_layers):\n",
        "            self.blocks.append(HybridBlock(\n",
        "                cfg.d_model, is_attention=(i in swa_indices),\n",
        "                n_heads=cfg.n_heads, window_size=cfg.window_size,\n",
        "                expand_k=cfg.expand_k, expand_v=cfg.expand_v, layer_idx=i\n",
        "            ))\n",
        "            self.ffns.append(SwiGLUFFN(cfg.d_model))\n",
        "        \n",
        "        self.norm_f = RMSNorm(cfg.d_model)\n",
        "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
        "        if cfg.tie_weights:\n",
        "            self.lm_head.weight = self.embed.weight\n",
        "        else:\n",
        "            nn.init.normal_(self.lm_head.weight, mean=0.0, std=0.02)\n",
        "            \n",
        "    def forward(self, input_ids, targets=None, past_states=None, use_cache=False):\n",
        "        x = self.embed(input_ids)\n",
        "        new_states = [] if use_cache else None\n",
        "        \n",
        "        for i, (block, ffn) in enumerate(zip(self.blocks, self.ffns)):\n",
        "            layer_past = past_states[i] if (past_states is not None and len(past_states) > i) else None\n",
        "            \n",
        "            if self.cfg.use_gradient_checkpointing and self.training and not use_cache and i in self._swa_indices:\n",
        "                x = checkpoint(self._fwd_block, block, ffn, x, use_reentrant=False)\n",
        "            else:\n",
        "                x, layer_new_state = block(x, layer_past, use_cache)\n",
        "                x = ffn(x)\n",
        "                if use_cache:\n",
        "                    new_states.append(layer_new_state)\n",
        "        \n",
        "        logits = self.lm_head(self.norm_f(x))\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None\n",
        "        return logits, loss, new_states\n",
        "    \n",
        "    @staticmethod\n",
        "    def _fwd_block(block, ffn, x):\n",
        "        x, _ = block(x, None, False)\n",
        "        return ffn(x)\n",
        "    \n",
        "    def get_layer_types(self):\n",
        "        return ['SWA' if i in self._swa_indices else 'GDN' for i in range(self.cfg.n_layers)]\n",
        "    \n",
        "    def count_parameters(self):\n",
        "        c = {'embed': sum(p.numel() for p in self.embed.parameters()), 'gdn': 0, 'swa': 0, 'ffn': 0}\n",
        "        for i, (b, f) in enumerate(zip(self.blocks, self.ffns)):\n",
        "            bp = sum(p.numel() for p in b.parameters())\n",
        "            fp = sum(p.numel() for p in f.parameters())\n",
        "            c['swa' if i in self._swa_indices else 'gdn'] += bp\n",
        "            c['ffn'] += fp\n",
        "        c['total'] = sum(c.values())\n",
        "        return c\n",
        "\n",
        "print(\"Model components defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New embed std: 0.0200\n",
            "Output tuple length: 3\n",
            "  [0]: shape=torch.Size([1, 128, 256]), dtype=torch.bfloat16\n",
            "  [1]: None\n",
            "  [2]: None\n"
          ]
        }
      ],
      "source": [
        "# Rebuild model reference\n",
        "model = GroundThinkLM(MODEL_CFG).to(DEVICE).to(DTYPE)\n",
        "\n",
        "# Now probe GatedDeltaNet state in eval mode (where it returns state)\n",
        "from fla.layers import GatedDeltaNet\n",
        "\n",
        "gdn = GatedDeltaNet(\n",
        "    hidden_size=256,\n",
        "    num_heads=8,\n",
        "    head_dim=32,\n",
        "    mode='chunk',\n",
        "    layer_idx=0\n",
        ").cuda().bfloat16()\n",
        "\n",
        "gdn.eval()\n",
        "x = torch.randn(1, 128, 256, device='cuda', dtype=torch.bfloat16)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Try to get state\n",
        "    out = gdn(x, use_cache=True, output_attentions=True)\n",
        "    \n",
        "print(f\"Output tuple length: {len(out)}\")\n",
        "for i, item in enumerate(out):\n",
        "    if item is None:\n",
        "        print(f\"  [{i}]: None\")\n",
        "    elif hasattr(item, 'shape'):\n",
        "        print(f\"  [{i}]: shape={item.shape}, dtype={item.dtype}\")\n",
        "    else:\n",
        "        print(f\"  [{i}]: type={type(item)}\")\n",
        "        # Dig deeper\n",
        "        if hasattr(item, '__dict__'):\n",
        "            for k, v in vars(item).items():\n",
        "                if hasattr(v, 'shape'):\n",
        "                    print(f\"       .{k}: shape={v.shape}\")\n",
        "                elif v is not None:\n",
        "                    print(f\"       .{k}: {type(v)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'abc', 'attn', 'based', 'chunk_abc', 'chunk_comba', 'chunk_delta_rule', 'chunk_dplr_delta_rule', 'chunk_gated_delta_rule', 'chunk_gla', 'chunk_gsa', 'chunk_iplr_delta_rule', 'chunk_kda', 'chunk_lightning_attn', 'chunk_linear_attn', 'chunk_log_linear_attn', 'chunk_mesa_net', 'chunk_retention', 'chunk_rwkv6', 'chunk_rwkv7', 'chunk_simple_gla', 'comba', 'common', 'delta_rule', 'deltaformer', 'forgetting_attn', 'fused_chunk_based', 'fused_chunk_delta_rule', 'fused_chunk_gla', 'fused_chunk_linear_attn', 'fused_chunk_retention', 'fused_chunk_simple_gla', 'fused_recurrent_comba', 'fused_recurrent_delta_rule', 'fused_recurrent_dplr_delta_rule', 'fused_recurrent_gated_delta_rule', 'fused_recurrent_gla', 'fused_recurrent_gsa', 'fused_recurrent_hgrn', 'fused_recurrent_iplr_delta_rule', 'fused_recurrent_kda', 'fused_recurrent_lightning_attn', 'fused_recurrent_linear_attn', 'fused_recurrent_retention', 'fused_recurrent_rwkv6', 'fused_recurrent_rwkv7', 'fused_recurrent_simple_gla', 'gated_delta_product', 'gated_delta_rule', 'generalized_delta_rule', 'gla', 'gsa', 'hgrn', 'kda', 'lightning_attn', 'linear_attn', 'log_linear_attn', 'mesa_net', 'nsa', 'parallel_attn', 'parallel_based', 'parallel_forgetting_attn', 'parallel_nsa', 'parallel_path_attn', 'parallel_retention', 'parallel_simple_gla', 'path_attn', 'rebased', 'retention', 'rwkv6', 'rwkv7', 'simple_gla', 'utils']\n",
            "['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'chunk', 'chunk_gated_delta_rule', 'fused_recurrent', 'fused_recurrent_gated_delta_rule', 'wy_fast']\n",
            "\n",
            "chunk_gated_delta_rule parameters:\n",
            "  q: REQUIRED\n",
            "  k: REQUIRED\n",
            "  v: REQUIRED\n",
            "  g: REQUIRED\n",
            "  beta: REQUIRED\n",
            "  scale: None\n",
            "  initial_state: None\n",
            "  output_final_state: False\n",
            "  use_qk_l2norm_in_kernel: False\n",
            "  cu_seqlens: None\n",
            "  kwargs: REQUIRED\n"
          ]
        }
      ],
      "source": [
        "import fla.ops as fla_ops\n",
        "print(dir(fla_ops))\n",
        "\n",
        "# Look for gated_delta or delta_net ops\n",
        "import fla.ops.gated_delta_rule as gdr\n",
        "print(dir(gdr))\n",
        "\n",
        "# Check the actual chunk function signature\n",
        "import inspect\n",
        "if hasattr(gdr, 'chunk_gated_delta_rule'):\n",
        "    sig = inspect.signature(gdr.chunk_gated_delta_rule)\n",
        "    print(\"\\nchunk_gated_delta_rule parameters:\")\n",
        "    for name, param in sig.parameters.items():\n",
        "        print(f\"  {name}: {param.default if param.default is not inspect.Parameter.empty else 'REQUIRED'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: torch.Size([1, 128, 8, 64])\n",
            "Final state: torch.Size([1, 8, 32, 64])\n",
            "State dtype: torch.float32\n"
          ]
        }
      ],
      "source": [
        "from fla.ops.gated_delta_rule import chunk_gated_delta_rule\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "B, T, H, D = 1, 128, 8, 32  # batch, seq, heads, head_dim\n",
        "V = D * 2  # value dim (expand_v=2)\n",
        "\n",
        "q = torch.randn(B, T, H, D, device='cuda', dtype=torch.bfloat16)\n",
        "k = F.normalize(torch.randn(B, T, H, D, device='cuda', dtype=torch.float32), p=2, dim=-1).to(torch.bfloat16)\n",
        "v = torch.randn(B, T, H, V, device='cuda', dtype=torch.bfloat16)\n",
        "beta = torch.rand(B, T, H, device='cuda', dtype=torch.bfloat16).sigmoid()\n",
        "g = F.logsigmoid(torch.rand(B, T, H, device='cuda', dtype=torch.bfloat16))  # gate in log space\n",
        "\n",
        "# WITH output_final_state=True\n",
        "output, final_state = chunk_gated_delta_rule(q, k, v, g, beta, output_final_state=True)\n",
        "\n",
        "print(f\"Output: {output.shape}\")\n",
        "print(f\"Final state: {final_state.shape if final_state is not None else None}\")\n",
        "print(f\"State dtype: {final_state.dtype if final_state is not None else None}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape: torch.Size([1, 16, 50257])\n",
            "Logits mean: 0.0005, std: 0.3203\n",
            "Expected init loss: 10.8249\n",
            "GDN block change magnitude: 0.028198\n",
            "SWA block change magnitude: 0.106445\n"
          ]
        }
      ],
      "source": [
        "# Sanity check the model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. Check embedding -> logit path\n",
        "model.eval()\n",
        "x = torch.randint(0, 1000, (1, 16), device='cuda')\n",
        "with torch.no_grad():\n",
        "    logits, _, _ = model(x)\n",
        "\n",
        "print(f\"Logits shape: {logits.shape}\")\n",
        "print(f\"Logits mean: {logits.mean().item():.4f}, std: {logits.std().item():.4f}\")\n",
        "print(f\"Expected init loss: {np.log(MODEL_CFG.vocab_size):.4f}\")\n",
        "\n",
        "# 2. Check if GatedDeltaNet is actually processing (not just passing through)\n",
        "gdn_block = model.blocks[0]  # First GDN block\n",
        "x_in = model.embed(x)\n",
        "x_out, _ = gdn_block(x_in, None, False)\n",
        "diff = (x_out - x_in).abs().mean().item()\n",
        "print(f\"GDN block change magnitude: {diff:.6f}\")\n",
        "\n",
        "# 3. Check SWA block\n",
        "swa_idx = list(model._swa_indices)[0]\n",
        "swa_block = model.blocks[swa_idx]\n",
        "x_out_swa, _ = swa_block(x_in, None, False)\n",
        "diff_swa = (x_out_swa - x_in).abs().mean().item()\n",
        "print(f\"SWA block change magnitude: {diff_swa:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Triton version: 3.6.0\n",
            "GDN mode: chunk\n",
            "\n",
            "Weight statistics:\n",
            "Embedding std: 1.0000\n",
            "LM head std: 1.0000\n",
            "Block 0 q_proj std: 0.0361\n",
            "Block 0 o_proj std: 0.0104\n",
            "Block 1 q_proj std: 0.0361\n",
            "Block 1 o_proj std: 0.0104\n",
            "Block 2 q_proj std: 0.0361\n",
            "Block 2 o_proj std: 0.0104\n"
          ]
        }
      ],
      "source": [
        "# 1. Check if Triton kernels are being called\n",
        "import os\n",
        "os.environ['TRITON_PRINT_AUTOTUNING'] = '1'  # Will print when triton tunes\n",
        "\n",
        "# Run a forward pass and watch for triton output\n",
        "x = torch.randint(0, 1000, (1, 64), device='cuda')\n",
        "with torch.no_grad():\n",
        "    out = model(x)\n",
        "\n",
        "# 2. Check triton is importable and what FLA is using\n",
        "import triton\n",
        "print(f\"Triton version: {triton.__version__}\")\n",
        "\n",
        "# 3. Check what mode GatedDeltaNet is actually using\n",
        "gdn_layer = model.blocks[0].layer  # The actual GatedDeltaNet\n",
        "print(f\"GDN mode: {gdn_layer.mode}\")\n",
        "\n",
        "# 4. Check the weight scales - this is likely the real problem\n",
        "print(\"\\nWeight statistics:\")\n",
        "print(f\"Embedding std: {model.embed.weight.std().item():.4f}\")\n",
        "print(f\"LM head std: {model.lm_head.weight.std().item():.4f}\")\n",
        "\n",
        "for i, block in enumerate(model.blocks[:3]):\n",
        "    if hasattr(block.layer, 'q_proj'):\n",
        "        print(f\"Block {i} q_proj std: {block.layer.q_proj.weight.std().item():.4f}\")\n",
        "    if hasattr(block.layer, 'o_proj'):\n",
        "        print(f\"Block {i} o_proj std: {block.layer.o_proj.weight.std().item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape: torch.Size([2, 512, 50257])\n",
            "Logits range: [-82.50, 274.00]\n",
            "Loss: 235.8252\n",
            "Expected random loss: 10.8249\n",
            "Prob sum: 1.0000\n",
            "Max prob: 1.000000\n"
          ]
        }
      ],
      "source": [
        "# Check loss calculation\n",
        "x, y = get_batch()\n",
        "with torch.amp.autocast('cuda', dtype=DTYPE):\n",
        "    logits, loss, _ = model(x, y)\n",
        "\n",
        "print(f\"Logits shape: {logits.shape}\")\n",
        "print(f\"Logits range: [{logits.min().item():.2f}, {logits.max().item():.2f}]\")\n",
        "print(f\"Loss: {loss.item():.4f}\")\n",
        "print(f\"Expected random loss: {np.log(MODEL_CFG.vocab_size):.4f}\")\n",
        "\n",
        "# Check if logits are sane\n",
        "probs = F.softmax(logits[0, 0].float(), dim=-1)\n",
        "print(f\"Prob sum: {probs.sum().item():.4f}\")\n",
        "print(f\"Max prob: {probs.max().item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Monitoring functions ready\n"
          ]
        }
      ],
      "source": [
        "# CELL 3: MONITORING\n",
        "\n",
        "def print_gradient_summary(model):\n",
        "    agg = {'embed': [], 'gdn': [], 'swa': [], 'ffn': []}\n",
        "    for name, p in model.named_parameters():\n",
        "        if p.grad is None:\n",
        "            continue\n",
        "        n = p.grad.norm().item()\n",
        "        if 'embed' in name:\n",
        "            agg['embed'].append(n)\n",
        "        elif 'ffn' in name:\n",
        "            agg['ffn'].append(n)\n",
        "        elif 'blocks' in name:\n",
        "            idx = int(name.split('.')[1])\n",
        "            agg['swa' if idx in model._swa_indices else 'gdn'].append(n)\n",
        "    print(\"Gradients:\")\n",
        "    for k, v in agg.items():\n",
        "        if v:\n",
        "            print(f\"  {k}: mean={np.mean(v):.3f} max={np.max(v):.2f}\")\n",
        "\n",
        "\n",
        "def needle_test(model, tokenizer, seq_len=512, n_trials=50, needle_token=50250, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    probs = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_trials):\n",
        "            tokens = torch.randint(1000, 10000, (1, seq_len), device=device)\n",
        "            pos = torch.randint(64, seq_len - 64, (1,)).item()\n",
        "            tokens[0, pos] = needle_token\n",
        "            with torch.amp.autocast('cuda', dtype=DTYPE):\n",
        "                logits, _, _ = model(tokens)\n",
        "            probs.append(F.softmax(logits[0, -1].float(), dim=-1)[needle_token].item())\n",
        "    rc = 1.0 / tokenizer.vocab_size\n",
        "    return {'mean': np.mean(probs), 'ratio': np.mean(probs) / rc}\n",
        "\n",
        "\n",
        "def probe_layers(model, needle_id=50250, seq_len=512, pos=256, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    tokens = torch.randint(1000, 10000, (1, seq_len), device=device)\n",
        "    tokens[0, pos] = needle_id\n",
        "    with torch.no_grad():\n",
        "        x = model.embed(tokens)\n",
        "        emb = model.embed.weight[needle_id].float()\n",
        "        print(\"Needle representation through layers:\")\n",
        "        for i, (b, f) in enumerate(zip(model.blocks, model.ffns)):\n",
        "            x, _ = b(x, None, False)\n",
        "            x = f(x)\n",
        "            sim = F.cosine_similarity(x[0, pos].float(), emb, dim=0).item()\n",
        "            ltype = 'SWA' if i in model._swa_indices else 'GDN'\n",
        "            print(f\"  L{i:2d}[{ltype}]: {sim:+.3f}\")\n",
        "\n",
        "print(\"Monitoring functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming HuggingFaceFW/fineweb-edu...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   0%|          | 846/10000000 [00:04<14:49:03, 187.45tok/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1055 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Tokenizing: 10000241tok [00:24, 400932.79tok/s]                            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 10,000,000 tokens\n"
          ]
        }
      ],
      "source": [
        "# CELL 4: DATA LOADING\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "MODEL_CFG.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "print(f\"Streaming {TRAIN_CFG.dataset_name}...\")\n",
        "ds = load_dataset(TRAIN_CFG.dataset_name, name=TRAIN_CFG.dataset_subset, split=\"train\", streaming=True)\n",
        "\n",
        "buf = []\n",
        "pbar = tqdm(total=TRAIN_CFG.target_tokens, unit=\"tok\", desc=\"Tokenizing\")\n",
        "for ex in ds:\n",
        "    toks = tokenizer.encode(ex['text']) + [tokenizer.eos_token_id]\n",
        "    buf.extend(toks)\n",
        "    pbar.update(len(toks))\n",
        "    if len(buf) >= TRAIN_CFG.target_tokens:\n",
        "        break\n",
        "pbar.close()\n",
        "\n",
        "all_tokens = torch.tensor(buf[:TRAIN_CFG.target_tokens], dtype=torch.long)\n",
        "del buf, ds\n",
        "print(f\"Loaded {len(all_tokens):,} tokens\")\n",
        "\n",
        "def get_batch():\n",
        "    ix = torch.randint(len(all_tokens) - TRAIN_CFG.seq_len - 1, (TRAIN_CFG.batch_size,))\n",
        "    x = torch.stack([all_tokens[i:i+TRAIN_CFG.seq_len] for i in ix])\n",
        "    y = torch.stack([all_tokens[i+1:i+TRAIN_CFG.seq_len+1] for i in ix])\n",
        "    return x.to(DEVICE), y.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building model...\n",
            "Parameters: 48.71M\n",
            "  GDN: 28.57M, SWA: 0.79M, FFN: 6.49M\n",
            "Layers: ['GDN', 'GDN', 'GDN', 'SWA', 'GDN', 'GDN', 'GDN', 'SWA', 'GDN', 'GDN', 'GDN', 'SWA']\n",
            "\n",
            "Testing forward/backward...\n",
            "Forward OK: loss=235.1660\n",
            "Peak memory: 2.45GB\n"
          ]
        }
      ],
      "source": [
        "# CELL 5: BUILD MODEL\n",
        "print(\"Building model...\")\n",
        "model = GroundThinkLM(MODEL_CFG).to(DEVICE).to(DTYPE)\n",
        "\n",
        "p = model.count_parameters()\n",
        "print(f\"Parameters: {p['total']/1e6:.2f}M\")\n",
        "print(f\"  GDN: {p['gdn']/1e6:.2f}M, SWA: {p['swa']/1e6:.2f}M, FFN: {p['ffn']/1e6:.2f}M\")\n",
        "print(f\"Layers: {model.get_layer_types()}\")\n",
        "\n",
        "# Test forward/backward\n",
        "print(\"\\nTesting forward/backward...\")\n",
        "x, y = get_batch()\n",
        "with torch.amp.autocast('cuda', dtype=DTYPE):\n",
        "    _, loss, _ = model(x, y)\n",
        "loss.backward()\n",
        "print(f\"Forward OK: loss={loss.item():.4f}\")\n",
        "print(f\"Peak memory: {torch.cuda.max_memory_allocated()/1e9:.2f}GB\")\n",
        "model.zero_grad()\n",
        "torch.cuda.reset_peak_memory_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape: torch.Size([2, 512, 50257])\n",
            "Logits range: [-86.50, 272.00]\n",
            "Loss: 235.0752\n",
            "Expected random loss: 10.8249\n",
            "Prob sum: 1.0000\n",
            "Max prob: 1.000000\n"
          ]
        }
      ],
      "source": [
        "# Check loss calculation\n",
        "x, y = get_batch()\n",
        "with torch.amp.autocast('cuda', dtype=DTYPE):\n",
        "    logits, loss, _ = model(x, y)\n",
        "\n",
        "print(f\"Logits shape: {logits.shape}\")\n",
        "print(f\"Logits range: [{logits.min().item():.2f}, {logits.max().item():.2f}]\")\n",
        "print(f\"Loss: {loss.item():.4f}\")\n",
        "print(f\"Expected random loss: {np.log(MODEL_CFG.vocab_size):.4f}\")\n",
        "\n",
        "# Check if logits are sane\n",
        "probs = F.softmax(logits[0, 0].float(), dim=-1)\n",
        "print(f\"Prob sum: {probs.sum().item():.4f}\")\n",
        "print(f\"Max prob: {probs.max().item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TRAINING 5000 steps (effective batch=4)\n",
            "\n",
            "[    0/5000] loss=233.8555 lr=6.00e-07 2,756 tok/s\n",
            "[   50/5000] loss=232.5334 lr=3.06e-05 8,188 tok/s\n",
            "[  100/5000] loss=210.8668 lr=6.06e-05 8,748 tok/s\n",
            "[  150/5000] loss=64.4376 lr=9.06e-05 8,521 tok/s\n",
            "[  200/5000] loss=35.3392 lr=1.21e-04 8,606 tok/s\n",
            "[  250/5000] loss=32.2790 lr=1.51e-04 8,813 tok/s\n",
            "[  300/5000] loss=31.0059 lr=1.81e-04 8,979 tok/s\n",
            "[  350/5000] loss=30.5083 lr=2.11e-04 8,990 tok/s\n",
            "[  400/5000] loss=30.2468 lr=2.41e-04 9,079 tok/s\n",
            "[  450/5000] loss=29.6533 lr=2.71e-04 9,111 tok/s\n",
            "Gradients:\n",
            "  >>> NIAH@500: 0.00x random [FAIL]\n",
            "[  500/5000] loss=29.0275 lr=3.00e-04 7,580 tok/s\n",
            "[  550/5000] loss=28.3613 lr=3.00e-04 7,618 tok/s\n",
            "[  600/5000] loss=27.9643 lr=3.00e-04 7,711 tok/s\n",
            "[  650/5000] loss=27.6132 lr=2.99e-04 7,801 tok/s\n",
            "[  700/5000] loss=27.3086 lr=2.99e-04 7,847 tok/s\n",
            "[  750/5000] loss=26.9528 lr=2.98e-04 7,927 tok/s\n",
            "[  800/5000] loss=26.8805 lr=2.97e-04 7,958 tok/s\n",
            "[  850/5000] loss=26.6274 lr=2.96e-04 8,007 tok/s\n",
            "[  900/5000] loss=26.3930 lr=2.94e-04 8,062 tok/s\n",
            "[  950/5000] loss=26.2841 lr=2.93e-04 8,076 tok/s\n",
            "Gradients:\n",
            "  >>> NIAH@1000: 0.00x random [FAIL]\n",
            "[ 1000/5000] loss=26.1932 lr=2.91e-04 8,107 tok/s\n",
            "[ 1050/5000] loss=25.9664 lr=2.89e-04 8,175 tok/s\n",
            "[ 1100/5000] loss=25.8066 lr=2.87e-04 8,217 tok/s\n",
            "[ 1150/5000] loss=25.9862 lr=2.85e-04 8,272 tok/s\n",
            "[ 1200/5000] loss=25.8367 lr=2.82e-04 8,327 tok/s\n",
            "[ 1250/5000] loss=25.5796 lr=2.80e-04 8,352 tok/s\n",
            "[ 1300/5000] loss=25.3412 lr=2.77e-04 8,400 tok/s\n",
            "[ 1350/5000] loss=25.5642 lr=2.74e-04 8,433 tok/s\n",
            "[ 1400/5000] loss=25.3699 lr=2.71e-04 8,454 tok/s\n",
            "[ 1450/5000] loss=25.4290 lr=2.68e-04 8,485 tok/s\n",
            "Gradients:\n",
            "[ 1500/5000] loss=25.2992 lr=2.65e-04 8,523 tok/s\n",
            "[ 1550/5000] loss=25.2790 lr=2.61e-04 8,542 tok/s\n",
            "[ 1600/5000] loss=25.1626 lr=2.58e-04 8,575 tok/s\n",
            "[ 1650/5000] loss=25.0341 lr=2.54e-04 8,610 tok/s\n",
            "[ 1700/5000] loss=25.1927 lr=2.50e-04 8,609 tok/s\n",
            "[ 1750/5000] loss=24.8833 lr=2.46e-04 8,636 tok/s\n",
            "[ 1800/5000] loss=25.0903 lr=2.42e-04 8,653 tok/s\n",
            "[ 1850/5000] loss=25.0148 lr=2.38e-04 8,660 tok/s\n",
            "[ 1900/5000] loss=25.1018 lr=2.34e-04 8,651 tok/s\n",
            "[ 1950/5000] loss=24.9750 lr=2.29e-04 8,665 tok/s\n",
            "Gradients:\n",
            "  >>> NIAH@2000: 0.00x random [FAIL]\n",
            "[ 2000/5000] loss=24.8598 lr=2.25e-04 8,656 tok/s\n",
            "[ 2050/5000] loss=24.9396 lr=2.20e-04 8,683 tok/s\n",
            "[ 2100/5000] loss=24.8476 lr=2.16e-04 8,707 tok/s\n",
            "[ 2150/5000] loss=24.7094 lr=2.11e-04 8,719 tok/s\n",
            "[ 2200/5000] loss=24.6427 lr=2.06e-04 8,743 tok/s\n",
            "[ 2250/5000] loss=24.6404 lr=2.01e-04 8,766 tok/s\n",
            "[ 2300/5000] loss=24.5621 lr=1.96e-04 8,764 tok/s\n",
            "[ 2350/5000] loss=24.5903 lr=1.91e-04 8,761 tok/s\n",
            "[ 2400/5000] loss=25.0335 lr=1.86e-04 8,752 tok/s\n",
            "[ 2450/5000] loss=24.7160 lr=1.81e-04 8,749 tok/s\n",
            "Gradients:\n",
            "[ 2500/5000] loss=24.6187 lr=1.76e-04 8,763 tok/s\n",
            "[ 2550/5000] loss=24.2899 lr=1.71e-04 8,768 tok/s\n",
            "[ 2600/5000] loss=24.4374 lr=1.66e-04 8,752 tok/s\n",
            "[ 2650/5000] loss=24.7265 lr=1.60e-04 8,758 tok/s\n",
            "[ 2700/5000] loss=24.6548 lr=1.55e-04 8,762 tok/s\n",
            "[ 2750/5000] loss=24.2486 lr=1.50e-04 8,747 tok/s\n",
            "[ 2800/5000] loss=24.6833 lr=1.45e-04 8,750 tok/s\n",
            "[ 2850/5000] loss=24.3629 lr=1.40e-04 8,758 tok/s\n",
            "[ 2900/5000] loss=24.2761 lr=1.34e-04 8,735 tok/s\n",
            "[ 2950/5000] loss=24.2988 lr=1.29e-04 8,721 tok/s\n",
            "Gradients:\n",
            "  >>> NIAH@3000: 0.00x random [FAIL]\n",
            "[ 3000/5000] loss=24.5079 lr=1.24e-04 8,707 tok/s\n",
            "[ 3050/5000] loss=24.4690 lr=1.19e-04 8,719 tok/s\n",
            "[ 3100/5000] loss=24.4047 lr=1.14e-04 8,728 tok/s\n",
            "[ 3150/5000] loss=24.5648 lr=1.09e-04 8,713 tok/s\n",
            "[ 3200/5000] loss=24.3413 lr=1.04e-04 8,701 tok/s\n",
            "[ 3250/5000] loss=24.3597 lr=9.87e-05 8,706 tok/s\n",
            "[ 3300/5000] loss=24.1516 lr=9.38e-05 8,696 tok/s\n",
            "[ 3350/5000] loss=24.2521 lr=8.90e-05 8,702 tok/s\n",
            "[ 3400/5000] loss=24.2409 lr=8.42e-05 8,689 tok/s\n",
            "[ 3450/5000] loss=24.1215 lr=7.96e-05 8,686 tok/s\n",
            "Gradients:\n",
            "[ 3500/5000] loss=24.1403 lr=7.50e-05 8,673 tok/s\n",
            "[ 3550/5000] loss=24.1045 lr=7.05e-05 8,661 tok/s\n",
            "[ 3600/5000] loss=23.9338 lr=6.61e-05 8,668 tok/s\n",
            "[ 3650/5000] loss=24.1736 lr=6.18e-05 8,658 tok/s\n",
            "[ 3700/5000] loss=24.3945 lr=5.77e-05 8,644 tok/s\n",
            "[ 3750/5000] loss=24.0867 lr=5.36e-05 8,641 tok/s\n",
            "[ 3800/5000] loss=24.2010 lr=4.96e-05 8,630 tok/s\n",
            "[ 3850/5000] loss=24.3145 lr=4.58e-05 8,634 tok/s\n",
            "[ 3900/5000] loss=24.1274 lr=4.21e-05 8,646 tok/s\n",
            "[ 3950/5000] loss=24.0147 lr=3.85e-05 8,647 tok/s\n",
            "Gradients:\n",
            "[ 4000/5000] loss=24.5408 lr=3.51e-05 8,660 tok/s\n",
            "[ 4050/5000] loss=24.1536 lr=3.18e-05 8,655 tok/s\n",
            "[ 4100/5000] loss=24.1528 lr=2.86e-05 8,647 tok/s\n",
            "[ 4150/5000] loss=24.2756 lr=2.56e-05 8,643 tok/s\n",
            "[ 4200/5000] loss=24.0819 lr=2.28e-05 8,650 tok/s\n",
            "[ 4250/5000] loss=24.1736 lr=2.01e-05 8,640 tok/s\n",
            "[ 4300/5000] loss=24.0082 lr=1.76e-05 8,634 tok/s\n",
            "[ 4350/5000] loss=24.0300 lr=1.52e-05 8,643 tok/s\n",
            "[ 4400/5000] loss=24.0396 lr=1.30e-05 8,636 tok/s\n",
            "[ 4450/5000] loss=24.1267 lr=1.09e-05 8,638 tok/s\n",
            "Gradients:\n",
            "[ 4500/5000] loss=24.5261 lr=9.05e-06 8,631 tok/s\n",
            "[ 4550/5000] loss=24.1993 lr=7.34e-06 8,634 tok/s\n",
            "[ 4600/5000] loss=24.3853 lr=5.81e-06 8,639 tok/s\n",
            "[ 4650/5000] loss=24.1412 lr=4.46e-06 8,632 tok/s\n",
            "[ 4700/5000] loss=24.3540 lr=3.28e-06 8,633 tok/s\n",
            "[ 4750/5000] loss=24.0258 lr=2.28e-06 8,634 tok/s\n",
            "[ 4800/5000] loss=24.2462 lr=1.46e-06 8,628 tok/s\n",
            "[ 4850/5000] loss=24.1846 lr=8.22e-07 8,631 tok/s\n",
            "[ 4900/5000] loss=24.1397 lr=3.65e-07 8,632 tok/s\n",
            "[ 4950/5000] loss=24.2389 lr=9.14e-08 8,636 tok/s\n",
            "Gradients:\n",
            "  >>> NIAH@5000: 0.01x random [FAIL]\n",
            "\n",
            "============================================================\n",
            "Training complete in 19.8 minutes\n",
            "Loss: 232.6178 -> 24.3749\n",
            "NIAH trajectory: [(500, np.float64(2.1173002576548843e-14)), (1000, np.float64(1.3853888318802298e-13)), (2000, np.float64(2.204889717933467e-12)), (3000, np.float64(3.108887419832232e-07)), (5000, np.float64(0.009930688081596056))]\n"
          ]
        }
      ],
      "source": [
        "# CELL 6: TRAINING LOOP\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=TRAIN_CFG.lr, betas=TRAIN_CFG.betas, weight_decay=TRAIN_CFG.weight_decay)\n",
        "losses, niah_traj = [], []\n",
        "start = time.time()\n",
        "\n",
        "print(f\"\\nTRAINING {TRAIN_CFG.steps} steps (effective batch={TRAIN_CFG.effective_batch_size})\\n\")\n",
        "model.train()\n",
        "\n",
        "for step in range(TRAIN_CFG.steps):\n",
        "    # LR schedule: warmup then cosine decay\n",
        "    if step < TRAIN_CFG.warmup_steps:\n",
        "        lr = TRAIN_CFG.lr * (step + 1) / TRAIN_CFG.warmup_steps\n",
        "    else:\n",
        "        progress = (step - TRAIN_CFG.warmup_steps) / (TRAIN_CFG.steps - TRAIN_CFG.warmup_steps)\n",
        "        lr = TRAIN_CFG.lr * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "    for pg in opt.param_groups:\n",
        "        pg['lr'] = lr\n",
        "    \n",
        "    # Gradient accumulation\n",
        "    acc_loss = 0\n",
        "    for _ in range(TRAIN_CFG.accum_steps):\n",
        "        x, y = get_batch()\n",
        "        with torch.amp.autocast('cuda', dtype=DTYPE):\n",
        "            _, loss, _ = model(x, y)\n",
        "        (loss / TRAIN_CFG.accum_steps).backward()\n",
        "        acc_loss += loss.item()\n",
        "    \n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), TRAIN_CFG.grad_clip)\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "    losses.append(acc_loss / TRAIN_CFG.accum_steps)\n",
        "    \n",
        "    # Logging\n",
        "    if step % TRAIN_CFG.log_interval == 0:\n",
        "        avg = np.mean(losses[-50:]) if len(losses) >= 50 else np.mean(losses)\n",
        "        elapsed = time.time() - start\n",
        "        tps = (step + 1) * TRAIN_CFG.effective_batch_size * TRAIN_CFG.seq_len / elapsed\n",
        "        print(f\"[{step:5d}/{TRAIN_CFG.steps}] loss={avg:.4f} lr={lr:.2e} {tps:,.0f} tok/s\")\n",
        "    \n",
        "    if (step + 1) % TRAIN_CFG.grad_log_interval == 0:\n",
        "        print_gradient_summary(model)\n",
        "    \n",
        "    # NIAH checkpoint\n",
        "    if (step + 1) in TRAIN_CFG.niah_checkpoints:\n",
        "        n = needle_test(model, tokenizer, TRAIN_CFG.seq_len, 30, device=DEVICE)\n",
        "        niah_traj.append((step + 1, n['ratio']))\n",
        "        status = \"PASS\" if n['ratio'] > 1.0 else \"FAIL\"\n",
        "        print(f\"  >>> NIAH@{step+1}: {n['ratio']:.2f}x random [{status}]\")\n",
        "        model.train()\n",
        "\n",
        "# Summary\n",
        "elapsed = time.time() - start\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training complete in {elapsed/60:.1f} minutes\")\n",
        "print(f\"Loss: {np.mean(losses[:50]):.4f} -> {np.mean(losses[-50:]):.4f}\")\n",
        "print(f\"NIAH trajectory: {niah_traj}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7: FINAL EVALUATION\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# NIAH at multiple lengths\n",
        "for L in [128, 256, 512]:\n",
        "    n = needle_test(model, tokenizer, L, 50, device=DEVICE)\n",
        "    status = \"PASS\" if n['ratio'] > 1.0 else \"FAIL\"\n",
        "    print(f\"NIAH@{L}: {n['ratio']:.2f}x random [{status}]\")\n",
        "\n",
        "# Layer probing\n",
        "print()\n",
        "probe_layers(model, device=DEVICE)\n",
        "\n",
        "# Verdict\n",
        "lm_pass = np.mean(losses[:50]) - np.mean(losses[-50:]) > 2.0\n",
        "niah_pass = any(r > 1.0 for _, r in niah_traj)\n",
        "\n",
        "print(f\"\\nVerdict:\")\n",
        "print(f\"  LM Training: {'PASS' if lm_pass else 'MARGINAL'}\")\n",
        "print(f\"  NIAH: {'PASS' if niah_pass else 'FAIL'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 8: SAVE CHECKPOINT\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "save_dir = Path(\"./checkpoints\")\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "\n",
        "ckpt_path = save_dir / \"groundthink_v6_final.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'config': MODEL_CFG,\n",
        "    'losses': losses,\n",
        "    'niah_trajectory': niah_traj,\n",
        "}, ckpt_path)\n",
        "\n",
        "print(f\"Saved checkpoint: {ckpt_path}\")\n",
        "print(f\"Size: {ckpt_path.stat().st_size / 1e6:.1f} MB\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
