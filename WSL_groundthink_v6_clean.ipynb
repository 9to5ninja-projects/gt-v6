{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GroundThink V6 - Hybrid GatedDeltaNet + SWA (WSL Local)\n",
        "\n",
        "**Gated Delta Rule:** `Sₜ = αₜ Sₜ₋₁ + βₜ Δₜ`\n",
        "- `αₜ` (gate): rapid forgetting from Mamba2\n",
        "- `βₜΔₜ` (delta): targeted updates from DeltaNet\n",
        "\n",
        "**Architecture:** GatedDeltaNet (FLA) + SlidingWindowAttention (flash_attn)\n",
        "\n",
        "**Required Environment:**\n",
        "- PyTorch nightly (cu126)\n",
        "- flash-attn (prebuilt wheel)\n",
        "- flash-linear-attention 0.4.2+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 0: VERIFY ENVIRONMENT\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Compute Capability: {torch.cuda.get_device_capability(0)}\")\n",
        "\n",
        "# Verify flash_attn\n",
        "try:\n",
        "    from flash_attn import flash_attn_func\n",
        "    import flash_attn\n",
        "    print(f\"flash_attn: {flash_attn.__version__}\")\n",
        "    FLASH_ATTN_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    print(f\"flash_attn: NOT AVAILABLE - {e}\")\n",
        "    FLASH_ATTN_AVAILABLE = False\n",
        "\n",
        "# Verify FLA\n",
        "try:\n",
        "    from fla.layers import GatedDeltaNet\n",
        "    print(\"FLA GatedDeltaNet: OK\")\n",
        "except ImportError as e:\n",
        "    raise ImportError(f\"FLA not available: {e}\")\n",
        "\n",
        "# Enable TF32 for Ampere+\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "print(\"\\n✓ Environment ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 1: CONFIG\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Hardware detection\n",
        "USE_FLASH = False\n",
        "DTYPE = torch.float32\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    major, minor = torch.cuda.get_device_capability(0)\n",
        "    print(f\"GPU: {props.name} (Compute {major}.{minor}, {props.total_memory/1e9:.1f}GB)\")\n",
        "    \n",
        "    # FlashAttention requires Ampere+ (sm_80+)\n",
        "    if major >= 8 and FLASH_ATTN_AVAILABLE:\n",
        "        USE_FLASH = True\n",
        "        print(\"FlashAttention: ENABLED\")\n",
        "    else:\n",
        "        print(f\"FlashAttention: DISABLED (need Ampere+ and flash_attn installed)\")\n",
        "    \n",
        "    # bfloat16 for Ampere+, float16 for older\n",
        "    DTYPE = torch.bfloat16 if major >= 8 else torch.float16\n",
        "    print(f\"Training dtype: {DTYPE}\")\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    vocab_size: int = 50257\n",
        "    d_model: int = 256        # Small for RTX 4050\n",
        "    n_layers: int = 12\n",
        "    n_heads: int = 8\n",
        "    head_dim: int = 32\n",
        "    attn_interval: int = 4    # SWA every 4th layer (3:1 ratio)\n",
        "    window_size: int = 512\n",
        "    expand_k: float = 1.0\n",
        "    expand_v: float = 2.0\n",
        "    use_gradient_checkpointing: bool = True\n",
        "    tie_weights: bool = True\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "    \n",
        "    def get_swa_layer_indices(self):\n",
        "        return [i for i in range(self.n_layers) if i % self.attn_interval == (self.attn_interval - 1)]\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    dataset_name: str = \"HuggingFaceFW/fineweb-edu\"\n",
        "    dataset_subset: str = \"sample-10BT\"\n",
        "    target_tokens: int = 10_000_000  # Smaller for local\n",
        "    batch_size: int = 2\n",
        "    seq_len: int = 512\n",
        "    accum_steps: int = 2\n",
        "    steps: int = 5000\n",
        "    warmup_ratio: float = 0.1\n",
        "    lr: float = 3e-4\n",
        "    weight_decay: float = 0.1\n",
        "    grad_clip: float = 1.0\n",
        "    betas: tuple = (0.9, 0.95)\n",
        "    log_interval: int = 50\n",
        "    grad_log_interval: int = 500\n",
        "    niah_checkpoints: List[int] = field(default_factory=lambda: [500, 1000, 2000, 3000, 5000])\n",
        "    \n",
        "    @property\n",
        "    def warmup_steps(self): return int(self.steps * self.warmup_ratio)\n",
        "    @property\n",
        "    def effective_batch_size(self): return self.batch_size * self.accum_steps\n",
        "\n",
        "MODEL_CFG = ModelConfig()\n",
        "TRAIN_CFG = TrainConfig()\n",
        "print(f\"\\nConfig: d={MODEL_CFG.d_model}, layers={MODEL_CFG.n_layers}, SWA@{MODEL_CFG.get_swa_layer_indices()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2: MODEL COMPONENTS\n",
        "from transformers import AutoTokenizer\n",
        "from fla.layers import GatedDeltaNet\n",
        "\n",
        "if USE_FLASH:\n",
        "    from flash_attn import flash_attn_func\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        norm = x.float().pow(2).mean(-1, keepdim=True).add(self.eps).rsqrt()\n",
        "        return (x.float() * norm).type_as(x) * self.weight\n",
        "\n",
        "\n",
        "class SwiGLUFFN(nn.Module):\n",
        "    def __init__(self, d_model, expansion=8/3):\n",
        "        super().__init__()\n",
        "        hidden = ((int(d_model * expansion) + 63) // 64) * 64\n",
        "        self.w1 = nn.Linear(d_model, hidden, bias=False)\n",
        "        self.w3 = nn.Linear(d_model, hidden, bias=False)\n",
        "        self.w2 = nn.Linear(hidden, d_model, bias=False)\n",
        "        self.norm = RMSNorm(d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.norm(x)\n",
        "        return x + self.w2(F.silu(self.w1(h)) * self.w3(h))\n",
        "\n",
        "\n",
        "class SlidingWindowAttention(nn.Module):\n",
        "    \"\"\"SWA with KV-Cache for inference.\"\"\"\n",
        "    def __init__(self, d_model, n_heads, window_size, layer_idx=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.window_size = window_size\n",
        "        \n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "    \n",
        "    def forward(self, x, past_key_values=None, use_cache=False):\n",
        "        B, T, D = x.shape\n",
        "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
        "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
        "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
        "        \n",
        "        current_cache = None\n",
        "        if use_cache:\n",
        "            if past_key_values is not None:\n",
        "                pk, pv = past_key_values\n",
        "                k = torch.cat([pk, k], dim=1)\n",
        "                v = torch.cat([pv, v], dim=1)\n",
        "            current_cache = (k[:, -self.window_size:].detach(), v[:, -self.window_size:].detach())\n",
        "        \n",
        "        # Inference mode with cache\n",
        "        if use_cache and past_key_values is not None:\n",
        "            q_t, k_t, v_t = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "            out = F.scaled_dot_product_attention(q_t, k_t, v_t, is_causal=False)\n",
        "            out = out.transpose(1, 2)\n",
        "        elif USE_FLASH:\n",
        "            # Training with FlashAttention\n",
        "            out = flash_attn_func(q, k, v, causal=True, window_size=(self.window_size, 0))\n",
        "        else:\n",
        "            # Manual sliding window fallback\n",
        "            q_t, k_t, v_t = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "            mask = torch.ones(T, T, device=x.device, dtype=torch.bool).triu(1)\n",
        "            mask |= torch.ones(T, T, device=x.device, dtype=torch.bool).tril(-self.window_size - 1)\n",
        "            attn = (q_t @ k_t.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "            attn = attn.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "            out = (F.softmax(attn, dim=-1) @ v_t).transpose(1, 2)\n",
        "        \n",
        "        return self.out_proj(out.reshape(B, T, D)), current_cache\n",
        "\n",
        "\n",
        "class HybridBlock(nn.Module):\n",
        "    \"\"\"GatedDeltaNet or SlidingWindowAttention block.\"\"\"\n",
        "    def __init__(self, d_model, is_attention, n_heads=8, window_size=512,\n",
        "                 expand_k=1.0, expand_v=2.0, layer_idx=0):\n",
        "        super().__init__()\n",
        "        self.is_attention = is_attention\n",
        "        self.layer_idx = layer_idx\n",
        "        self.norm = RMSNorm(d_model)\n",
        "        \n",
        "        if is_attention:\n",
        "            self.layer = SlidingWindowAttention(d_model, n_heads, window_size, layer_idx)\n",
        "        else:\n",
        "            # GatedDeltaNet: Sₜ = αₜ Sₜ₋₁ + βₜ Δₜ\n",
        "            self.layer = GatedDeltaNet(\n",
        "                hidden_size=d_model,\n",
        "                expand_k=expand_k,\n",
        "                expand_v=expand_v,\n",
        "                layer_idx=layer_idx\n",
        "            )\n",
        "    \n",
        "    def forward(self, x, past_state=None, use_cache=False):\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "        new_state = None\n",
        "        \n",
        "        if self.is_attention:\n",
        "            x, new_state = self.layer(x, past_key_values=past_state, use_cache=use_cache)\n",
        "        else:\n",
        "            # GatedDeltaNet always returns (output, state) tuple\n",
        "            if use_cache:\n",
        "                x, new_state = self.layer(x, initial_state=past_state, use_cache=True, output_final_state=True)\n",
        "            else:\n",
        "                out = self.layer(x)\n",
        "                if isinstance(out, tuple):\n",
        "                    x, new_state = out\n",
        "                else:\n",
        "                    x = out\n",
        "        \n",
        "        return residual + x, new_state\n",
        "\n",
        "\n",
        "class GroundThinkLM(nn.Module):\n",
        "    \"\"\"Hybrid LM: GatedDeltaNet + SlidingWindowAttention\"\"\"\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
        "        \n",
        "        swa_indices = set(cfg.get_swa_layer_indices())\n",
        "        self._swa_indices = swa_indices\n",
        "        \n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.ffns = nn.ModuleList()\n",
        "        for i in range(cfg.n_layers):\n",
        "            self.blocks.append(HybridBlock(\n",
        "                cfg.d_model, is_attention=(i in swa_indices),\n",
        "                n_heads=cfg.n_heads, window_size=cfg.window_size,\n",
        "                expand_k=cfg.expand_k, expand_v=cfg.expand_v, layer_idx=i\n",
        "            ))\n",
        "            self.ffns.append(SwiGLUFFN(cfg.d_model))\n",
        "        \n",
        "        self.norm_f = RMSNorm(cfg.d_model)\n",
        "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
        "        if cfg.tie_weights:\n",
        "            self.lm_head.weight = self.embed.weight\n",
        "    \n",
        "    def forward(self, input_ids, targets=None, past_states=None, use_cache=False):\n",
        "        x = self.embed(input_ids)\n",
        "        new_states = [] if use_cache else None\n",
        "        \n",
        "        for i, (block, ffn) in enumerate(zip(self.blocks, self.ffns)):\n",
        "            layer_past = past_states[i] if (past_states is not None and len(past_states) > i) else None\n",
        "            \n",
        "            if self.cfg.use_gradient_checkpointing and self.training and not use_cache and i in self._swa_indices:\n",
        "                x = checkpoint(self._fwd_block, block, ffn, x, use_reentrant=False)\n",
        "            else:\n",
        "                x, layer_new_state = block(x, layer_past, use_cache)\n",
        "                x = ffn(x)\n",
        "                if use_cache:\n",
        "                    new_states.append(layer_new_state)\n",
        "        \n",
        "        logits = self.lm_head(self.norm_f(x))\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None\n",
        "        return logits, loss, new_states\n",
        "    \n",
        "    @staticmethod\n",
        "    def _fwd_block(block, ffn, x):\n",
        "        x, _ = block(x, None, False)\n",
        "        return ffn(x)\n",
        "    \n",
        "    def get_layer_types(self):\n",
        "        return ['SWA' if i in self._swa_indices else 'GDN' for i in range(self.cfg.n_layers)]\n",
        "    \n",
        "    def count_parameters(self):\n",
        "        c = {'embed': sum(p.numel() for p in self.embed.parameters()), 'gdn': 0, 'swa': 0, 'ffn': 0}\n",
        "        for i, (b, f) in enumerate(zip(self.blocks, self.ffns)):\n",
        "            bp = sum(p.numel() for p in b.parameters())\n",
        "            fp = sum(p.numel() for p in f.parameters())\n",
        "            c['swa' if i in self._swa_indices else 'gdn'] += bp\n",
        "            c['ffn'] += fp\n",
        "        c['total'] = sum(c.values())\n",
        "        return c\n",
        "\n",
        "print(\"Model components defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3: MONITORING\n",
        "\n",
        "def print_gradient_summary(model):\n",
        "    agg = {'embed': [], 'gdn': [], 'swa': [], 'ffn': []}\n",
        "    for name, p in model.named_parameters():\n",
        "        if p.grad is None:\n",
        "            continue\n",
        "        n = p.grad.norm().item()\n",
        "        if 'embed' in name:\n",
        "            agg['embed'].append(n)\n",
        "        elif 'ffn' in name:\n",
        "            agg['ffn'].append(n)\n",
        "        elif 'blocks' in name:\n",
        "            idx = int(name.split('.')[1])\n",
        "            agg['swa' if idx in model._swa_indices else 'gdn'].append(n)\n",
        "    print(\"Gradients:\")\n",
        "    for k, v in agg.items():\n",
        "        if v:\n",
        "            print(f\"  {k}: mean={np.mean(v):.3f} max={np.max(v):.2f}\")\n",
        "\n",
        "\n",
        "def needle_test(model, tokenizer, seq_len=512, n_trials=50, needle_token=50250, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    probs = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_trials):\n",
        "            tokens = torch.randint(1000, 10000, (1, seq_len), device=device)\n",
        "            pos = torch.randint(64, seq_len - 64, (1,)).item()\n",
        "            tokens[0, pos] = needle_token\n",
        "            with torch.amp.autocast('cuda', dtype=DTYPE):\n",
        "                logits, _, _ = model(tokens)\n",
        "            probs.append(F.softmax(logits[0, -1].float(), dim=-1)[needle_token].item())\n",
        "    rc = 1.0 / tokenizer.vocab_size\n",
        "    return {'mean': np.mean(probs), 'ratio': np.mean(probs) / rc}\n",
        "\n",
        "\n",
        "def probe_layers(model, needle_id=50250, seq_len=512, pos=256, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    tokens = torch.randint(1000, 10000, (1, seq_len), device=device)\n",
        "    tokens[0, pos] = needle_id\n",
        "    with torch.no_grad():\n",
        "        x = model.embed(tokens)\n",
        "        emb = model.embed.weight[needle_id].float()\n",
        "        print(\"Needle representation through layers:\")\n",
        "        for i, (b, f) in enumerate(zip(model.blocks, model.ffns)):\n",
        "            x, _ = b(x, None, False)\n",
        "            x = f(x)\n",
        "            sim = F.cosine_similarity(x[0, pos].float(), emb, dim=0).item()\n",
        "            ltype = 'SWA' if i in model._swa_indices else 'GDN'\n",
        "            print(f\"  L{i:2d}[{ltype}]: {sim:+.3f}\")\n",
        "\n",
        "print(\"Monitoring functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4: DATA LOADING\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "MODEL_CFG.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "print(f\"Streaming {TRAIN_CFG.dataset_name}...\")\n",
        "ds = load_dataset(TRAIN_CFG.dataset_name, name=TRAIN_CFG.dataset_subset, split=\"train\", streaming=True)\n",
        "\n",
        "buf = []\n",
        "pbar = tqdm(total=TRAIN_CFG.target_tokens, unit=\"tok\", desc=\"Tokenizing\")\n",
        "for ex in ds:\n",
        "    toks = tokenizer.encode(ex['text']) + [tokenizer.eos_token_id]\n",
        "    buf.extend(toks)\n",
        "    pbar.update(len(toks))\n",
        "    if len(buf) >= TRAIN_CFG.target_tokens:\n",
        "        break\n",
        "pbar.close()\n",
        "\n",
        "all_tokens = torch.tensor(buf[:TRAIN_CFG.target_tokens], dtype=torch.long)\n",
        "del buf, ds\n",
        "print(f\"Loaded {len(all_tokens):,} tokens\")\n",
        "\n",
        "def get_batch():\n",
        "    ix = torch.randint(len(all_tokens) - TRAIN_CFG.seq_len - 1, (TRAIN_CFG.batch_size,))\n",
        "    x = torch.stack([all_tokens[i:i+TRAIN_CFG.seq_len] for i in ix])\n",
        "    y = torch.stack([all_tokens[i+1:i+TRAIN_CFG.seq_len+1] for i in ix])\n",
        "    return x.to(DEVICE), y.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5: BUILD MODEL\n",
        "print(\"Building model...\")\n",
        "model = GroundThinkLM(MODEL_CFG).to(DEVICE).to(DTYPE)\n",
        "\n",
        "p = model.count_parameters()\n",
        "print(f\"Parameters: {p['total']/1e6:.2f}M\")\n",
        "print(f\"  GDN: {p['gdn']/1e6:.2f}M, SWA: {p['swa']/1e6:.2f}M, FFN: {p['ffn']/1e6:.2f}M\")\n",
        "print(f\"Layers: {model.get_layer_types()}\")\n",
        "\n",
        "# Test forward/backward\n",
        "print(\"\\nTesting forward/backward...\")\n",
        "x, y = get_batch()\n",
        "with torch.amp.autocast('cuda', dtype=DTYPE):\n",
        "    _, loss, _ = model(x, y)\n",
        "loss.backward()\n",
        "print(f\"Forward OK: loss={loss.item():.4f}\")\n",
        "print(f\"Peak memory: {torch.cuda.max_memory_allocated()/1e9:.2f}GB\")\n",
        "model.zero_grad()\n",
        "torch.cuda.reset_peak_memory_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6: TRAINING LOOP\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=TRAIN_CFG.lr, betas=TRAIN_CFG.betas, weight_decay=TRAIN_CFG.weight_decay)\n",
        "losses, niah_traj = [], []\n",
        "start = time.time()\n",
        "\n",
        "print(f\"\\nTRAINING {TRAIN_CFG.steps} steps (effective batch={TRAIN_CFG.effective_batch_size})\\n\")\n",
        "model.train()\n",
        "\n",
        "for step in range(TRAIN_CFG.steps):\n",
        "    # LR schedule: warmup then cosine decay\n",
        "    if step < TRAIN_CFG.warmup_steps:\n",
        "        lr = TRAIN_CFG.lr * (step + 1) / TRAIN_CFG.warmup_steps\n",
        "    else:\n",
        "        progress = (step - TRAIN_CFG.warmup_steps) / (TRAIN_CFG.steps - TRAIN_CFG.warmup_steps)\n",
        "        lr = TRAIN_CFG.lr * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "    for pg in opt.param_groups:\n",
        "        pg['lr'] = lr\n",
        "    \n",
        "    # Gradient accumulation\n",
        "    acc_loss = 0\n",
        "    for _ in range(TRAIN_CFG.accum_steps):\n",
        "        x, y = get_batch()\n",
        "        with torch.amp.autocast('cuda', dtype=DTYPE):\n",
        "            _, loss, _ = model(x, y)\n",
        "        (loss / TRAIN_CFG.accum_steps).backward()\n",
        "        acc_loss += loss.item()\n",
        "    \n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), TRAIN_CFG.grad_clip)\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "    losses.append(acc_loss / TRAIN_CFG.accum_steps)\n",
        "    \n",
        "    # Logging\n",
        "    if step % TRAIN_CFG.log_interval == 0:\n",
        "        avg = np.mean(losses[-50:]) if len(losses) >= 50 else np.mean(losses)\n",
        "        elapsed = time.time() - start\n",
        "        tps = (step + 1) * TRAIN_CFG.effective_batch_size * TRAIN_CFG.seq_len / elapsed\n",
        "        print(f\"[{step:5d}/{TRAIN_CFG.steps}] loss={avg:.4f} lr={lr:.2e} {tps:,.0f} tok/s\")\n",
        "    \n",
        "    if (step + 1) % TRAIN_CFG.grad_log_interval == 0:\n",
        "        print_gradient_summary(model)\n",
        "    \n",
        "    # NIAH checkpoint\n",
        "    if (step + 1) in TRAIN_CFG.niah_checkpoints:\n",
        "        n = needle_test(model, tokenizer, TRAIN_CFG.seq_len, 30, device=DEVICE)\n",
        "        niah_traj.append((step + 1, n['ratio']))\n",
        "        status = \"PASS\" if n['ratio'] > 1.0 else \"FAIL\"\n",
        "        print(f\"  >>> NIAH@{step+1}: {n['ratio']:.2f}x random [{status}]\")\n",
        "        model.train()\n",
        "\n",
        "# Summary\n",
        "elapsed = time.time() - start\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training complete in {elapsed/60:.1f} minutes\")\n",
        "print(f\"Loss: {np.mean(losses[:50]):.4f} -> {np.mean(losses[-50:]):.4f}\")\n",
        "print(f\"NIAH trajectory: {niah_traj}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7: FINAL EVALUATION\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# NIAH at multiple lengths\n",
        "for L in [128, 256, 512]:\n",
        "    n = needle_test(model, tokenizer, L, 50, device=DEVICE)\n",
        "    status = \"PASS\" if n['ratio'] > 1.0 else \"FAIL\"\n",
        "    print(f\"NIAH@{L}: {n['ratio']:.2f}x random [{status}]\")\n",
        "\n",
        "# Layer probing\n",
        "print()\n",
        "probe_layers(model, device=DEVICE)\n",
        "\n",
        "# Verdict\n",
        "lm_pass = np.mean(losses[:50]) - np.mean(losses[-50:]) > 2.0\n",
        "niah_pass = any(r > 1.0 for _, r in niah_traj)\n",
        "\n",
        "print(f\"\\nVerdict:\")\n",
        "print(f\"  LM Training: {'PASS' if lm_pass else 'MARGINAL'}\")\n",
        "print(f\"  NIAH: {'PASS' if niah_pass else 'FAIL'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 8: SAVE CHECKPOINT\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "save_dir = Path(\"./checkpoints\")\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "\n",
        "ckpt_path = save_dir / \"groundthink_v6_final.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'config': MODEL_CFG,\n",
        "    'losses': losses,\n",
        "    'niah_trajectory': niah_traj,\n",
        "}, ckpt_path)\n",
        "\n",
        "print(f\"Saved checkpoint: {ckpt_path}\")\n",
        "print(f\"Size: {ckpt_path.stat().st_size / 1e6:.1f} MB\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
