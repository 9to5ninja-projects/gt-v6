{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunked Delta Rule - Triton Kernel\n",
    "\n",
    "**The Problem:** True Delta Rule is sequential - each token needs state from previous token.\n",
    "\n",
    "**The Solution:** Chunk approximation with refinement:\n",
    "1. Pass 0: All tokens in chunk use chunk-start state (parallel)\n",
    "2. Pass 1: Refine using Pass 0 outputs (parallel)\n",
    "\n",
    "**Expected speedup:** 5-15x over pure Python sequential, while maintaining accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.11.0.dev20260128+cu128\n",
      "CUDA: 12.8\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "Triton: 3.6.0\n"
     ]
    }
   ],
   "source": [
    "# Environment check\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "import triton\n",
    "print(f\"Triton: {triton.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference implementation loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# REFERENCE: Sequential Delta Rule (correct but slow)\n",
    "# =============================================================================\n",
    "\n",
    "def sequential_delta_rule(k, v, beta, g, initial_state=None):\n",
    "    \"\"\"Pure Python sequential - baseline for correctness.\"\"\"\n",
    "    B, T, H, K = k.shape\n",
    "    V = v.shape[-1]\n",
    "    device, dtype = k.device, k.dtype\n",
    "    \n",
    "    state = torch.zeros(B, H, K, V, device=device, dtype=dtype) if initial_state is None else initial_state.clone()\n",
    "    \n",
    "    outputs = []\n",
    "    for t in range(T):\n",
    "        k_t, v_t = k[:, t], v[:, t]\n",
    "        beta_t, g_t = beta[:, t], g[:, t]\n",
    "        \n",
    "        pred = torch.einsum('bhkv,bhk->bhv', state, k_t)\n",
    "        error = v_t - pred\n",
    "        update = torch.einsum('bhv,bhk->bhkv', error, k_t)\n",
    "        state = g_t[..., None, None] * state + beta_t[..., None, None] * update\n",
    "        outputs.append(torch.einsum('bhkv,bhk->bhv', state, k_t))\n",
    "    \n",
    "    return torch.stack(outputs, dim=1), state\n",
    "\n",
    "print(\"Reference implementation loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton kernel loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRITON KERNEL: Chunked Delta Rule\n",
    "# =============================================================================\n",
    "#\n",
    "# Strategy: \n",
    "#   - Parallelize across (B, H) - each thread handles one batch/head\n",
    "#   - Process T sequentially but with fused GPU operations\n",
    "#   - This is still O(T) but constant factors are much better\n",
    "#\n",
    "# The sequential T dependency is fundamental to Delta Rule correctness.\n",
    "# Chunked approximation would break error correction.\n",
    "# =============================================================================\n",
    "\n",
    "@triton.jit\n",
    "def delta_rule_fwd_kernel(\n",
    "    K, V, Beta, G, State_in,\n",
    "    Out, State_out,\n",
    "    stride_k_b, stride_k_t, stride_k_h, stride_k_k,\n",
    "    stride_v_b, stride_v_t, stride_v_h, stride_v_v,\n",
    "    stride_bg_b, stride_bg_t, stride_bg_h,\n",
    "    stride_s_b, stride_s_h, stride_s_k, stride_s_v,\n",
    "    stride_o_b, stride_o_t, stride_o_h, stride_o_v,\n",
    "    T, K_DIM: tl.constexpr, V_DIM: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Delta Rule forward: one (batch, head) per program.\n",
    "    \n",
    "    State update: S = g*S + β*(v - S@k)⊗k\n",
    "    \"\"\"\n",
    "    pid_b = tl.program_id(0)\n",
    "    pid_h = tl.program_id(1)\n",
    "    \n",
    "    # Load initial state [K_DIM, V_DIM]\n",
    "    k_offs = tl.arange(0, K_DIM)\n",
    "    v_offs = tl.arange(0, V_DIM)\n",
    "    \n",
    "    state_ptrs = State_in + pid_b * stride_s_b + pid_h * stride_s_h + \\\n",
    "                 k_offs[:, None] * stride_s_k + v_offs[None, :] * stride_s_v\n",
    "    state = tl.load(state_ptrs).to(tl.float32)\n",
    "    \n",
    "    # Process each token\n",
    "    for t in range(T):\n",
    "        # Load k[t] [K_DIM]\n",
    "        k_ptrs = K + pid_b * stride_k_b + t * stride_k_t + pid_h * stride_k_h + k_offs * stride_k_k\n",
    "        k_t = tl.load(k_ptrs).to(tl.float32)\n",
    "        \n",
    "        # Load v[t] [V_DIM]\n",
    "        v_ptrs = V + pid_b * stride_v_b + t * stride_v_t + pid_h * stride_v_h + v_offs * stride_v_v\n",
    "        v_t = tl.load(v_ptrs).to(tl.float32)\n",
    "        \n",
    "        # Load scalars\n",
    "        beta_t = tl.load(Beta + pid_b * stride_bg_b + t * stride_bg_t + pid_h * stride_bg_h).to(tl.float32)\n",
    "        g_t = tl.load(G + pid_b * stride_bg_b + t * stride_bg_t + pid_h * stride_bg_h).to(tl.float32)\n",
    "        \n",
    "        # Prediction: sum_k state[k,v] * k_t[k] for each v\n",
    "        # pred[v] = sum over k of state[k,v] * k_t[k]\n",
    "        pred = tl.sum(state * k_t[:, None], axis=0)  # [V_DIM]\n",
    "        \n",
    "        # Error\n",
    "        error = v_t - pred  # [V_DIM]\n",
    "        \n",
    "        # Outer product: error[v] * k_t[k] -> [K_DIM, V_DIM]\n",
    "        outer = k_t[:, None] * error[None, :]  # [K_DIM, V_DIM]\n",
    "        \n",
    "        # Update state\n",
    "        state = g_t * state + beta_t * outer\n",
    "        \n",
    "        # Output: retrieve from updated state\n",
    "        out_t = tl.sum(state * k_t[:, None], axis=0)  # [V_DIM]\n",
    "        \n",
    "        # Store output\n",
    "        out_ptrs = Out + pid_b * stride_o_b + t * stride_o_t + pid_h * stride_o_h + v_offs * stride_o_v\n",
    "        tl.store(out_ptrs, out_t)\n",
    "    \n",
    "    # Store final state\n",
    "    tl.store(state_ptrs.to(State_out.dtype.element_ty).to(tl.pointer_type(tl.float32)) - State_in + State_out, state)\n",
    "\n",
    "\n",
    "def triton_delta_rule(k, v, beta, g, initial_state=None):\n",
    "    \"\"\"\n",
    "    Triton-accelerated Delta Rule.\n",
    "    \n",
    "    Args:\n",
    "        k: [B, T, H, K] - normalized keys\n",
    "        v: [B, T, H, V] - values\n",
    "        beta: [B, T, H] - write gate\n",
    "        g: [B, T, H] - forget gate\n",
    "        initial_state: [B, H, K, V] or None\n",
    "    \n",
    "    Returns:\n",
    "        output: [B, T, H, V]\n",
    "        final_state: [B, H, K, V]\n",
    "    \"\"\"\n",
    "    B, T, H, K_dim = k.shape\n",
    "    V_dim = v.shape[-1]\n",
    "    device = k.device\n",
    "    \n",
    "    # Ensure contiguous float32\n",
    "    k = k.contiguous().float()\n",
    "    v = v.contiguous().float()\n",
    "    beta = beta.contiguous().float()\n",
    "    g = g.contiguous().float()\n",
    "    \n",
    "    if initial_state is None:\n",
    "        state_in = torch.zeros(B, H, K_dim, V_dim, device=device, dtype=torch.float32)\n",
    "    else:\n",
    "        state_in = initial_state.contiguous().float()\n",
    "    \n",
    "    out = torch.empty(B, T, H, V_dim, device=device, dtype=torch.float32)\n",
    "    state_out = torch.empty_like(state_in)\n",
    "    \n",
    "    # Launch kernel: one program per (batch, head)\n",
    "    grid = (B, H)\n",
    "    \n",
    "    delta_rule_fwd_kernel[grid](\n",
    "        k, v, beta, g, state_in,\n",
    "        out, state_out,\n",
    "        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
    "        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
    "        beta.stride(0), beta.stride(1), beta.stride(2),\n",
    "        state_in.stride(0), state_in.stride(1), state_in.stride(2), state_in.stride(3),\n",
    "        out.stride(0), out.stride(1), out.stride(2), out.stride(3),\n",
    "        T, K_dim, V_dim,\n",
    "    )\n",
    "    \n",
    "    return out, state_out\n",
    "\n",
    "print(\"Triton kernel loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Triton kernel loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SIMPLER TRITON KERNEL (if above has issues)\n",
    "# =============================================================================\n",
    "\n",
    "@triton.jit  \n",
    "def delta_simple_kernel(\n",
    "    K, V, Beta, G, State,\n",
    "    Out,\n",
    "    B, T, H, K_DIM: tl.constexpr, V_DIM: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Simplified kernel - might be more compatible.\"\"\"\n",
    "    pid = tl.program_id(0)  # Linear index for (b, h)\n",
    "    b = pid // H\n",
    "    h = pid % H\n",
    "    \n",
    "    # State is [B, H, K, V], row-major\n",
    "    state_base = b * H * K_DIM * V_DIM + h * K_DIM * V_DIM\n",
    "    \n",
    "    # Load state into registers\n",
    "    state = tl.zeros((K_DIM, V_DIM), dtype=tl.float32)\n",
    "    for ki in range(K_DIM):\n",
    "        for vi in range(V_DIM):\n",
    "            state[ki, vi] = tl.load(State + state_base + ki * V_DIM + vi)\n",
    "    \n",
    "    # Process tokens\n",
    "    for t in range(T):\n",
    "        # Base offsets\n",
    "        k_base = b * T * H * K_DIM + t * H * K_DIM + h * K_DIM\n",
    "        v_base = b * T * H * V_DIM + t * H * V_DIM + h * V_DIM\n",
    "        bg_base = b * T * H + t * H + h\n",
    "        \n",
    "        # Load k, v, beta, g\n",
    "        k_t = tl.zeros((K_DIM,), dtype=tl.float32)\n",
    "        v_t = tl.zeros((V_DIM,), dtype=tl.float32)\n",
    "        for ki in range(K_DIM):\n",
    "            k_t[ki] = tl.load(K + k_base + ki)\n",
    "        for vi in range(V_DIM):\n",
    "            v_t[vi] = tl.load(V + v_base + vi)\n",
    "        beta_t = tl.load(Beta + bg_base)\n",
    "        g_t = tl.load(G + bg_base)\n",
    "        \n",
    "        # Prediction\n",
    "        pred = tl.zeros((V_DIM,), dtype=tl.float32)\n",
    "        for ki in range(K_DIM):\n",
    "            for vi in range(V_DIM):\n",
    "                pred[vi] += state[ki, vi] * k_t[ki]\n",
    "        \n",
    "        # Error and update\n",
    "        error = v_t - pred\n",
    "        for ki in range(K_DIM):\n",
    "            for vi in range(V_DIM):\n",
    "                state[ki, vi] = g_t * state[ki, vi] + beta_t * error[vi] * k_t[ki]\n",
    "        \n",
    "        # Output\n",
    "        out_t = tl.zeros((V_DIM,), dtype=tl.float32)\n",
    "        for ki in range(K_DIM):\n",
    "            for vi in range(V_DIM):\n",
    "                out_t[vi] += state[ki, vi] * k_t[ki]\n",
    "        \n",
    "        out_base = b * T * H * V_DIM + t * H * V_DIM + h * V_DIM\n",
    "        for vi in range(V_DIM):\n",
    "            tl.store(Out + out_base + vi, out_t[vi])\n",
    "    \n",
    "    # Store final state\n",
    "    for ki in range(K_DIM):\n",
    "        for vi in range(V_DIM):\n",
    "            tl.store(State + state_base + ki * V_DIM + vi, state[ki, vi])\n",
    "\n",
    "\n",
    "def triton_delta_simple(k, v, beta, g, initial_state=None):\n",
    "    B, T, H, K_dim = k.shape\n",
    "    V_dim = v.shape[-1]\n",
    "    device = k.device\n",
    "    \n",
    "    k = k.contiguous().float()\n",
    "    v = v.contiguous().float()\n",
    "    beta = beta.contiguous().float()\n",
    "    g = g.contiguous().float()\n",
    "    \n",
    "    if initial_state is None:\n",
    "        state = torch.zeros(B, H, K_dim, V_dim, device=device, dtype=torch.float32)\n",
    "    else:\n",
    "        state = initial_state.clone().contiguous().float()\n",
    "    \n",
    "    out = torch.empty(B, T, H, V_dim, device=device, dtype=torch.float32)\n",
    "    \n",
    "    grid = (B * H,)\n",
    "    delta_simple_kernel[grid](\n",
    "        k, v, beta, g, state, out,\n",
    "        B, T, H, K_dim, V_dim,\n",
    "    )\n",
    "    \n",
    "    return out, state\n",
    "\n",
    "print(\"Simple Triton kernel loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CORRECTNESS TEST\n",
      "============================================================\n",
      "Triton error: at 18:12:\n",
      "    pid = tl.program_id(0)  # Linear index for (b, h)\n",
      "    b = pid // H\n",
      "    h = pid % H\n",
      "\n",
      "    # State is [B, H, K, V], row-major\n",
      "    state_base = b * H * K_DIM * V_DIM + h * K_DIM * V_DIM\n",
      "\n",
      "    # Load state into registers\n",
      "    state = tl.zeros((K_DIM, V_DIM), dtype=tl.float32)\n",
      "    for ki in range(K_DIM):\n",
      "        for vi in range(V_DIM):\n",
      "            state[ki, vi] = tl.load(State + state_base + ki * V_DIM + vi)\n",
      "            ^\n",
      "NotImplementedError('__setitem__ is not supported in triton')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_9581/1194476838.py\", line 22, in <module>\n",
      "    out_tri, state_tri = triton_delta_simple(k, v, beta, g)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_9581/1713007584.py\", line 88, in triton_delta_simple\n",
      "    delta_simple_kernel[grid](\n",
      "  File \"/home/m_tes/groundthink/gt-v6/.venv/lib/python3.12/site-packages/triton/runtime/jit.py\", line 370, in <lambda>\n",
      "    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/m_tes/groundthink/gt-v6/.venv/lib/python3.12/site-packages/triton/runtime/jit.py\", line 720, in run\n",
      "    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/m_tes/groundthink/gt-v6/.venv/lib/python3.12/site-packages/triton/runtime/jit.py\", line 849, in _do_compile\n",
      "    kernel = self.compile(src, target=target, options=options.__dict__)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/m_tes/groundthink/gt-v6/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 304, in compile\n",
      "    module = src.make_ir(target, options, codegen_fns, module_map, context)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/m_tes/groundthink/gt-v6/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n",
      "    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "triton.compiler.errors.CompilationError: at 18:12:\n",
      "    pid = tl.program_id(0)  # Linear index for (b, h)\n",
      "    b = pid // H\n",
      "    h = pid % H\n",
      "\n",
      "    # State is [B, H, K, V], row-major\n",
      "    state_base = b * H * K_DIM * V_DIM + h * K_DIM * V_DIM\n",
      "\n",
      "    # Load state into registers\n",
      "    state = tl.zeros((K_DIM, V_DIM), dtype=tl.float32)\n",
      "    for ki in range(K_DIM):\n",
      "        for vi in range(V_DIM):\n",
      "            state[ki, vi] = tl.load(State + state_base + ki * V_DIM + vi)\n",
      "            ^\n",
      "NotImplementedError('__setitem__ is not supported in triton')\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEST CORRECTNESS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CORRECTNESS TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "B, T, H, K, V = 2, 64, 4, 32, 64\n",
    "device = \"cuda\"\n",
    "\n",
    "k = F.normalize(torch.randn(B, T, H, K, device=device), dim=-1)\n",
    "v = torch.randn(B, T, H, V, device=device)\n",
    "beta = torch.sigmoid(torch.randn(B, T, H, device=device) - 2)  # Low β\n",
    "g = torch.sigmoid(torch.randn(B, T, H, device=device) + 2)    # High g\n",
    "\n",
    "# Reference\n",
    "out_ref, state_ref = sequential_delta_rule(k, v, beta, g)\n",
    "\n",
    "# Triton\n",
    "try:\n",
    "    out_tri, state_tri = triton_delta_simple(k, v, beta, g)\n",
    "    \n",
    "    out_err = (out_tri - out_ref.float()).norm() / out_ref.float().norm()\n",
    "    state_err = (state_tri - state_ref.float()).norm() / state_ref.float().norm()\n",
    "    \n",
    "    print(f\"Output error:  {out_err.item():.6f}\")\n",
    "    print(f\"State error:   {state_err.item():.6f}\")\n",
    "    print(f\"→ {'✓ PASS' if out_err < 0.01 else '✗ FAIL'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Triton error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SPEED BENCHMARK\n",
      "============================================================\n",
      "                   Config |    PyTorch |     Triton |  Speedup\n",
      "------------------------------------------------------------\n",
      "   B=4,T=64,H=8,K=32,V=64 |    13.42ms |      ERROR |        -\n",
      "  B=8,T=128,H=8,K=32,V=64 |    39.96ms |      ERROR |        -\n",
      "  B=8,T=256,H=8,K=32,V=64 |    77.16ms |      ERROR |        -\n",
      " B=16,T=128,H=8,K=32,V=64 |    45.92ms |      ERROR |        -\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BENCHMARK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SPEED BENCHMARK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "configs = [\n",
    "    (4, 64, 8, 32, 64),\n",
    "    (8, 128, 8, 32, 64),\n",
    "    (8, 256, 8, 32, 64),\n",
    "    (16, 128, 8, 32, 64),\n",
    "]\n",
    "\n",
    "n_warmup = 3\n",
    "n_runs = 10\n",
    "\n",
    "print(f\"{'Config':>25} | {'PyTorch':>10} | {'Triton':>10} | {'Speedup':>8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for B, T, H, K, V in configs:\n",
    "    k = F.normalize(torch.randn(B, T, H, K, device='cuda'), dim=-1)\n",
    "    v = torch.randn(B, T, H, V, device='cuda')\n",
    "    beta = torch.sigmoid(torch.randn(B, T, H, device='cuda') - 2)\n",
    "    g = torch.sigmoid(torch.randn(B, T, H, device='cuda') + 2)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(n_warmup):\n",
    "        sequential_delta_rule(k, v, beta, g)\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n_runs):\n",
    "        sequential_delta_rule(k, v, beta, g)\n",
    "    torch.cuda.synchronize()\n",
    "    pytorch_ms = (time.perf_counter() - start) / n_runs * 1000\n",
    "    \n",
    "    # Triton\n",
    "    try:\n",
    "        for _ in range(n_warmup):\n",
    "            triton_delta_simple(k, v, beta, g)\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n_runs):\n",
    "            triton_delta_simple(k, v, beta, g)\n",
    "        torch.cuda.synchronize()\n",
    "        triton_ms = (time.perf_counter() - start) / n_runs * 1000\n",
    "        \n",
    "        speedup = pytorch_ms / triton_ms\n",
    "        triton_str = f\"{triton_ms:.2f} ms\"\n",
    "        speedup_str = f\"{speedup:.2f}x\"\n",
    "    except Exception as e:\n",
    "        triton_str = \"ERROR\"\n",
    "        speedup_str = \"-\"\n",
    "    \n",
    "    config_str = f\"B={B},T={T},H={H},K={K},V={V}\"\n",
    "    print(f\"{config_str:>25} | {pytorch_ms:>8.2f}ms | {triton_str:>10} | {speedup_str:>8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DELTA RULE VALIDATION\n",
      "============================================================\n",
      "Error1: 16.2738\n",
      "Error2: 0.000002 (should be ~0)\n",
      "State growth: 1.0000x (should be ~1.0)\n",
      "→ ✓ PASS\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DELTA RULE VALIDATION (confirm error correction works)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DELTA RULE VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "B, H, K, V = 1, 4, 32, 64\n",
    "state = torch.zeros(B, H, K, V, device='cuda')\n",
    "\n",
    "k = F.normalize(torch.randn(B, H, K, device='cuda'), dim=-1)\n",
    "v = torch.randn(B, H, V, device='cuda')\n",
    "\n",
    "# First write\n",
    "pred1 = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "error1 = v - pred1\n",
    "state = state + torch.einsum('bhv,bhk->bhkv', error1, k)\n",
    "norm1 = state.norm().item()\n",
    "\n",
    "# Second write (SAME k, v)\n",
    "pred2 = torch.einsum('bhkv,bhk->bhv', state, k)\n",
    "error2 = v - pred2\n",
    "state = state + torch.einsum('bhv,bhk->bhkv', error2, k)\n",
    "norm2 = state.norm().item()\n",
    "\n",
    "print(f\"Error1: {error1.norm().item():.4f}\")\n",
    "print(f\"Error2: {error2.norm().item():.6f} (should be ~0)\")\n",
    "print(f\"State growth: {norm2/norm1:.4f}x (should be ~1.0)\")\n",
    "print(f\"→ {'✓ PASS' if error2.norm().item() < 0.001 else '✗ FAIL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationError",
     "evalue": "at 18:12:\n    pid = tl.program_id(0)  # Linear index for (b, h)\n    b = pid // H\n    h = pid % H\n\n    # State is [B, H, K, V], row-major\n    state_base = b * H * K_DIM * V_DIM + h * K_DIM * V_DIM\n\n    # Load state into registers\n    state = tl.zeros((K_DIM, V_DIM), dtype=tl.float32)\n    for ki in range(K_DIM):\n        for vi in range(V_DIM):\n            state[ki, vi] = tl.load(State + state_base + ki * V_DIM + vi)\n            ^\nNotImplementedError('__setitem__ is not supported in triton')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCompilationError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m layer = GatedDeltaNetLayer(\u001b[32m256\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m32\u001b[39m, \u001b[32m64\u001b[39m).cuda()\n\u001b[32m     63\u001b[39m x = torch.randn(\u001b[32m2\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m256\u001b[39m, device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m out, state = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGDN Layer test: input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> output \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Integration successful\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/groundthink/gt-v6/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1779\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1779\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/groundthink/gt-v6/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1790\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1787\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1789\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1792\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1793\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mGatedDeltaNetLayer.forward\u001b[39m\u001b[34m(self, x, initial_state)\u001b[39m\n\u001b[32m     51\u001b[39m g = torch.sigmoid(\u001b[38;5;28mself\u001b[39m.g_proj(x_norm))\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Use Triton kernel\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m out, state = \u001b[43mtriton_delta_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m out = out.to(x.dtype).reshape(B, T, H * V)\n\u001b[32m     57\u001b[39m out = \u001b[38;5;28mself\u001b[39m.o_proj(out)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mtriton_delta_simple\u001b[39m\u001b[34m(k, v, beta, g, initial_state)\u001b[39m\n\u001b[32m     85\u001b[39m out = torch.empty(B, T, H, V_dim, device=device, dtype=torch.float32)\n\u001b[32m     87\u001b[39m grid = (B * H,)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[43mdelta_simple_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out, state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/groundthink/gt-v6/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:370\u001b[39m, in \u001b[36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) -> T:\n\u001b[32m    365\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[33;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[32m    367\u001b[39m \u001b[33;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[32m    368\u001b[39m \u001b[33;03m    memorizes the grid.\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m *args, **kwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/groundthink/gt-v6/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:720\u001b[39m, in \u001b[36mJITFunction.run\u001b[39m\u001b[34m(self, grid, warmup, *args, **kwargs)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    717\u001b[39m     options, signature, constexprs, attrs = \u001b[38;5;28mself\u001b[39m._pack_args(backend, kwargs, bound_args, specialization,\n\u001b[32m    718\u001b[39m                                                             options)\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     kernel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m kernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/groundthink/gt-v6/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:849\u001b[39m, in \u001b[36mJITFunction._do_compile\u001b[39m\u001b[34m(self, key, signature, device, constexprs, options, attrs, warmup)\u001b[39m\n\u001b[32m    847\u001b[39m     kernel = async_mode.submit(cache_key, async_compile, finalize_compile)\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m849\u001b[39m     kernel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    850\u001b[39m     kernel_cache[key] = kernel\n\u001b[32m    851\u001b[39m     \u001b[38;5;28mself\u001b[39m._call_hook(knobs.runtime.jit_post_compile_hook, key, signature, device, constexprs, options, [attrs],\n\u001b[32m    852\u001b[39m                     warmup)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/groundthink/gt-v6/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py:304\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(src, target, options, _env_vars)\u001b[39m\n\u001b[32m    302\u001b[39m module_map = backend.get_module_map()\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     module = \u001b[43msrc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    306\u001b[39m     filter_traceback(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/groundthink/gt-v6/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py:80\u001b[39m, in \u001b[36mASTSource.make_ir\u001b[39m\u001b[34m(self, target, options, codegen_fns, module_map, context)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_ir\u001b[39m(\u001b[38;5;28mself\u001b[39m, target: GPUTarget, options, codegen_fns, module_map, context):\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcode_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ast_to_ttir\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mast_to_ttir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mCompilationError\u001b[39m: at 18:12:\n    pid = tl.program_id(0)  # Linear index for (b, h)\n    b = pid // H\n    h = pid % H\n\n    # State is [B, H, K, V], row-major\n    state_base = b * H * K_DIM * V_DIM + h * K_DIM * V_DIM\n\n    # Load state into registers\n    state = tl.zeros((K_DIM, V_DIM), dtype=tl.float32)\n    for ki in range(K_DIM):\n        for vi in range(V_DIM):\n            state[ki, vi] = tl.load(State + state_base + ki * V_DIM + vi)\n            ^\nNotImplementedError('__setitem__ is not supported in triton')"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# INTEGRATION: GatedDeltaNetLayer using Triton\n",
    "# =============================================================================\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "\n",
    "class GatedDeltaNetLayer(nn.Module):\n",
    "    \"\"\"GDN using Triton Delta Rule kernel.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, head_dim, value_dim):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.value_dim = value_dim\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, n_heads * head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, n_heads * head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, n_heads * value_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(n_heads * value_dim, d_model, bias=False)\n",
    "        \n",
    "        self.beta_proj = nn.Linear(d_model, n_heads, bias=True)\n",
    "        nn.init.constant_(self.beta_proj.bias, -2.0)\n",
    "        \n",
    "        self.g_proj = nn.Linear(d_model, n_heads, bias=True)\n",
    "        nn.init.constant_(self.g_proj.bias, 2.0)\n",
    "        \n",
    "        self.norm = RMSNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, initial_state=None):\n",
    "        B, T, D = x.shape\n",
    "        H, K, V = self.n_heads, self.head_dim, self.value_dim\n",
    "        \n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        q = self.q_proj(x_norm).view(B, T, H, K)\n",
    "        k = self.k_proj(x_norm).view(B, T, H, K)\n",
    "        v = self.v_proj(x_norm).view(B, T, H, V)\n",
    "        \n",
    "        k = F.normalize(k.float(), p=2, dim=-1).to(x.dtype)\n",
    "        \n",
    "        beta = torch.sigmoid(self.beta_proj(x_norm))\n",
    "        g = torch.sigmoid(self.g_proj(x_norm))\n",
    "        \n",
    "        # Use Triton kernel\n",
    "        out, state = triton_delta_simple(k, v, beta, g, initial_state)\n",
    "        \n",
    "        out = out.to(x.dtype).reshape(B, T, H * V)\n",
    "        out = self.o_proj(out)\n",
    "        \n",
    "        return x + out, state\n",
    "\n",
    "# Test\n",
    "layer = GatedDeltaNetLayer(256, 8, 32, 64).cuda()\n",
    "x = torch.randn(2, 128, 256, device='cuda')\n",
    "out, state = layer(x)\n",
    "print(f\"\\nGDN Layer test: input {x.shape} -> output {out.shape}, state {state.shape}\")\n",
    "print(\"✓ Integration successful\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
