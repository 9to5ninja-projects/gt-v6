{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroundThink V6 - Hybrid GatedDeltaNet + SWAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 0: INSTALL & EXPLORE FLA\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install -q triton\n",
    "!pip install -q flash-linear-attention\n",
    "!pip install -q datasets transformers\n",
    "\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "\n",
    "# Explore FLA structure\n",
    "import fla\n",
    "print(f\"\\nFLA top-level: {[x for x in dir(fla) if not x.startswith('_')]}\")\n",
    "\n",
    "# Find all submodules\n",
    "import pkgutil\n",
    "print(\"\\nFLA submodules:\")\n",
    "for importer, modname, ispkg in pkgutil.walk_packages(fla.__path__, fla.__name__ + '.'):\n",
    "    print(f\"  {modname}\")\n",
    "\n",
    "# Try common import paths\n",
    "print(\"\\n--- Trying imports ---\")\n",
    "try:\n",
    "    from fla.layers import GatedDeltaNet\n",
    "    print(\"from fla.layers import GatedDeltaNet: OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"from fla.layers: {e}\")\n",
    "\n",
    "try:\n",
    "    from fla.models import GatedDeltaNetModel\n",
    "    print(\"from fla.models import GatedDeltaNetModel: OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"from fla.models: {e}\")\n",
    "\n",
    "try:\n",
    "    from fla.ops.gated_delta_rule import GatedDeltaNet\n",
    "    print(\"from fla.ops.gated_delta_rule: OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"from fla.ops.gated_delta_rule: {e}\")\n",
    "\n",
    "# Check what's in fla.layers if it exists\n",
    "try:\n",
    "    import fla.layers as fla_layers\n",
    "    print(f\"\\nfla.layers contents: {[x for x in dir(fla_layers) if not x.startswith('_')]}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Check what's in fla.models if it exists  \n",
    "try:\n",
    "    import fla.models as fla_models\n",
    "    print(f\"\\nfla.models contents: {[x for x in dir(fla_models) if not x.startswith('_')]}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: CONFIGURATION (run after Cell 0 confirms imports)\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = 50257\n",
    "    d_model: int = 512\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 8\n",
    "    head_dim: int = 64\n",
    "    attn_interval: int = 4\n",
    "    window_size: int = 2048\n",
    "    expand_k: float = 1.0\n",
    "    expand_v: float = 2.0\n",
    "    max_seq_len: int = 2048\n",
    "    use_gradient_checkpointing: bool = True\n",
    "    tie_weights: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.head_dim = self.d_model // self.n_heads\n",
    "        \n",
    "    def get_swa_layer_indices(self):\n",
    "        return [i for i in range(self.n_layers) if i % self.attn_interval == (self.attn_interval - 1)]\n",
    "\n",
    "@dataclass \n",
    "class TrainConfig:\n",
    "    dataset_name: str = \"HuggingFaceFW/fineweb-edu\"\n",
    "    dataset_subset: str = \"sample-10BT\"\n",
    "    target_tokens: int = 20_000_000\n",
    "    batch_size: int = 2\n",
    "    seq_len: int = 512\n",
    "    accum_steps: int = 2\n",
    "    steps: int = 10000\n",
    "    warmup_ratio: float = 0.1\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.1\n",
    "    grad_clip: float = 1.0\n",
    "    betas: tuple = (0.9, 0.95)\n",
    "    log_interval: int = 50\n",
    "    grad_log_interval: int = 500\n",
    "    niah_checkpoints: List[int] = field(default_factory=lambda: [500, 1000, 2000, 3000, 5000, 7500, 10000])\n",
    "    \n",
    "    @property\n",
    "    def warmup_steps(self): return int(self.steps * self.warmup_ratio)\n",
    "    @property\n",
    "    def effective_batch_size(self): return self.batch_size * self.accum_steps\n",
    "\n",
    "MODEL_CFG = ModelConfig()\n",
    "TRAIN_CFG = TrainConfig()\n",
    "print(f\"Config: d={MODEL_CFG.d_model}, layers={MODEL_CFG.n_layers}, SWA@{MODEL_CFG.get_swa_layer_indices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: IMPORTS - UPDATE THESE BASED ON CELL 0 OUTPUT\n",
    "import math, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {props.name} ({props.total_memory/1e9:.1f}GB)\")\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# ============================================================\n",
    "# FLA IMPORTS - UPDATE BASED ON CELL 0 EXPLORATION\n",
    "# ============================================================\n",
    "# Option 1: If fla.layers works\n",
    "# from fla.layers import GatedDeltaNet\n",
    "# from fla.layers import SlidingWindowAttention as FLA_SWA\n",
    "\n",
    "# Option 2: If using models\n",
    "# from fla.models.gated_delta_net import GatedDeltaNetConfig, GatedDeltaNetModel\n",
    "\n",
    "# Option 3: Build from ops\n",
    "# from fla.ops.gated_delta_rule import fused_recurrent_gated_delta_rule\n",
    "\n",
    "# For now, let's see what we found:\n",
    "import fla\n",
    "print(\"Checking fla structure...\")\n",
    "\n",
    "# Try the most likely paths\n",
    "GatedDeltaNet = None\n",
    "FLA_SWA = None\n",
    "\n",
    "# Try fla.layers\n",
    "try:\n",
    "    from fla.layers import GatedDeltaNet as GDN\n",
    "    GatedDeltaNet = GDN\n",
    "    print(\"GatedDeltaNet from fla.layers: OK\")\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Try fla.layers.gated_delta_net\n",
    "if GatedDeltaNet is None:\n",
    "    try:\n",
    "        from fla.layers.gated_delta_net import GatedDeltaNet as GDN\n",
    "        GatedDeltaNet = GDN\n",
    "        print(\"GatedDeltaNet from fla.layers.gated_delta_net: OK\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "# Try sliding window\n",
    "try:\n",
    "    from fla.layers import SlidingWindowAttention as SWA\n",
    "    FLA_SWA = SWA\n",
    "    print(\"SlidingWindowAttention from fla.layers: OK\")\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Fallback: use standard MultiheadAttention with window masking\n",
    "if FLA_SWA is None:\n",
    "    print(\"SlidingWindowAttention not found - will use custom implementation\")\n",
    "    FLA_SWA = None\n",
    "\n",
    "if GatedDeltaNet is None:\n",
    "    raise ImportError(\"Could not find GatedDeltaNet in FLA. Check Cell 0 output for available modules.\")\n",
    "\n",
    "print(f\"\\nGatedDeltaNet: {GatedDeltaNet}\")\n",
    "print(f\"SWA: {FLA_SWA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: MODEL COMPONENTS\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        return (x.float() * x.float().pow(2).mean(-1, keepdim=True).add(self.eps).rsqrt()).type_as(x) * self.weight\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, d_model, expansion=8/3):\n",
    "        super().__init__()\n",
    "        hidden = ((int(d_model * expansion) + 63) // 64) * 64\n",
    "        self.w1 = nn.Linear(d_model, hidden, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, hidden, bias=False)\n",
    "        self.w2 = nn.Linear(hidden, d_model, bias=False)\n",
    "        self.norm = RMSNorm(d_model)\n",
    "    def forward(self, x):\n",
    "        h = self.norm(x)\n",
    "        return x + self.w2(F.silu(self.w1(h)) * self.w3(h))\n",
    "\n",
    "# Custom SWA if FLA doesn't have one\n",
    "class CustomSlidingWindowAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with sliding window mask.\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, window_size, layer_idx=0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.window_size = window_size\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        self.qkv = nn.Linear(hidden_size, 3 * hidden_size, bias=False)\n",
    "        self.out = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, D = x.shape\n",
    "        qkv = self.qkv(x).view(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Sliding window + causal mask\n",
    "        mask = torch.ones(T, T, device=x.device, dtype=torch.bool).triu(1)  # causal\n",
    "        mask |= torch.ones(T, T, device=x.device, dtype=torch.bool).tril(-self.window_size)  # window\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn = attn.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, T, D)\n",
    "        return self.out(out)\n",
    "\n",
    "# Use FLA's SWA if available, otherwise custom\n",
    "SlidingWindowAttention = FLA_SWA if FLA_SWA is not None else CustomSlidingWindowAttention\n",
    "\n",
    "class HybridBlock(nn.Module):\n",
    "    def __init__(self, d_model, is_attention, n_heads=8, window_size=2048,\n",
    "                 expand_k=1.0, expand_v=2.0, layer_idx=0):\n",
    "        super().__init__()\n",
    "        self.is_attention = is_attention\n",
    "        self.layer_idx = layer_idx\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        \n",
    "        if is_attention:\n",
    "            self.layer = SlidingWindowAttention(\n",
    "                hidden_size=d_model, num_heads=n_heads,\n",
    "                window_size=window_size, layer_idx=layer_idx)\n",
    "        else:\n",
    "            # GatedDeltaNet from FLA\n",
    "            self.layer = GatedDeltaNet(\n",
    "                hidden_size=d_model, \n",
    "                expand_k=expand_k,\n",
    "                expand_v=expand_v, \n",
    "                layer_idx=layer_idx)\n",
    "    \n",
    "    def forward(self, x, attention_mask=None, past_state=None, use_cache=False):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        new_state = None\n",
    "        \n",
    "        if use_cache:\n",
    "            if self.is_attention:\n",
    "                x, new_state = self.layer(x, attention_mask=attention_mask,\n",
    "                                          past_key_values=past_state, use_cache=True)\n",
    "            else:\n",
    "                x, new_state = self.layer(x, past_state=past_state, use_cache=True)\n",
    "        else:\n",
    "            if self.is_attention:\n",
    "                x = self.layer(x, attention_mask=attention_mask)\n",
    "            else:\n",
    "                x = self.layer(x)\n",
    "        \n",
    "        return residual + x, new_state\n",
    "\n",
    "class GroundThinkLM(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        \n",
    "        swa_indices = set(cfg.get_swa_layer_indices())\n",
    "        self._swa_indices = swa_indices\n",
    "        self._gdn_indices = set(range(cfg.n_layers)) - swa_indices\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        for i in range(cfg.n_layers):\n",
    "            self.blocks.append(HybridBlock(\n",
    "                cfg.d_model, is_attention=(i in swa_indices),\n",
    "                n_heads=cfg.n_heads, window_size=cfg.window_size,\n",
    "                expand_k=cfg.expand_k, expand_v=cfg.expand_v, layer_idx=i))\n",
    "            self.ffns.append(SwiGLUFFN(cfg.d_model))\n",
    "        \n",
    "        self.norm_f = RMSNorm(cfg.d_model)\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "        if cfg.tie_weights:\n",
    "            self.lm_head.weight = self.embed.weight\n",
    "        \n",
    "    def forward(self, input_ids, targets=None, attention_mask=None,\n",
    "                past_states=None, use_cache=False):\n",
    "        x = self.embed(input_ids)\n",
    "        new_states = [] if use_cache else None\n",
    "        \n",
    "        for i, (block, ffn) in enumerate(zip(self.blocks, self.ffns)):\n",
    "            past_state = past_states[i] if past_states else None\n",
    "            if self.cfg.use_gradient_checkpointing and self.training and i in self._swa_indices:\n",
    "                x = checkpoint(self._fwd_block, block, ffn, x, attention_mask, use_reentrant=False)\n",
    "            else:\n",
    "                x, state = block(x, attention_mask, past_state, use_cache)\n",
    "                x = ffn(x)\n",
    "                if use_cache: new_states.append(state)\n",
    "        \n",
    "        logits = self.lm_head(self.norm_f(x))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None\n",
    "        return logits, loss, new_states\n",
    "    \n",
    "    @staticmethod\n",
    "    def _fwd_block(block, ffn, x, mask):\n",
    "        x, _ = block(x, mask, None, False)\n",
    "        return ffn(x)\n",
    "    \n",
    "    def get_layer_types(self):\n",
    "        return ['swa' if i in self._swa_indices else 'gdn' for i in range(self.cfg.n_layers)]\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        c = {'embed': sum(p.numel() for p in self.embed.parameters()), 'gdn': 0, 'swa': 0, 'ffn': 0}\n",
    "        for i, (b, f) in enumerate(zip(self.blocks, self.ffns)):\n",
    "            bp, fp = sum(p.numel() for p in b.parameters()), sum(p.numel() for p in f.parameters())\n",
    "            c['swa' if i in self._swa_indices else 'gdn'] += bp\n",
    "            c['ffn'] += fp\n",
    "        c['total'] = sum(c.values())\n",
    "        return c\n",
    "\n",
    "print(\"Model components defined\")\n",
    "print(f\"Using SWA: {SlidingWindowAttention}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: MONITORING\n",
    "\n",
    "def print_gradient_summary(model):\n",
    "    agg = {'embed': [], 'gdn': [], 'swa': [], 'ffn': []}\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.grad is None: continue\n",
    "        n = p.grad.norm().item()\n",
    "        if 'embed' in name: agg['embed'].append(n)\n",
    "        elif 'ffn' in name: agg['ffn'].append(n)\n",
    "        elif 'blocks' in name:\n",
    "            idx = int(name.split('.')[1])\n",
    "            agg['swa' if idx in model._swa_indices else 'gdn'].append(n)\n",
    "    print(\"Gradient Norms:\")\n",
    "    for k, v in agg.items():\n",
    "        if v: print(f\"  {k}: mean={np.mean(v):.3f} max={np.max(v):.2f}\")\n",
    "\n",
    "def needle_test(model, tokenizer, seq_len=512, n_trials=50, needle_token=50250, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_trials):\n",
    "            tokens = torch.randint(1000, 10000, (1, seq_len), device=device)\n",
    "            pos = torch.randint(64, seq_len - 64, (1,)).item()\n",
    "            tokens[0, pos] = needle_token\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits, _, _ = model(tokens)\n",
    "            probs.append(F.softmax(logits[0, -1].float(), dim=-1)[needle_token].item())\n",
    "    rc = 1.0 / tokenizer.vocab_size\n",
    "    return {'mean': np.mean(probs), 'std': np.std(probs), 'ratio': np.mean(probs) / rc}\n",
    "\n",
    "def probe_layers(model, tokenizer, needle_id=50250, seq_len=512, pos=256, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    tokens = torch.randint(1000, 10000, (1, seq_len), device=device)\n",
    "    tokens[0, pos] = needle_id\n",
    "    with torch.no_grad():\n",
    "        x = model.embed(tokens)\n",
    "        emb = model.embed.weight[needle_id].float()\n",
    "        print(f\"Needle representation:\")\n",
    "        for i, (b, f) in enumerate(zip(model.blocks, model.ffns)):\n",
    "            x, _ = b(x, None, None, False)\n",
    "            x = f(x)\n",
    "            sim = F.cosine_similarity(x[0, pos].float(), emb, dim=0).item()\n",
    "            t = \"SWA\" if i in model._swa_indices else \"GDN\"\n",
    "            print(f\"  L{i:2d}[{t}]: {sim:+.3f}\")\n",
    "\n",
    "print(\"Monitoring ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: DATA\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "MODEL_CFG.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(f\"Streaming {TRAIN_CFG.dataset_name}...\")\n",
    "ds = load_dataset(TRAIN_CFG.dataset_name, name=TRAIN_CFG.dataset_subset, split=\"train\", streaming=True)\n",
    "buf = []\n",
    "pbar = tqdm(total=TRAIN_CFG.target_tokens, unit=\"tok\")\n",
    "for ex in ds:\n",
    "    toks = tokenizer.encode(ex['text']) + [tokenizer.eos_token_id]\n",
    "    buf.extend(toks)\n",
    "    pbar.update(len(toks))\n",
    "    if len(buf) >= TRAIN_CFG.target_tokens: break\n",
    "pbar.close()\n",
    "\n",
    "all_tokens = torch.tensor(buf[:TRAIN_CFG.target_tokens], dtype=torch.long)\n",
    "del buf, ds\n",
    "print(f\"Loaded {len(all_tokens):,} tokens\")\n",
    "\n",
    "def get_batch():\n",
    "    ix = torch.randint(len(all_tokens) - TRAIN_CFG.seq_len - 1, (TRAIN_CFG.batch_size,))\n",
    "    x = torch.stack([all_tokens[i:i+TRAIN_CFG.seq_len] for i in ix])\n",
    "    y = torch.stack([all_tokens[i+1:i+TRAIN_CFG.seq_len+1] for i in ix])\n",
    "    return x.to(DEVICE), y.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: BUILD MODEL\n",
    "print(\"Building...\")\n",
    "model = GroundThinkLM(MODEL_CFG).to(DEVICE).to(torch.bfloat16)\n",
    "p = model.count_parameters()\n",
    "print(f\"Params: {p['total']/1e6:.2f}M (GDN:{p['gdn']/1e6:.1f}M, SWA:{p['swa']/1e6:.1f}M, FFN:{p['ffn']/1e6:.1f}M)\")\n",
    "print(f\"Layers: {model.get_layer_types()}\")\n",
    "\n",
    "x, y = get_batch()\n",
    "with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "    _, loss, _ = model(x, y)\n",
    "loss.backward()\n",
    "print(f\"Test: loss={loss.item():.4f}, mem={torch.cuda.max_memory_allocated()/1e9:.2f}GB\")\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: TRAIN\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=TRAIN_CFG.lr, betas=TRAIN_CFG.betas, weight_decay=TRAIN_CFG.weight_decay)\n",
    "losses, niah_traj = [], []\n",
    "start = time.time()\n",
    "\n",
    "print(f\"\\nTRAINING {TRAIN_CFG.steps} steps, batch={TRAIN_CFG.effective_batch_size}\\n\")\n",
    "model.train()\n",
    "\n",
    "for step in range(TRAIN_CFG.steps):\n",
    "    lr = TRAIN_CFG.lr * (step+1)/TRAIN_CFG.warmup_steps if step < TRAIN_CFG.warmup_steps else \\\n",
    "         TRAIN_CFG.lr * 0.5 * (1 + math.cos(math.pi * (step-TRAIN_CFG.warmup_steps)/(TRAIN_CFG.steps-TRAIN_CFG.warmup_steps)))\n",
    "    for pg in opt.param_groups: pg['lr'] = lr\n",
    "    \n",
    "    acc_loss = 0\n",
    "    for _ in range(TRAIN_CFG.accum_steps):\n",
    "        x, y = get_batch()\n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            _, loss, _ = model(x, y)\n",
    "        (loss / TRAIN_CFG.accum_steps).backward()\n",
    "        acc_loss += loss.item()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), TRAIN_CFG.grad_clip)\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    losses.append(acc_loss / TRAIN_CFG.accum_steps)\n",
    "    \n",
    "    if step % TRAIN_CFG.log_interval == 0:\n",
    "        avg = np.mean(losses[-50:]) if len(losses) >= 50 else np.mean(losses)\n",
    "        tps = (step+1) * TRAIN_CFG.effective_batch_size * TRAIN_CFG.seq_len / (time.time()-start)\n",
    "        print(f\"[{step:5d}] loss={avg:.4f} lr={lr:.2e} {tps:,.0f}tok/s\")\n",
    "    \n",
    "    if (step+1) % TRAIN_CFG.grad_log_interval == 0:\n",
    "        print_gradient_summary(model)\n",
    "    \n",
    "    if (step+1) in TRAIN_CFG.niah_checkpoints:\n",
    "        n = needle_test(model, tokenizer, TRAIN_CFG.seq_len, 30, device=DEVICE)\n",
    "        niah_traj.append((step+1, n['ratio']))\n",
    "        print(f\"  NIAH@{step+1}: {n['ratio']:.2f}x\")\n",
    "        model.train()\n",
    "\n",
    "print(f\"\\nDone in {(time.time()-start)/60:.1f}min\")\n",
    "print(f\"Loss: {np.mean(losses[:50]):.4f} -> {np.mean(losses[-50:]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: EVAL\n",
    "print(\"\\nFINAL EVAL\")\n",
    "for L in [128, 256, 512, 1024]:\n",
    "    n = needle_test(model, tokenizer, L, 50, device=DEVICE)\n",
    "    print(f\"  NIAH@{L}: {n['ratio']:.2f}x\")\n",
    "probe_layers(model, tokenizer, device=DEVICE)\n",
    "print(f\"\\nLM: {'PASS' if np.mean(losses[:50])-np.mean(losses[-50:])>2 else 'MARGINAL'}\")\n",
    "print(f\"NIAH: {'PASS' if any(r>1 for _,r in niah_traj) else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: SAVE\n",
    "from datetime import datetime\n",
    "rid = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "path = f\"/content/drive/MyDrive/groundthink/colab-exports/v6_{rid}.pt\"\n",
    "torch.save({'state': model.state_dict(), 'cfg': MODEL_CFG, 'losses': losses, 'niah': niah_traj}, path)\n",
    "print(f\"Saved: {path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {"gpuType": "A100"},
  "kernelspec": {"display_name": "Python 3", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
