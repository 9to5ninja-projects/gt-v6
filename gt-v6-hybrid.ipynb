{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL: Transparent Hybrid Architecture with Diagnostics\n",
    "# Uses raw fla.ops, exposes all state, configurable layers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fla.ops.gated_delta_rule import chunk_gated_delta_rule\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Literal\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class HybridConfig:\n",
    "    \"\"\"Fully configurable - start small, scale up\"\"\"\n",
    "    d_model: int = 256\n",
    "    n_heads: int = 8\n",
    "    head_dim: int = 32        # d_model // n_heads\n",
    "    expand_v: float = 2.0     # value expansion (GDN default)\n",
    "    vocab_size: int = 50257\n",
    "    \n",
    "    # Layer pattern: 'G' = GDN, 'S' = SWA\n",
    "    # Examples: \"GS\", \"GGS\", \"GGSG\", \"GGGSGGGS\"\n",
    "    layer_pattern: str = \"GS\"\n",
    "    \n",
    "    # SWA config\n",
    "    window_size: int = 1024\n",
    "    \n",
    "    # Init\n",
    "    init_std: float = 0.02\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.head_dim = self.d_model // self.n_heads\n",
    "        self.value_dim = int(self.head_dim * self.expand_v)\n",
    "        \n",
    "    @property\n",
    "    def n_layers(self):\n",
    "        return len(self.layer_pattern)\n",
    "    \n",
    "    def layer_type(self, idx: int) -> str:\n",
    "        return self.layer_pattern[idx]\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = x.float().pow(2).mean(-1, keepdim=True).add(self.eps).rsqrt()\n",
    "        return (x.float() * norm).type_as(x) * self.weight\n",
    "\n",
    "\n",
    "class GatedDeltaNetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transparent GDN using raw op.\n",
    "    \n",
    "    Delta Rule: Sₜ = αₜ * Sₜ₋₁ + βₜ * (vₜ ⊗ kₜ)\n",
    "    - αₜ (gate g): controls forgetting (in log space)\n",
    "    - βₜ (beta): controls write strength\n",
    "    - Sₜ: state matrix [B, H, K, V] - the memory\n",
    "    \n",
    "    Output: Sₜ @ qₜ (query the memory)\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: HybridConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(cfg.d_model, cfg.n_heads * cfg.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(cfg.d_model, cfg.n_heads * cfg.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(cfg.d_model, cfg.n_heads * cfg.value_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(cfg.n_heads * cfg.value_dim, cfg.d_model, bias=False)\n",
    "        \n",
    "        # Gate projections (per-head scalars)\n",
    "        self.beta_proj = nn.Linear(cfg.d_model, cfg.n_heads, bias=False)  # write strength\n",
    "        self.g_proj = nn.Linear(cfg.d_model, cfg.n_heads, bias=False)     # forget gate (log space)\n",
    "        \n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, \n",
    "                initial_state: Optional[torch.Tensor] = None,\n",
    "                output_state: bool = True) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, T, D]\n",
    "            initial_state: [B, H, K, V] or None\n",
    "            output_state: whether to return final state\n",
    "            \n",
    "        Returns:\n",
    "            output: [B, T, D]\n",
    "            state: [B, H, K, V] if output_state else None\n",
    "            diagnostics: dict with internal values for inspection\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        H, K, V = self.cfg.n_heads, self.cfg.head_dim, self.cfg.value_dim\n",
    "        \n",
    "        # Pre-norm\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        # Project to q, k, v\n",
    "        q = self.q_proj(x_norm).view(B, T, H, K)\n",
    "        k = self.k_proj(x_norm).view(B, T, H, K)\n",
    "        v = self.v_proj(x_norm).view(B, T, H, V)\n",
    "        \n",
    "        # Normalize k for stability (as per FLA convention)\n",
    "        k = F.normalize(k.float(), p=2, dim=-1).to(x.dtype)\n",
    "        \n",
    "        # Gates\n",
    "        beta = self.beta_proj(x_norm).sigmoid()  # [B, T, H] write strength ∈ (0,1)\n",
    "        g = F.logsigmoid(self.g_proj(x_norm))    # [B, T, H] forget gate in log space\n",
    "        \n",
    "        # Core delta rule op\n",
    "        output, state = chunk_gated_delta_rule(\n",
    "            q, k, v, g, beta,\n",
    "            initial_state=initial_state,\n",
    "            output_final_state=output_state\n",
    "        )\n",
    "        \n",
    "        # Project back\n",
    "        output = output.reshape(B, T, H * V)\n",
    "        output = self.o_proj(output)\n",
    "        \n",
    "        # Residual\n",
    "        output = x + output\n",
    "        \n",
    "        # Diagnostics for understanding\n",
    "        diagnostics = {\n",
    "            'beta_mean': beta.mean().item(),      # avg write strength\n",
    "            'beta_std': beta.std().item(),\n",
    "            'g_mean': g.exp().mean().item(),      # avg forget rate (converted from log)\n",
    "            'g_std': g.exp().std().item(),\n",
    "            'state_norm': state.norm().item() if state is not None else 0,\n",
    "            'state_shape': tuple(state.shape) if state is not None else None,\n",
    "        }\n",
    "        \n",
    "        return output, state, diagnostics\n",
    "\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    SWA that can also attend to GDN state.\n",
    "    \n",
    "    Two attention sources:\n",
    "    1. Local window (standard SWA)\n",
    "    2. Global state from GDN (optional cross-attention)\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: HybridConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        # Standard attention projections\n",
    "        self.q_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        \n",
    "        # For querying GDN state: project state [H, K, V] to something usable\n",
    "        # State is [B, H, K, V] - we can treat it as H key-value pairs\n",
    "        self.state_k_proj = nn.Linear(cfg.head_dim, cfg.head_dim, bias=False)\n",
    "        self.state_v_proj = nn.Linear(cfg.value_dim, cfg.head_dim, bias=False)  # compress V->K\n",
    "        \n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        self.scale = cfg.head_dim ** -0.5\n",
    "        \n",
    "    def forward(self, x: torch.Tensor,\n",
    "                gdn_state: Optional[torch.Tensor] = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, T, D]\n",
    "            gdn_state: [B, H, K, V] accumulated state from GDN layers\n",
    "            \n",
    "        Returns:\n",
    "            output: [B, T, D]\n",
    "            diagnostics: dict\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        H = self.cfg.n_heads\n",
    "        K = self.cfg.head_dim\n",
    "        W = self.cfg.window_size\n",
    "        \n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        q = self.q_proj(x_norm).view(B, T, H, K)\n",
    "        k = self.k_proj(x_norm).view(B, T, H, K)\n",
    "        v = self.v_proj(x_norm).view(B, T, H, K)\n",
    "        \n",
    "        # Transpose for attention: [B, H, T, K]\n",
    "        q, k, v = [t.transpose(1, 2) for t in (q, k, v)]\n",
    "        \n",
    "        # Sliding window mask\n",
    "        mask = torch.ones(T, T, device=x.device, dtype=torch.bool)\n",
    "        mask = mask.triu(1) | mask.tril(-W - 1)  # causal + window\n",
    "        \n",
    "        # Local attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        attn_weights_local = F.softmax(attn, dim=-1)\n",
    "        local_out = attn_weights_local @ v  # [B, H, T, K]\n",
    "        \n",
    "        # State attention (if GDN state provided)\n",
    "        state_out = None\n",
    "        state_attn_weight = 0.0     # Is this influence amount from GDN to SWA?\n",
    "        if gdn_state is not None:\n",
    "            # gdn_state: [B, H, K, V]\n",
    "            # Treat each row of state as a \"memory slot\"\n",
    "            # Query: q [B, H, T, K]\n",
    "            # State keys: state [B, H, K, V] -> we use the K dimension as \"memory slots\"\n",
    "            \n",
    "            # Simple approach: q @ state -> [B, H, T, V], then project back\n",
    "            state_retrieved = q @ gdn_state.to(q.dtype)  # [B, H, T, V]\n",
    "            state_out = self.state_v_proj(state_retrieved)  # [B, H, T, K]\n",
    "            \n",
    "            # Combine local and state (learnable mixing would be better, but start simple)\n",
    "            # For now: add them\n",
    "            state_attn_weight = (state_out.norm() / (local_out.norm() + 1e-8)).item()\n",
    "        \n",
    "        # Combine\n",
    "        if state_out is not None:\n",
    "            out = local_out + state_out\n",
    "        else:\n",
    "            out = local_out\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).reshape(B, T, D)\n",
    "        out = self.o_proj(out)\n",
    "        out = x + out\n",
    "        \n",
    "        diagnostics = {\n",
    "            'local_attn_entropy': -(attn_weights_local * attn_weights_local.clamp(min=1e-8).log()).sum(-1).mean().item(),\n",
    "            'state_contribution': state_attn_weight,\n",
    "        }\n",
    "        \n",
    "        return out, diagnostics\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    \"\"\"Simple SwiGLU FFN\"\"\"\n",
    "    def __init__(self, cfg: HybridConfig):\n",
    "        super().__init__()\n",
    "        hidden = int(cfg.d_model * 8/3)\n",
    "        hidden = ((hidden + 63) // 64) * 64  # round to 64\n",
    "        \n",
    "        self.w1 = nn.Linear(cfg.d_model, hidden, bias=False)\n",
    "        self.w3 = nn.Linear(cfg.d_model, hidden, bias=False)\n",
    "        self.w2 = nn.Linear(hidden, cfg.d_model, bias=False)\n",
    "        self.norm = RMSNorm(cfg.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.norm(x)\n",
    "        return x + self.w2(F.silu(self.w1(h)) * self.w3(h))\n",
    "\n",
    "\n",
    "class TransparentHybrid(nn.Module):\n",
    "    \"\"\"\n",
    "    Configurable GDN + SWA hybrid with full visibility.\n",
    "    \n",
    "    Key insight: GDN state Sₜ is a [H, K, V] matrix per batch.\n",
    "    - It accumulates information across the sequence\n",
    "    - SWA can query it to retrieve global context\n",
    "    - This is how a needle at pos 100 reaches output at pos 500\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: HybridConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Embedding\n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        nn.init.normal_(self.embed.weight, std=cfg.init_std)\n",
    "        \n",
    "        # Build layers according to pattern\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        \n",
    "        for i, layer_type in enumerate(cfg.layer_pattern):\n",
    "            if layer_type == 'G':\n",
    "                self.layers.append(GatedDeltaNetLayer(cfg, i))\n",
    "            elif layer_type == 'S':\n",
    "                self.layers.append(SlidingWindowAttention(cfg, i))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
    "            self.ffns.append(FFN(cfg))\n",
    "        \n",
    "        self.norm_f = RMSNorm(cfg.d_model)\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embed.weight  # tie weights\n",
    "        \n",
    "        # Track which layers are GDN for state accumulation\n",
    "        self.gdn_indices = [i for i, t in enumerate(cfg.layer_pattern) if t == 'G']\n",
    "        self.swa_indices = [i for i, t in enumerate(cfg.layer_pattern) if t == 'S']\n",
    "        \n",
    "        print(f\"Architecture: {cfg.layer_pattern}\")\n",
    "        print(f\"  GDN layers: {self.gdn_indices}\")\n",
    "        print(f\"  SWA layers: {self.swa_indices}\")\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, \n",
    "                targets: Optional[torch.Tensor] = None,\n",
    "                return_diagnostics: bool = False) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [B, T]\n",
    "            targets: [B, T] for loss computation\n",
    "            return_diagnostics: whether to return per-layer diagnostics\n",
    "            \n",
    "        Returns:\n",
    "            logits: [B, T, V]\n",
    "            loss: scalar if targets provided\n",
    "            diagnostics: dict if return_diagnostics\n",
    "        \"\"\"\n",
    "        x = self.embed(input_ids)\n",
    "        \n",
    "        # Track GDN state - accumulate across GDN layers\n",
    "        accumulated_state = None\n",
    "        all_diagnostics = []\n",
    "        \n",
    "        for i, (layer, ffn) in enumerate(zip(self.layers, self.ffns)):\n",
    "            layer_type = self.cfg.layer_pattern[i]\n",
    "            \n",
    "            if layer_type == 'G':\n",
    "                x, state, diag = layer(x, initial_state=accumulated_state, output_state=True)\n",
    "                # Accumulate state (could also replace - design choice)\n",
    "                if accumulated_state is None:\n",
    "                    accumulated_state = state\n",
    "                else:\n",
    "                    # Weighted combination - newer state more important\n",
    "                    accumulated_state = 0.5 * accumulated_state + 0.5 * state\n",
    "                diag['layer_type'] = 'GDN'\n",
    "                \n",
    "            elif layer_type == 'S':\n",
    "                x, diag = layer(x, gdn_state=accumulated_state)\n",
    "                diag['layer_type'] = 'SWA'\n",
    "            \n",
    "            x = ffn(x)\n",
    "            diag['layer_idx'] = i\n",
    "            all_diagnostics.append(diag)\n",
    "        \n",
    "        x = self.norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        if return_diagnostics:\n",
    "            return logits, loss, all_diagnostics, accumulated_state\n",
    "        return logits, loss, None, None\n",
    "    \n",
    "    def probe_state(self, input_ids: torch.Tensor, \n",
    "                    needle_pos: int,\n",
    "                    query_pos: int) -> dict:\n",
    "        \"\"\"\n",
    "        Diagnostic: Check if needle information is in the state.\n",
    "        \n",
    "        Returns analysis of whether the needle token at needle_pos\n",
    "        can be retrieved from state at query_pos.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            x = self.embed(input_ids)\n",
    "            accumulated_state = None\n",
    "            \n",
    "            results = {'per_layer': []}\n",
    "            \n",
    "            for i, (layer, ffn) in enumerate(zip(self.layers, self.ffns)):\n",
    "                layer_type = self.cfg.layer_pattern[i]\n",
    "                \n",
    "                if layer_type == 'G':\n",
    "                    x, state, _ = layer(x, initial_state=accumulated_state, output_state=True)\n",
    "                    \n",
    "                    if accumulated_state is None:\n",
    "                        accumulated_state = state\n",
    "                    else:\n",
    "                        accumulated_state = 0.5 * accumulated_state + 0.5 * state\n",
    "                    \n",
    "                    # Check: can we find needle in state?\n",
    "                    # Get needle's key representation\n",
    "                    needle_embed = self.embed.weight[input_ids[0, needle_pos]]\n",
    "                    \n",
    "                    # State is [B, H, K, V] - query it\n",
    "                    # A simple probe: project needle through k_proj, query state\n",
    "                    k_proj = layer.k_proj\n",
    "                    needle_key = k_proj(needle_embed).view(self.cfg.n_heads, self.cfg.head_dim)\n",
    "                    needle_key = F.normalize(needle_key.float(), p=2, dim=-1)\n",
    "                    \n",
    "                    # Query state: needle_key @ state -> retrieval\n",
    "                    # state: [1, H, K, V]\n",
    "                    retrieved = torch.einsum('hk,bhkv->bhv', needle_key, accumulated_state.float())\n",
    "                    \n",
    "                    results['per_layer'].append({\n",
    "                        'layer': i,\n",
    "                        'type': 'GDN',\n",
    "                        'state_norm': state.norm().item(),\n",
    "                        'retrieved_norm': retrieved.norm().item(),\n",
    "                    })\n",
    "                    \n",
    "                elif layer_type == 'S':\n",
    "                    x, _ = layer(x, gdn_state=accumulated_state)\n",
    "                    results['per_layer'].append({\n",
    "                        'layer': i,\n",
    "                        'type': 'SWA',\n",
    "                    })\n",
    "                \n",
    "                x = ffn(x)\n",
    "            \n",
    "            results['final_state_norm'] = accumulated_state.norm().item() if accumulated_state is not None else 0\n",
    "            \n",
    "        return results\n",
    "\n",
    "\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    gdn = sum(p.numel() for i, l in enumerate(model.layers) if model.cfg.layer_pattern[i] == 'G' for p in l.parameters())\n",
    "    swa = sum(p.numel() for i, l in enumerate(model.layers) if model.cfg.layer_pattern[i] == 'S' for p in l.parameters())\n",
    "    ffn = sum(p.numel() for f in model.ffns for p in f.parameters())\n",
    "    return {'total': total, 'gdn': gdn, 'swa': swa, 'ffn': ffn}\n",
    "\n",
    "\n",
    "# ============ TEST IT ============\n",
    "print(\"=\"*60)\n",
    "print(\"Building TransparentHybrid\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Start minimal\n",
    "cfg = HybridConfig(\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    layer_pattern=\"GS\",  # Just 1 GDN + 1 SWA to understand\n",
    "    window_size=128,\n",
    ")\n",
    "\n",
    "model = TransparentHybrid(cfg).cuda().bfloat16()\n",
    "params = count_params(model)\n",
    "print(f\"Parameters: {params['total']/1e6:.2f}M (GDN: {params['gdn']/1e6:.2f}M, SWA: {params['swa']/1e6:.2f}M)\")\n",
    "\n",
    "# Test forward\n",
    "x = torch.randint(0, 1000, (1, 128), device='cuda')\n",
    "y = torch.randint(0, 1000, (1, 128), device='cuda')\n",
    "\n",
    "logits, loss, diagnostics, state = model(x, y, return_diagnostics=True)\n",
    "print(f\"\\nForward pass:\")\n",
    "print(f\"  Logits: {logits.shape}\")\n",
    "print(f\"  Loss: {loss.item():.4f} (expected ~{math.log(cfg.vocab_size):.2f})\")\n",
    "print(f\"  Final state: {state.shape if state is not None else None}\")\n",
    "\n",
    "print(f\"\\nPer-layer diagnostics:\")\n",
    "for d in diagnostics:\n",
    "    print(f\"  Layer {d['layer_idx']} [{d['layer_type']}]: {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de549145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. NIAH test - does the needle actually get stored and retrieved?\n",
    "def simple_niah(model, seq_len=128, needle_pos=32, n_trials=20):\n",
    "    \"\"\"Put a rare token early, see if model predicts it at the end\"\"\"\n",
    "    model.eval()\n",
    "    needle_token = 50000  # rare token\n",
    "    \n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_trials):\n",
    "            # Random tokens with needle inserted\n",
    "            tokens = torch.randint(1000, 10000, (1, seq_len), device='cuda')\n",
    "            tokens[0, needle_pos] = needle_token\n",
    "            \n",
    "            logits, _, diags, state = model(tokens, return_diagnostics=True)\n",
    "            \n",
    "            # Check: does the final position predict the needle?\n",
    "            final_probs = F.softmax(logits[0, -1].float(), dim=-1)\n",
    "            needle_prob = final_probs[needle_token].item()\n",
    "            random_baseline = 1.0 / model.cfg.vocab_size\n",
    "            \n",
    "            results.append({\n",
    "                'needle_prob': needle_prob,\n",
    "                'ratio': needle_prob / random_baseline,\n",
    "                'state_norm': state.norm().item(),\n",
    "            })\n",
    "    \n",
    "    avg_ratio = sum(r['ratio'] for r in results) / len(results)\n",
    "    print(f\"NIAH (untrained): {avg_ratio:.4f}x random\")\n",
    "    print(f\"  (>1.0 means model finds needle, <1.0 means no retrieval)\")\n",
    "    return results\n",
    "\n",
    "# Test before training\n",
    "niah_results = simple_niah(model)\n",
    "\n",
    "# 2. Probe: where is needle info stored?\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Probing state for needle information\")\n",
    "print(\"=\"*60)\n",
    "tokens = torch.randint(1000, 10000, (1, 128), device='cuda')\n",
    "tokens[0, 32] = 50000  # needle at pos 32\n",
    "\n",
    "probe = model.probe_state(tokens, needle_pos=32, query_pos=127)\n",
    "print(f\"Final state norm: {probe['final_state_norm']:.4f}\")\n",
    "for layer in probe['per_layer']:\n",
    "    print(f\"  {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e595490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING WITH MONITORING\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Data setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading data...\")\n",
    "ds = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\", split=\"train\", streaming=True)\n",
    "\n",
    "# Buffer tokens\n",
    "token_buffer = []\n",
    "target_tokens = 2_000_000  # 2M for quick test\n",
    "\n",
    "for doc in ds:\n",
    "    toks = tokenizer.encode(doc['text'])\n",
    "    token_buffer.extend(toks)\n",
    "    if len(token_buffer) >= target_tokens:\n",
    "        break\n",
    "\n",
    "token_tensor = torch.tensor(token_buffer[:target_tokens], device='cuda')\n",
    "print(f\"Loaded {len(token_tensor):,} tokens\")\n",
    "\n",
    "def get_batch(batch_size=4, seq_len=128):\n",
    "    ix = torch.randint(0, len(token_tensor) - seq_len - 1, (batch_size,))\n",
    "    x = torch.stack([token_tensor[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([token_tensor[i+1:i+seq_len+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Training config\n",
    "STEPS = 20000\n",
    "LR = 3e-4\n",
    "BATCH = 4\n",
    "SEQ_LEN = 128\n",
    "LOG_EVERY = 100\n",
    "NIAH_EVERY = 500\n",
    "\n",
    "# Optimizer\n",
    "opt = AdamW(model.parameters(), lr=LR, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "\n",
    "# Tracking\n",
    "history = {\n",
    "    'loss': [],\n",
    "    'niah_ratio': [],\n",
    "    'gdn_beta': [],\n",
    "    'gdn_g': [],\n",
    "    'state_norm': [],\n",
    "    'swa_state_contrib': [],\n",
    "}\n",
    "\n",
    "print(f\"\\nTraining {STEPS} steps, batch={BATCH}, seq_len={SEQ_LEN}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.train()\n",
    "start = time.time()\n",
    "\n",
    "for step in range(STEPS):\n",
    "    # LR schedule: linear warmup then cosine\n",
    "    if step < 200:\n",
    "        lr = LR * (step + 1) / 200\n",
    "    else:\n",
    "        progress = (step - 200) / (STEPS - 200)\n",
    "        lr = LR * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    for pg in opt.param_groups:\n",
    "        pg['lr'] = lr\n",
    "    \n",
    "    # Forward\n",
    "    x, y = get_batch(BATCH, SEQ_LEN)\n",
    "    logits, loss, diags, state = model(x, y, return_diagnostics=True)\n",
    "    \n",
    "    # Backward\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    opt.step()\n",
    "    \n",
    "    # Track\n",
    "    history['loss'].append(loss.item())\n",
    "    \n",
    "    # Extract diagnostics\n",
    "    gdn_diag = [d for d in diags if d['layer_type'] == 'GDN'][0]\n",
    "    swa_diag = [d for d in diags if d['layer_type'] == 'SWA'][0]\n",
    "    \n",
    "    history['gdn_beta'].append(gdn_diag['beta_mean'])\n",
    "    history['gdn_g'].append(gdn_diag['g_mean'])\n",
    "    history['state_norm'].append(gdn_diag['state_norm'])\n",
    "    history['swa_state_contrib'].append(swa_diag['state_contribution'])\n",
    "    \n",
    "    # Log\n",
    "    if step % LOG_EVERY == 0:\n",
    "        elapsed = time.time() - start\n",
    "        tps = (step + 1) * BATCH * SEQ_LEN / elapsed\n",
    "        avg_loss = sum(history['loss'][-50:]) / min(50, len(history['loss']))\n",
    "        \n",
    "        print(f\"[{step:4d}] loss={avg_loss:.3f} lr={lr:.2e} | \"\n",
    "              f\"β={gdn_diag['beta_mean']:.3f} g={gdn_diag['g_mean']:.3f} \"\n",
    "              f\"state={gdn_diag['state_norm']:.1f} swa_state={swa_diag['state_contribution']:.2f} | \"\n",
    "              f\"{tps:,.0f} tok/s\")\n",
    "    \n",
    "    # NIAH check\n",
    "    if (step + 1) % NIAH_EVERY == 0:\n",
    "        model.eval()\n",
    "        niah = simple_niah(model, seq_len=SEQ_LEN, needle_pos=32, n_trials=30)\n",
    "        avg_ratio = sum(r['ratio'] for r in niah) / len(niah)\n",
    "        history['niah_ratio'].append((step + 1, avg_ratio))\n",
    "        status = \"PASS\" if avg_ratio > 1.0 else \"FAIL\"\n",
    "        print(f\"  >>> NIAH@{step+1}: {avg_ratio:.2f}x random [{status}]\")\n",
    "        model.train()\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "elapsed = time.time() - start\n",
    "print(f\"Time: {elapsed/60:.1f} min\")\n",
    "print(f\"Loss: {history['loss'][0]:.2f} -> {sum(history['loss'][-50:])/50:.2f}\")\n",
    "print(f\"NIAH trajectory: {history['niah_ratio']}\")\n",
    "\n",
    "# Final NIAH at multiple positions\n",
    "print(\"\\nFinal NIAH at different needle positions:\")\n",
    "model.eval()\n",
    "for needle_pos in [16, 32, 64, 96]:\n",
    "    niah = simple_niah(model, seq_len=128, needle_pos=needle_pos, n_trials=30)\n",
    "    avg = sum(r['ratio'] for r in niah) / len(niah)\n",
    "    print(f\"  needle@{needle_pos}: {avg:.2f}x random\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
